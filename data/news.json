{
  "lastUpdated": "2025-06-24T01:07:57.603Z",
  "totalFound": 823,
  "news": [
    {
      "title": "Court filings reveal OpenAI and io’s early work on an AI device",
      "titleJa": "Court filings reveal openai and io’s early work on an ai device",
      "link": "https://techcrunch.com/2025/06/23/court-filings-reveal-openai-and-ios-early-work-on-an-ai-device/",
      "pubDate": "Mon, 23 Jun 2025 23:44:38 +0000",
      "content": "Legal filings submitted earlier this month from lawyers representing OpenAI and Jony Ive’s io reveal new details about the companies’ efforts to build a mass-market AI hardware device. The filings are part of a trademark dispute lawsuit filed this month by iyO, a Google-backed hardware startup developing custom-molded earpieces that connect to other devices. Over […]",
      "summary": "Legal filings submitted earlier this month from lawyers representing OpenAI and Jony Ive’s io reveal new details about the companies’ efforts to build...",
      "summaryJa": "Legal filings submitted earlier this month from lawyers representing openai and jony ive’s io reveal new details about the companies’ efforts to build...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 75
    },
    {
      "title": "Beyond static AI: MIT’s new framework lets models teach themselves",
      "titleJa": "Beyond static ai: mit’s new framework lets models teach themselves",
      "link": "https://venturebeat.com/ai/beyond-static-ai-mits-new-framework-lets-models-teach-themselves/",
      "pubDate": "Mon, 23 Jun 2025 21:58:44 +0000",
      "content": "MIT researchers developed SEAL, a framework that lets language models continuously learn new knowledge and tasks.",
      "summary": "MIT researchers developed SEAL, a framework that lets language models continuously learn new knowledge and tasks.",
      "summaryJa": "Mit researchers developed seal, a framework that lets language models continuously learn new knowledge and tasks.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 30
    },
    {
      "title": "Salesforce launches Agentforce 3 with AI agent observability and MCP support",
      "titleJa": "Salesforce ローンチ agentforce 3 with ai agent observability and mcp support",
      "link": "https://venturebeat.com/ai/salesforce-launches-agentforce-3-with-ai-agent-observability-and-mcp-support/",
      "pubDate": "Mon, 23 Jun 2025 21:03:09 +0000",
      "content": "Salesforce Agentforce 3 has AI agent observability and native MCP support, providing real-time visibility and secure interoperability.",
      "summary": "Salesforce Agentforce 3 has AI agent observability and native MCP support, providing real-time visibility and secure interoperability.",
      "summaryJa": "Salesforce agentforce 3 has ai agent observability and native mcp support, providing real-time visibility and secure interoperability.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 55
    },
    {
      "title": "Databricks, Perplexity co-founder pledges $100M on new fund for AI researchers",
      "titleJa": "Databricks, perplexity co-founder pledges $100m on new fund for ai researchers",
      "link": "https://techcrunch.com/2025/06/23/databricks-perplexity-co-founder-pledges-100m-on-new-fund-for-ai-researchers/",
      "pubDate": "Mon, 23 Jun 2025 20:30:03 +0000",
      "content": "Andy Konwinski is pledging $100 million of his own money for a new kind of institute to fund researchers. It's already backed Ion Stoica's new lab.",
      "summary": "Andy Konwinski is pledging $100 million of his own money for a new kind of institute to fund researchers.",
      "summaryJa": "Andy konwinski is pledging $100 million of his own money for a new kind of institute to fund researchers.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 40
    },
    {
      "title": "Leak reveals Grok might soon edit your spreadsheets",
      "titleJa": "Leak reveals grok might soon edit your spreadsheets",
      "link": "https://techcrunch.com/2025/06/23/leak-reveals-grok-might-soon-edit-your-spreadsheets/",
      "pubDate": "Mon, 23 Jun 2025 18:16:57 +0000",
      "content": "Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI, Google, and Microsoft by embedding AI copilots into productivity tools.",
      "summary": "Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI,...",
      "summaryJa": "Leaked code suggests xai is developing an advanced file editor for grok with spreadsheet support, signaling the company’s push to compete with openai,...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 75
    },
    {
      "title": "Four months after a $3B valuation, Harvey AI grows to $5B",
      "titleJa": "Four months after a $3b valuation, harvey ai grows to $5b",
      "link": "https://techcrunch.com/2025/06/23/four-months-after-a-3b-valuation-harvey-ai-grows-to-5b/",
      "pubDate": "Mon, 23 Jun 2025 18:03:00 +0000",
      "content": "The round, which comes just four months after $300 million Series D, was co-led by Kleiner Perkins and Coatue.",
      "summary": "The round, which comes just four months after $300 million Series D, was co-led by Kleiner Perkins and Coatue.",
      "summaryJa": "The round, which comes just four months after $300 million series d, was co-led by kleiner perkins and coatue.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 40
    },
    {
      "title": "Over a million people now have access to the gen-AI powered Alexa+",
      "titleJa": "Over a million people now have access to the gen-ai powered alexa+",
      "link": "https://techcrunch.com/2025/06/23/over-a-million-people-now-have-access-to-the-gen-ai-powered-alexa/",
      "pubDate": "Mon, 23 Jun 2025 17:38:40 +0000",
      "content": "Alexa+ is available for free during Early Access and will later be free for Prime customers. Non-Prime users will be able to use the service for $19.99 per month after it publicly launches.",
      "summary": "Alexa+ is available for free during Early Access and will later be free for Prime customers.",
      "summaryJa": "Alexa+ is available for free during early access and will later be free for prime customers.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 45
    },
    {
      "title": "Musk’s attempts to politicize his Grok AI are bad for users and enterprises — here’s why",
      "titleJa": "Musk’s attempts to politicize his grok ai are bad for users and enterprises — here’s why",
      "link": "https://venturebeat.com/ai/musks-attempts-to-politicize-his-grok-ai-are-bad-for-users-and-enterprises-heres-why/",
      "pubDate": "Mon, 23 Jun 2025 17:29:51 +0000",
      "content": "As an independent business owner or leader, how could you possibly trust Grok to give you unbiased results?",
      "summary": "As an independent business owner or leader, how could you possibly trust Grok to give you unbiased results?",
      "summaryJa": "As an independent business owner or leader, how could you possibly trust grok to give you unbiased results?",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 30
    },
    {
      "title": "AllSpice’s platform is the GitHub for electrical engineering teams",
      "titleJa": "Allspice’s platform is the github for electrical engineering teams",
      "link": "https://techcrunch.com/2025/06/23/allspices-platform-is-the-github-for-electrical-engineering-teams/",
      "pubDate": "Mon, 23 Jun 2025 16:00:00 +0000",
      "content": "AllSpice's platform sits between existing workflow software. It allows hardware teams to collaborate on the types of documents they traditionally work in — documents that don't easily translate over Slack and email — like PCB files and electronic CAD files, both of which are used to design circuit boards. AllSpice's platform has landed customers such as Blue Origin and Bose and just raised a $15 million Series A round.",
      "summary": "AllSpice's platform sits between existing workflow software.",
      "summaryJa": "Allspice's platform sits between existing workflow software.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 30
    },
    {
      "title": "A Chinese firm has just launched a constantly changing set of AI benchmarks",
      "titleJa": "A chinese firm has just launched a constantly changing set of ai benchmarks",
      "link": "https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/",
      "pubDate": "Mon, 23 Jun 2025 15:46:28 +0000",
      "content": "When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That’s thanks to the way it evaluates models not only on the…",
      "summary": "When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data.",
      "summaryJa": "When testing an ai モデル, it’s hard to tell if it is reasoning or just regurgitating answers from its 学習 data.",
      "source": "MIT Technology Review AI",
      "category": "tech",
      "importance": 45
    },
    {
      "title": "Why we’re focusing VB Transform on the agentic revolution – and what’s at stake for enterprise AI leaders",
      "titleJa": "Why we’re focusing vb transform on the agentic revolution – and what’s at stake for enterprise ai leaders",
      "link": "https://venturebeat.com/ai/why-were-focusing-vb-transform-on-the-agentic-revolution-and-whats-at-stake-for-enterprise-ai-leaders/",
      "pubDate": "Mon, 23 Jun 2025 15:44:31 +0000",
      "content": "VB Transform 2025 tackles the agentic AI revolution—how enterprises can close the infrastructure gap and turn dazzling demos into deployed, trusted agents.",
      "summary": "VB Transform 2025 tackles the agentic AI revolution—how enterprises can close the infrastructure gap and turn dazzling demos into deployed, trusted ag...",
      "summaryJa": "Vb transform 2025 tackles the agentic ai revolution—how enterprises can close the infrastructure gap and turn dazzling demos into deployed, trusted ag...",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 30
    },
    {
      "title": "Google adds AI features to Chromebook Plus devices",
      "titleJa": "Google adds ai features to chromebook plus devices",
      "link": "https://techcrunch.com/2025/06/23/google-adds-ai-features-to-chromebook-plus-devices/",
      "pubDate": "Mon, 23 Jun 2025 13:00:00 +0000",
      "content": "Google's adding a slew of AI features to the Chromebook Plus line, including a search and text capture tool, NotebookLM, and a tool for simplifying text.",
      "summary": "Google's adding a slew of AI features to the Chromebook Plus line, including a search and text capture tool, NotebookLM, and a tool for simplifying te...",
      "summaryJa": "Google's adding a slew of ai features to the chromebook plus line, including a search and text capture tool, notebooklm, and a tool for simplifying te...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 45
    },
    {
      "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
      "titleJa": "Lego-puzzles: how good are mllms at multi-step spatial reasoning?",
      "link": "https://arxiv.org/abs/2503.19990",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.19990v3 Announce Type: replace \nAbstract: Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 125
    },
    {
      "title": "SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents",
      "titleJa": "Shade-arena: evaluating sabotage and monitoring in LLM agents",
      "link": "https://arxiv.org/abs/2506.15740",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15740v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling",
      "titleJa": "From LLM-anation to LLM-orchestrator: coordinating small models for data labeling",
      "link": "https://arxiv.org/abs/2506.16393",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16393v1 Announce Type: cross \nAbstract: Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges",
      "titleJa": "Llms and stack overflow discussions: reliability, impact, and challenges",
      "link": "https://arxiv.org/abs/2402.08801",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2402.08801v2 Announce Type: replace-cross \nAbstract: Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (i) quantify the reliability of LLMs answers and their potential to replace Stack Overflow in the long term; (ii) identify and understand why LLMs fail; (iii) measure users activity evolution with Stack Overflow over time; and (iv) compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs and provide guidelines for future challenges faced by users and researchers.",
      "summary": "arXiv:2402.",
      "summaryJa": "Arxiv:2402.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
      "titleJa": "Aqa-bench: an interactive benchmark for evaluating llms' sequential reasoning ability",
      "link": "https://arxiv.org/abs/2402.09404",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2402.09404v2 Announce Type: replace-cross \nAbstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models' performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.",
      "summary": "arXiv:2402.",
      "summaryJa": "Arxiv:2402.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation",
      "titleJa": "Ssr-zero: simple self-rewarding 強化学習 for machine translation",
      "link": "https://arxiv.org/abs/2505.16637",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.16637v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks",
      "titleJa": "Swe-factory: your automated factory for issue resolution 学習 data and evaluation benchmarks",
      "link": "https://arxiv.org/abs/2506.10954",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.10954v2 Announce Type: replace-cross \nAbstract: Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials",
      "titleJa": "Integrating dynamical systems learning with foundational models: a meta-evolutionary ai framework for clinical trials",
      "link": "https://arxiv.org/abs/2506.14782",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14782v2 Announce Type: replace \nAbstract: Artificial intelligence (AI) has evolved into an ecosystem of specialized \"species,\" each with unique strengths. We analyze two: DeepSeek-V3, a 671-billion-parameter Mixture of Experts large language model (LLM) exemplifying scale-driven generality, and NetraAI, a dynamical system-based framework engineered for stability and interpretability on small clinical trial datasets. We formalize NetraAI's foundations, combining contraction mappings, information geometry, and evolutionary algorithms to identify predictive patient cohorts. Features are embedded in a metric space and iteratively contracted toward stable attractors that define latent subgroups. A pseudo-temporal embedding and long-range memory enable exploration of higher-order feature interactions, while an internal evolutionary loop selects compact, explainable 2-4-variable bundles (\"Personas\").\n  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary layer that observes Persona outputs, prioritizes promising variables, injects domain knowledge, and assesses robustness. This two-tier architecture mirrors the human scientific process: NetraAI as experimentalist, the LLM as theorist, forming a self-improving loop.\n  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI uncovered small, high-effect-size subpopulations that transformed weak baseline models (AUC ~0.50-0.68) into near-perfect classifiers using only a few features. We position NetraAI at the intersection of dynamical systems, information geometry, and evolutionary learning, aligned with emerging concept-level reasoning paradigms such as LeCun's Joint Embedding Predictive Architecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI offers a new generation of adaptive, self-reflective AI to accelerate clinical discovery.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 110
    },
    {
      "title": "Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning",
      "titleJa": "Context matters! relaxing goals with llms for feasible 3d scene planning",
      "link": "https://arxiv.org/abs/2506.15828",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15828v1 Announce Type: cross \nAbstract: Classical planning in AI and Robotics addresses complex tasks by shifting from imperative to declarative approaches (e.g., PDDL). However, these methods often fail in real scenarios due to limited robot perception and the need to ground perceptions to planning predicates. This often results in heavily hard-coded behaviors that struggle to adapt, even with scenarios where goals can be achieved through relaxed planning. Meanwhile, Large Language Models (LLMs) lead to planning systems that leverage commonsense reasoning but often at the cost of generating unfeasible and/or unsafe plans. To address these limitations, we present an approach integrating classical planning with LLMs, leveraging their ability to extract commonsense knowledge and ground actions. We propose a hierarchical formulation that enables robots to make unfeasible tasks tractable by defining functionally equivalent goals through gradual relaxation. This mechanism supports partial achievement of the intended objective, suited to the agent's specific context. Our method demonstrates its ability to adapt and execute tasks effectively within environments modeled using 3D Scene Graphs through comprehensive qualitative and quantitative evaluations. We also show how this method succeeds in complex scenarios where other benchmark methods are more likely to fail. Code, dataset, and additional material are released to the community.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 105
    },
    {
      "title": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights",
      "titleJa": "Web archives metadata generation with GPT-4o: challenges and insights",
      "link": "https://arxiv.org/abs/2411.05409",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.05409v3 Announce Type: replace-cross \nAbstract: Current metadata creation for web archives is time consuming and costly due to reliance on human effort. This paper explores the use of gpt-4o for metadata generation within the Web Archive Singapore, focusing on scalability, efficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files using data reduction techniques, achieving a notable 99.9% reduction in metadata generation costs. By prompt engineering, we generated titles and abstracts, which were evaluated both intrinsically using Levenshtein Distance and BERTScore, and extrinsically with human cataloguers using McNemar's test. Results indicate that while our method offers significant cost savings and efficiency gains, human curated metadata maintains an edge in quality. The study identifies key challenges including content inaccuracies, hallucinations, and translation issues, suggesting that Large Language Models (LLMs) should serve as complements rather than replacements for human cataloguers. Future work will focus on refining prompts, improving content filtering, and addressing privacy concerns through experimentation with smaller models. This research advances the integration of LLMs in web archiving, offering valuable insights into their current capabilities and outlining directions for future enhancements. The code is available at https://github.com/masamune-prog/warc2summary for further development and use by institutions facing similar challenges.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 105
    },
    {
      "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
      "titleJa": "Codev-r1: reasoning-enhanced verilog generation",
      "link": "https://arxiv.org/abs/2505.24183",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.24183v2 Announce Type: replace \nAbstract: Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 105
    },
    {
      "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks",
      "titleJa": "Is-bench: evaluating interactive safety of vlm-driven embodied agents in daily household tasks",
      "link": "https://arxiv.org/abs/2506.16402",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16402v1 Announce Type: new \nAbstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal",
      "titleJa": "Quantum 人工知能 for secure 自律的 vehicle navigation: an architectural proposal",
      "link": "https://arxiv.org/abs/2506.16000",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16000v1 Announce Type: cross \nAbstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which heavily relies on collecting and processing large amounts of data in various states and taking a confident and safe decision to define the next vehicle maneuver. In this paper, we propose a novel architecture based on Quantum Artificial Intelligence by enabling quantum and AI at various levels of navigation decision making and communication process in Autonomous vehicles : Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum reinforcement learning for navigation policy optimization and finally post-quantum cryptographic protocols for secure communication. Quantum neural networks uses quantum amplitude encoding to fuse data from various sensors like LiDAR, radar, camera, GPS and weather etc., This approach gives a unified quantum state representation between heterogeneous sensor modalities. Nav-Q module processes the fused quantum states through variational quantum circuits to learn optimal navigation policies under swift dynamic and complex conditions. Finally, post quantum cryptographic protocols are used to secure communication channels for both within vehicle communication and V2X (Vehicle to Everything) communications and thus secures the autonomous vehicle communication from both classical and quantum security threats. Thus, the proposed framework addresses fundamental challenges in autonomous vehicles navigation by providing quantum performance and future proof security. Index Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders",
      "titleJa": "A brain-to-population graph learning framework for diagnosing brain disorders",
      "link": "https://arxiv.org/abs/2506.16096",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16096v1 Announce Type: cross \nAbstract: Recent developed graph-based methods for diagnosing brain disorders using functional connectivity highly rely on predefined brain atlases, but overlook the rich information embedded within atlases and the confounding effects of site and phenotype variability. To address these challenges, we propose a two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates the semantic similarity of brain regions and condition-based population graph modeling. In the first stage, termed brain representation learning, we leverage brain atlas knowledge from GPT-4 to enrich the graph representation and refine the brain graph through an adaptive node reassignment graph attention network. In the second stage, termed population disorder diagnosis, phenotypic data is incorporated into population graph construction and feature fusion to mitigate confounding effects and enhance diagnosis performance. Experiments on the ABIDE I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms state-of-the-art methods in prediction accuracy while enhancing interpretability. Overall, our proposed framework offers a reliable and personalized approach to brain disorder diagnosis, advancing clinical applicability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "NepaliGPT: A Generative Language Model for the Nepali Language",
      "titleJa": "Nepaligpt: a generative language モデル for the nepali language",
      "link": "https://arxiv.org/abs/2506.16399",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16399v1 Announce Type: cross \nAbstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \\textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\\%, and causal consistency of 85.41\\%.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support",
      "titleJa": "Do we talk to robots like therapists, and do they respond accordingly? language alignment in ai emotional support",
      "link": "https://arxiv.org/abs/2506.16473",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16473v1 Announce Type: cross \nAbstract: As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "LLMs in Coding and their Impact on the Commercial Software Engineering Landscape",
      "titleJa": "Llms in coding and their impact on the commercial software engineering landscape",
      "link": "https://arxiv.org/abs/2506.16653",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16653v1 Announce Type: cross \nAbstract: Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "TabArena: A Living Benchmark for Machine Learning on Tabular Data",
      "titleJa": "Tabarena: a living benchmark for 機械学習 on tabular data",
      "link": "https://arxiv.org/abs/2506.16791",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16791v1 Announce Type: cross \nAbstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning and investigate the contributions of individual models. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control",
      "titleJa": "Parkformer: a トランスフォーマー-based parking 政策 with goal embedding and pedestrian-aware control",
      "link": "https://arxiv.org/abs/2506.16856",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16856v1 Announce Type: cross \nAbstract: Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "BEADs: Bias Evaluation Across Domains",
      "titleJa": "Beads: バイアス evaluation across domains",
      "link": "https://arxiv.org/abs/2406.04220",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.04220v5 Announce Type: replace-cross \nAbstract: Recent advancements in large language models (LLMs) have significantly improved natural language processing (NLP) applications. However, these models often inherit biases from their training data. While several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation, and lack comprehensive coverage across a broader range of tasks. To address this gap, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is the gold-standard annotation provided by GPT-4 for scalability, with expert verification to ensure high reliability. BEADs can be used for both fine-tuning models (for classification and generation tasks) and evaluating LLM behavior. Our findings show that BEADs effectively surfaces various biases during model fine-tuning and helps reduce biases in language generation tasks while maintaining output quality. The dataset also highlights prevalent demographic biases in LLMs during evaluation. We release BEADs as a practical resource for detecting and mitigating bias across domains, supporting the development of responsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management",
      "titleJa": "Can large language models replace human subjects? a large-scale replication of scenario-based experiments in psychology and management",
      "link": "https://arxiv.org/abs/2409.00128",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2409.00128v3 Announce Type: replace-cross \nAbstract: Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.",
      "summary": "arXiv:2409.",
      "summaryJa": "Arxiv:2409.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment",
      "titleJa": "Assistantx: an LLM-powered proactive assistant in collaborative human-populated environment",
      "link": "https://arxiv.org/abs/2409.17655",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2409.17655v3 Announce Type: replace-cross \nAbstract: Current service robots suffer from limited natural language communication abilities, heavy reliance on predefined commands, ongoing human intervention, and, most notably, a lack of proactive collaboration awareness in human-populated environments. This results in narrow applicability and low utility. In this paper, we introduce AssistantX, an LLM-powered proactive assistant designed for autonomous operation in realworld scenarios with high accuracy. AssistantX employs a multi-agent framework consisting of 4 specialized LLM agents, each dedicated to perception, planning, decision-making, and reflective review, facilitating advanced inference capabilities and comprehensive collaboration awareness, much like a human assistant by your side. We built a dataset of 210 real-world tasks to validate AssistantX, which includes instruction content and status information on whether relevant personnel are available. Extensive experiments were conducted in both text-based simulations and a real office environment over the course of a month and a half. Our experiments demonstrate the effectiveness of the proposed framework, showing that AssistantX can reactively respond to user instructions, actively adjust strategies to adapt to contingencies, and proactively seek assistance from humans to ensure successful task completion. More details and videos can be found at https://assistantx-agent.github.io/AssistantX/.",
      "summary": "arXiv:2409.",
      "summaryJa": "Arxiv:2409.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
      "titleJa": "Adapting while learning: grounding llms for scientific problems with intelligent tool usage adaptation",
      "link": "https://arxiv.org/abs/2411.00412",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.00412v4 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs. Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component, Tool Usage Adaptation (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones. We validate our method on six scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on four custom-created datasets. Our code is open-source at https://github.com/Rose-STL-Lab/Adapting-While-Learning.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments",
      "titleJa": "Hierarchical and modular network on non-prehensile manipulation in general environments",
      "link": "https://arxiv.org/abs/2502.20843",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.20843v2 Announce Type: replace-cross \nAbstract: For robots to operate in general environments like households, they must be able to perform non-prehensile manipulation actions such as toppling and rolling to manipulate ungraspable objects. However, prior works on non-prehensile manipulation cannot yet generalize across environments with diverse geometries. The main challenge lies in adapting to varying environmental constraints: within a cabinet, the robot must avoid walls and ceilings; to lift objects to the top of a step, the robot must account for the step's pose and extent. While deep reinforcement learning (RL) has demonstrated impressive success in non-prehensile manipulation, accounting for such variability presents a challenge for the generalist policy, as it must learn diverse strategies for each new combination of constraints. To address this, we propose a modular and reconfigurable architecture that adaptively reconfigures network modules based on task requirements. To capture the geometric variability in environments, we extend the contact-based object representation (CORN) to environment geometries, and propose a procedural algorithm for generating diverse environments to train our agent. Taken together, the resulting policy can zero-shot transfer to novel real-world environments and objects despite training entirely within a simulator. We additionally release a simulation-based benchmark featuring nine digital twins of real-world scenes with 353 objects to facilitate non-prehensile manipulation research in realistic domains.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Semantic Outlier Removal with Embedding Models and LLMs",
      "titleJa": "Semantic outlier removal with embedding models and llms",
      "link": "https://arxiv.org/abs/2506.16644",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16644v1 Announce Type: new \nAbstract: Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning",
      "titleJa": "Steering your diffusion 政策 with latent space 強化学習",
      "link": "https://arxiv.org/abs/2506.15799",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15799v1 Announce Type: cross \nAbstract: Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior -- an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies -- a state-of-the-art BC methodology -- we propose diffusion steering via reinforcement learning (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models",
      "titleJa": "Laion-c: an out-of-distribution benchmark for web-scale vision models",
      "link": "https://arxiv.org/abs/2506.16950",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16950v1 Announce Type: cross \nAbstract: Out-of-distribution (OOD) robustness is a desired property of computer vision models. Improving model robustness requires high-quality signals from robustness benchmarks to quantify progress. While various benchmark datasets such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C corruption types are no longer OOD relative to today's large, web-scraped datasets, which already contain common corruptions such as blur or JPEG compression artifacts. Consequently, these benchmarks are no longer well-suited for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent models show saturating scores on ImageNet-era OOD benchmarks, indicating that it is unclear whether models trained on web-scale datasets truly become better at OOD generalization or whether they have simply been exposed to the test distortions during training. To address this, we introduce LAION-C as a benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion types specifically designed to be OOD, even for web-scale datasets such as LAION. In a comprehensive evaluation of state-of-the-art models, we find that the LAION-C dataset poses significant challenges to contemporary models, including MLLMs such as Gemini and GPT-4o. We additionally conducted a psychophysical experiment to evaluate the difficulty of our corruptions for human observers, enabling a comparison of models to lab-quality human robustness data. We observe a paradigm shift in OOD generalization: from humans outperforming models, to the best models now matching or outperforming the best human observers.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Harmonizing Safety and Speed: A Human-Algorithm Approach to Enhance the FDA's Medical Device Clearance Policy",
      "titleJa": "Harmonizing safety and speed: a human-アルゴリズム approach to enhance the FDA（米国食品医薬品局）'s 医療 device clearance 政策",
      "link": "https://arxiv.org/abs/2407.11823",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2407.11823v2 Announce Type: replace \nAbstract: The United States Food and Drug Administration's (FDA's) Premarket Notification 510(k) pathway allows manufacturers to gain approval for a medical device by demonstrating its substantial equivalence to another legally marketed device. However, the inherent ambiguity of this regulatory procedure has led to high recall rates for many devices cleared through this pathway. This trend has raised significant concerns regarding the efficacy of the FDA's current approach, prompting a reassessment of the 510(k) regulatory framework. In this paper, we develop a combined human-algorithm approach to assist the FDA in improving its 510(k) medical device clearance process by reducing the risk of recalls and the workload imposed on the FDA. We first develop machine learning methods to estimate the risk of recall of 510(k) medical devices based on the information available at submission time. We then propose a data-driven clearance policy that recommends acceptance, rejection, or deferral to FDA's committees for in-depth evaluation. We conduct an empirical study using a unique large-scale dataset of over 31,000 medical devices that we assembled based on data sources from the FDA and Centers for Medicare and Medicaid Service (CMS). A conservative evaluation of our proposed policy based on this data shows a 32.9% improvement in the recall rate and a 40.5% reduction in the FDA's workload. Our analyses also indicate that implementing our policy could result in significant annual cost savings of $1.7 billion, which highlights the value of using a holistic and data-driven approach to improve the FDA's current 510(k) medical device evaluation pathway.",
      "summary": "arXiv:2407.",
      "summaryJa": "Arxiv:2407.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Watermarking Language Models through Language Models",
      "titleJa": "Watermarking language models through language models",
      "link": "https://arxiv.org/abs/2411.05091",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.05091v2 Announce Type: replace \nAbstract: Watermarking the outputs of large language models (LLMs) is critical for provenance tracing, content regulation, and model accountability. Existing approaches often rely on access to model internals or are constrained by static rules and token-level perturbations. Moreover, the idea of steering generative behavior via prompt-based instruction control remains largely underexplored. We introduce a prompt-guided watermarking framework that operates entirely at the input level and requires no access to model parameters or decoding logits. The framework comprises three cooperating components: a Prompting LM that synthesizes watermarking instructions from user prompts, a Marking LM that generates watermarked outputs conditioned on these instructions, and a Detecting LM trained to classify whether a response carries an embedded watermark. This modular design enables dynamic watermarking that adapts to individual prompts while remaining compatible with diverse LLM architectures, including both proprietary and open-weight models. We evaluate the framework over 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral, LLaMA3, and DeepSeek. Experimental results show that watermark signals generalize across architectures and remain robust under fine-tuning, model distillation, and prompt-based adversarial attacks, demonstrating the effectiveness and robustness of the proposed approach.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
      "titleJa": "Al-khwarizmi: discovering physical laws with foundation models",
      "link": "https://arxiv.org/abs/2502.01702",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.01702v2 Announce Type: replace \nAbstract: Inferring physical laws from data is a central challenge in science and engineering, including but not limited to healthcare, physical sciences, biosciences, social sciences, sustainability, climate, and robotics. Deep networks offer high-accuracy results but lack interpretability, prompting interest in models built from simple components. The Sparse Identification of Nonlinear Dynamics (SINDy) method has become the go-to approach for building such modular and interpretable models. SINDy leverages sparse regression with L1 regularization to identify key terms from a library of candidate functions. However, SINDy's choice of candidate library and optimization method requires significant technical expertise, limiting its widespread applicability. This work introduces Al-Khwarizmi, a novel agentic framework for physical law discovery from data, which integrates foundational models with SINDy. Leveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach automates physical law discovery, incorporating prior knowledge and iteratively refining candidate solutions via reflection. Al-Khwarizmi operates in two steps: it summarizes system observations-comprising textual descriptions, raw data, and plots-followed by a secondary step that generates candidate feature libraries and optimizer configurations to identify hidden physics laws correctly. Evaluating our algorithm on over 198 models, we demonstrate state-of-the-art performance compared to alternatives, reaching a 20 percent increase against the best-performing alternative.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 95
    },
    {
      "title": "From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation",
      "titleJa": "From general to targeted rewards: surpassing GPT-4 in open-ended long-context generation",
      "link": "https://arxiv.org/abs/2506.16024",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16024v1 Announce Type: cross \nAbstract: Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 90
    },
    {
      "title": "Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining",
      "titleJa": "Human2locoman: learning versatile quadrupedal manipulation with human pretraining",
      "link": "https://arxiv.org/abs/2506.16475",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16475v1 Announce Type: cross \nAbstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a cross-embodiment imitation learning system for quadrupedal manipulation, leveraging data collected from both humans and LocoMan, a quadruped equipped with multiple manipulation modes. Specifically, we develop a teleoperation and data collection pipeline, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient modularized architecture that supports co-training and pretraining on structured modality-aligned data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. We validate our system on six real-world manipulation tasks, where it achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings compared to the baseline. Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data. Our code, hardware, and data are open-sourced at: https://human2bots.github.io.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 90
    },
    {
      "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions",
      "titleJa": "Llms in disease 診断: a comparative study of deepseek-r1 and o3 mini across chronic health conditions",
      "link": "https://arxiv.org/abs/2503.10486",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.10486v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 90
    },
    {
      "title": "UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
      "titleJa": "Uniworld-v1: high-resolution semantic encoders for unified visual understanding and generation",
      "link": "https://arxiv.org/abs/2506.03147",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.03147v4 Announce Type: replace-cross \nAbstract: Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 90
    },
    {
      "title": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning",
      "titleJa": "Slr: an automated synthesis framework for scalable logical reasoning",
      "link": "https://arxiv.org/abs/2506.15787",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15787v1 Announce Type: new \nAbstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues",
      "titleJa": "Exploring big five personality and ai capability effects in LLM-simulated negotiation dialogues",
      "link": "https://arxiv.org/abs/2506.15928",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15928v1 Announce Type: new \nAbstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations",
      "titleJa": "Dual-objective 強化学習 with novel hamilton-jacobi-bellman formulations",
      "link": "https://arxiv.org/abs/2506.16016",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16016v1 Announce Type: new \nAbstract: Hard constraints in reinforcement learning (RL), whether imposed via the reward function or the model architecture, often degrade policy performance. Lagrangian methods offer a way to blend objectives with constraints, but often require intricate reward engineering and parameter tuning. In this work, we extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to propose two novel value functions for dual-objective satisfaction. Namely, we address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds of two distinct rewards. In contrast with temporal logic approaches, which typically involve representing an automaton, we derive explicit, tractable Bellman forms in this context by decomposing our problem into reach, avoid, and reach-avoid problems, as to leverage these aforementioned recent advances. From a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are complementary and fundamentally different from standard sum-of-rewards problems and temporal logic problems, providing a new perspective on constrained decision-making. We leverage our analysis to propose a variation of Proximal Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of tasks for safe-arrival and multi-target achievement, we demonstrate that DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches and out-competes a number of baselines in various metrics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach",
      "titleJa": "Explainable rule application via structured prompting: a neural-symbolic approach",
      "link": "https://arxiv.org/abs/2506.16335",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16335v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle with consistent rule application, exception handling, and explainability, particularly in domains like legal analysis that require both natural language understanding and precise logical inference. This paper introduces a structured prompting framework that decomposes reasoning into three verifiable steps: entity identification, property extraction, and symbolic rule application. By integrating neural and symbolic approaches, our method leverages LLMs' interpretive flexibility while ensuring logical consistency through formal verification. The framework externalizes task definitions, enabling domain experts to refine logical structures without altering the architecture. Evaluated on the LegalBench hearsay determination task, our approach significantly outperformed baselines, with OpenAI o-family models showing substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini reaching 0.867 using structured decomposition with complementary predicates, compared to their few-shot baselines of 0.714 and 0.74 respectively. This hybrid neural-symbolic system offers a promising pathway for transparent and consistent rule-based reasoning, suggesting potential for explainable AI applications in structured legal reasoning tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
      "titleJa": "Multimodal fused learning for solving the generalized traveling salesman problem in robotic task planning",
      "link": "https://arxiv.org/abs/2506.16931",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16931v1 Announce Type: new \nAbstract: Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
      "titleJa": "When can モデル-free 強化学習 be enough for thinking?",
      "link": "https://arxiv.org/abs/2506.17124",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17124v1 Announce Type: new \nAbstract: Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of \"thinking\" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to \"thinking\" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a \\textit{thought Markov decision process} (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making",
      "titleJa": "The medperturb データセット: what non-content perturbations reveal about human and clinical LLM decision making",
      "link": "https://arxiv.org/abs/2506.17163",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17163v1 Announce Type: new \nAbstract: Clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs), but key questions remain about how LLMs and humans may differ in response to the real-world variability typified by clinical settings. To address this, we introduce MedPerturb, a dataset designed to systematically evaluate medical LLMs under controlled perturbations of clinical input. MedPerturb consists of clinical vignettes spanning a range of pathologies, each transformed along three axes: (1) gender modifications (e.g., gender-swapping or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or summaries). With MedPerturb, we release a dataset of 800 clinical contexts grounded in realistic input variability, outputs from four LLMs, and three human expert reads per clinical context. We use MedPerturb in two case studies to reveal how shifts in gender identity cues, language style, or format reflect diverging treatment selections between humans and LLMs. We find that LLMs are more sensitive to gender and style perturbations while human annotators are more sensitive to LLM-generated format perturbations such as clinical summaries. Our results highlight the need for evaluation frameworks that go beyond static benchmarks to assess the similarity between human clinician and LLM decisions under the variability characteristic of clinical settings.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models",
      "titleJa": "Base-q: バイアス and asymmetric scaling enhanced rotational quantization for large language models",
      "link": "https://arxiv.org/abs/2506.15689",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15689v1 Announce Type: cross \nAbstract: Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking",
      "titleJa": "Contraction actor-critic: contraction metric-guided 強化学習 for robust path tracking",
      "link": "https://arxiv.org/abs/2506.15700",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15700v1 Announce Type: cross \nAbstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a controller and a corresponding contraction metric -- a positive-definite Riemannian metric under which a closed-loop system is guaranteed to be incrementally exponentially stable. However, the synthesized controller only ensures that all the trajectories of the system converge to one single trajectory and, as such, does not impose any notion of optimality across an entire trajectory. Furthermore, constructing CCMs requires a known dynamics model and non-trivial effort in solving an infinite-dimensional convex feasibility problem, which limits its scalability to complex systems featuring high dimensionality with uncertainty. To address these issues, we propose to integrate CCMs into reinforcement learning (RL), where CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error under unknown dynamics. We show that our algorithm, called contraction actor-critic (CAC), formally enhances the capability of CCMs to provide a set of contracting policies with the long-term optimality of RL in a fully automated setting. Given a pre-trained dynamics model, CAC simultaneously learns a contraction metric generator (CMG) -- which generates a contraction metric -- and uses an actor-critic algorithm to learn an optimal tracking policy guided by that metric. We demonstrate the effectiveness of our algorithm relative to established baselines through extensive empirical studies, including simulated and real-world robot experiments, and provide a theoretical rationale for incorporating contraction theory into RL.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
      "titleJa": "Rast: reasoning activation in llms via small-モデル transfer",
      "link": "https://arxiv.org/abs/2506.15710",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15710v1 Announce Type: cross \nAbstract: Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling",
      "titleJa": "Batterybert for realistic battery fault detection using point-masked signal modeling",
      "link": "https://arxiv.org/abs/2506.15712",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15712v1 Announce Type: cross \nAbstract: Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation",
      "titleJa": "Safemimic: towards safe and 自律的 human-to-robot imitation for mobile manipulation",
      "link": "https://arxiv.org/abs/2506.15847",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15847v1 Announce Type: cross \nAbstract: For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training",
      "titleJa": "Trainverify: equivalence-based verification for distributed LLM 学習",
      "link": "https://arxiv.org/abs/2506.15961",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15961v1 Announce Type: cross \nAbstract: Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "JETHICS: Japanese Ethics Understanding Evaluation Dataset",
      "titleJa": "Jethics: japanese 倫理 understanding evaluation データセット",
      "link": "https://arxiv.org/abs/2506.16187",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16187v1 Announce Type: cross \nAbstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Can structural correspondences ground real world representational content in Large Language Models?",
      "titleJa": "Can structural correspondences ground real world representational content in large language models?",
      "link": "https://arxiv.org/abs/2506.16370",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16370v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation",
      "titleJa": "Joint tensor-train parameterization for efficient and expressive low-rank adaptation",
      "link": "https://arxiv.org/abs/2506.16456",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16456v1 Announce Type: cross \nAbstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient fine-tuning of large-scale neural models. However, standard LoRA independently optimizes low-rank matrices, which inherently limits its expressivity and generalization capabilities. While classical tensor-train (TT) decomposition can be separately employed on individual LoRA matrices, this work demonstrates that the classical TT-based approach neither significantly improves parameter efficiency nor achieves substantial performance gains. This paper proposes TensorGuide, a novel tensor-train-guided adaptation framework to overcome these limitations. TensorGuide generates two correlated low-rank LoRA matrices through a unified TT structure driven by controlled Gaussian noise. The resulting joint TT representation inherently provides structured, low-rank adaptations, significantly enhancing expressivity, generalization, and parameter efficiency without increasing the number of trainable parameters. Theoretically, we justify these improvements through neural tangent kernel analyses, demonstrating superior optimization dynamics and enhanced generalization. Extensive experiments on quantum dot classification and GPT-2 fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently outperforms standard LoRA and TT-LoRA, achieving improved accuracy and scalability with fewer parameters.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Grounding Language Models with Semantic Digital Twins for Robotic Planning",
      "titleJa": "Grounding language models with semantic digital twins for robotic planning",
      "link": "https://arxiv.org/abs/2506.16493",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16493v1 Announce Type: cross \nAbstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs) with Large Language Models (LLMs) to enable adaptive and goal-driven robotic task execution in dynamic environments. The system decomposes natural language instructions into structured action triplets, which are grounded in contextual environmental data provided by the SDT. This semantic grounding allows the robot to interpret object affordances and interaction rules, enabling action planning and real-time adaptability. In case of execution failures, the LLM utilizes error feedback and SDT insights to generate recovery strategies and iteratively revise the action plan. We evaluate our approach using tasks from the ALFRED benchmark, demonstrating robust performance across various household scenarios. The proposed framework effectively combines high-level reasoning with semantic environment understanding, achieving reliable task completion in the face of uncertainty and failure.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples",
      "titleJa": "Relic: enhancing reward モデル generalization for low-resource indic languages with few-shot examples",
      "link": "https://arxiv.org/abs/2506.16502",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16502v1 Announce Type: cross \nAbstract: Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios",
      "titleJa": "Bida: a bi-level interaction decision-making アルゴリズム for 自律的 vehicles in dynamic traffic scenarios",
      "link": "https://arxiv.org/abs/2506.16546",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16546v1 Announce Type: cross \nAbstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to interact with other traffic participants while making real-time and safety-critical decisions accordingly. The unpredictability of human behaviors poses significant challenges, particularly in dynamic scenarios, such as multi-lane highways and unsignalized T-intersections. To address this gap, we design a bi-level interaction decision-making algorithm (BIDA) that integrates interactive Monte Carlo tree search (MCTS) with deep reinforcement learning (DRL), aiming to enhance interaction rationality, efficiency and safety of AVs in dynamic key traffic scenarios. Specifically, we adopt three types of DRL algorithms to construct a reliable value network and policy network, which guide the online deduction process of interactive MCTS by assisting in value update and node selection. Then, a dynamic trajectory planner and a trajectory tracking controller are designed and implemented in CARLA to ensure smooth execution of planned maneuvers. Experimental evaluations demonstrate that our BIDA not only enhances interactive deduction and reduces computational costs, but also outperforms other latest benchmarks, which exhibits superior safety, efficiency and interaction rationality under varying traffic conditions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions",
      "titleJa": "Ai-driven tools in modern software quality assurance: an assessment of benefits, challenges, and future directions",
      "link": "https://arxiv.org/abs/2506.16586",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16586v1 Announce Type: cross \nAbstract: Traditional quality assurance (QA) methods face significant challenges in addressing the complexity, scale, and rapid iteration cycles of modern software systems and are strained by limited resources available, leading to substantial costs associated with poor quality. The object of this research is the Quality Assurance processes for modern distributed software applications. The subject of the research is the assessment of the benefits, challenges, and prospects of integrating modern AI-oriented tools into quality assurance processes. We performed comprehensive analysis of implications on both verification and validation processes covering exploratory test analyses, equivalence partitioning and boundary analyses, metamorphic testing, finding inconsistencies in acceptance criteria (AC), static analyses, test case generation, unit test generation, test suit optimization and assessment, end to end scenario execution. End to end regression of sample enterprise application utilizing AI-agents over generated test scenarios was implemented as a proof of concept highlighting practical use of the study. The results, with only 8.3% flaky executions of generated test cases, indicate significant potential for the proposed approaches. However, the study also identified substantial challenges for practical adoption concerning generation of semantically identical coverage, \"black box\" nature and lack of explainability from state-of-the-art Large Language Models (LLMs), the tendency to correct mutated test cases to match expected results, underscoring the necessity for thorough verification of both generated artifacts and test execution results. The research demonstrates AI's transformative potential for QA but highlights the importance of a strategic approach to implementing these technologies, considering the identified limitations and the need for developing appropriate verification methodologies.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces",
      "titleJa": "Distribution parameter actor-critic: shifting the agent-environment boundary for diverse action spaces",
      "link": "https://arxiv.org/abs/2506.16608",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16608v1 Announce Type: cross \nAbstract: We introduce a novel reinforcement learning (RL) framework that treats distribution parameters as actions, redefining the boundary between agent and environment. This reparameterization makes the new action space continuous, regardless of the original action type (discrete, continuous, mixed, etc.). Under this new parameterization, we develop a generalized deterministic policy gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has lower variance than the gradient in the original action space. Although learning the critic over distribution parameters poses new challenges, we introduce interpolated critic learning (ICL), a simple yet effective strategy to enhance learning, supported by insights from bandit settings. Building on TD3, a strong baseline for continuous control, we propose a practical DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC). Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance on the same environments with discretized action spaces.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties",
      "titleJa": "The role of モデル confidence on バイアス effects in measured uncertainties",
      "link": "https://arxiv.org/abs/2506.16724",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16724v1 Announce Type: cross \nAbstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding",
      "titleJa": "Metapath-based hyperbolic contrastive learning for heterogeneous graph embedding",
      "link": "https://arxiv.org/abs/2506.16754",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16754v1 Announce Type: cross \nAbstract: The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Learning Dexterous Object Handover",
      "titleJa": "Learning dexterous object handover",
      "link": "https://arxiv.org/abs/2506.16822",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16822v1 Announce Type: cross \nAbstract: Object handover is an important skill that we use daily when interacting with other humans. To deploy robots in collaborative setting, like houses, being able to receive and handing over objects safely and efficiently becomes a crucial skill. In this work, we demonstrate the use of Reinforcement Learning (RL) for dexterous object handover between two multi-finger hands. Key to this task is the use of a novel reward function based on dual quaternions to minimize the rotation distance, which outperforms other rotation representations such as Euler and rotation matrices. The robustness of the trained policy is experimentally evaluated by testing w.r.t. objects that are not included in the training distribution, and perturbations during the handover process. The results demonstrate that the trained policy successfully perform this task, achieving a total success rate of 94% in the best-case scenario after 100 experiments, thereby showing the robustness of our policy with novel objects. In addition, the best-case performance of the policy decreases by only 13.8% when the other robot moves during the handover, proving that our policy is also robust to this type of perturbation, which is common in real-world object handovers.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "AnyTraverse: An off-road traversability framework with VLM and human operator in the loop",
      "titleJa": "Anytraverse: an off-road traversability framework with vlm and human operator in the loop",
      "link": "https://arxiv.org/abs/2506.16826",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16826v1 Announce Type: cross \nAbstract: Off-road traversability segmentation enables autonomous navigation with applications in search-and-rescue, military operations, wildlife exploration, and agriculture. Current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes, and are not adaptive to be used for different robot types. We present AnyTraverse, a framework combining natural language-based prompts with human-operator assistance to determine navigable regions for diverse robotic vehicles. The system segments scenes for a given set of prompts and calls the operator only when encountering previously unexplored scenery or unknown class not part of the prompt in its region-of-interest, thus reducing active supervision load while adapting to varying outdoor scenes. Our zero-shot learning approach eliminates the need for extensive data collection or retraining. Our experimental validation includes testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate real-world deployment on multiple robot platforms. The results show that AnyTraverse performs better than GA-NAV and Off-seg while offering a vehicle-agnostic approach to off-road traversability that balances automation with targeted human supervision.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
      "titleJa": "Enhancing step-by-step and verifiable 医療 reasoning in mllms",
      "link": "https://arxiv.org/abs/2506.16962",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16962v1 Announce Type: cross \nAbstract: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond",
      "titleJa": "Language bottleneck models: a framework for interpretable knowledge tracing and beyond",
      "link": "https://arxiv.org/abs/2506.16982",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16982v1 Announce Type: cross \nAbstract: Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs",
      "titleJa": "Tower+: bridging generality and translation specialization in multilingual llms",
      "link": "https://arxiv.org/abs/2506.17080",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17080v1 Announce Type: cross \nAbstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems",
      "titleJa": "Dissecting the swe-bench leaderboards: profiling submitters and architectures of LLM- and agent-based repair systems",
      "link": "https://arxiv.org/abs/2506.17208",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17208v1 Announce Type: cross \nAbstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive Diagnosis Theory",
      "titleJa": "Cds: knowledge component-driven data synthesis guided by cognitive 診断 theory",
      "link": "https://arxiv.org/abs/2501.07674",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.07674v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "LLM-Guided Indoor Navigation with Multimodal Map Understanding",
      "titleJa": "LLM-guided indoor navigation with multimodal map understanding",
      "link": "https://arxiv.org/abs/2503.11702",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.11702v4 Announce Type: replace \nAbstract: Indoor navigation presents unique challenges due to complex layouts and the unavailability of GNSS signals. Existing solutions often struggle with contextual adaptation, and typically require dedicated hardware. In this work, we explore the potential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural, context-aware navigation instructions from indoor map images. We design and evaluate test cases across different real-world environments, analyzing the effectiveness of LLMs in interpreting spatial layouts, handling user constraints, and planning efficient routes. Our findings demonstrate the potential of LLMs for supporting personalized indoor navigation, with an average of 86.59% correct indications and a maximum of 97.14%. The proposed system achieves high accuracy and reasoning performance. These results have key implications for AI-driven navigation and assistive technologies.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage",
      "titleJa": "Towards ai-driven policing: interdisciplinary knowledge discovery from police body-worn camera footage",
      "link": "https://arxiv.org/abs/2504.20007",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.20007v3 Announce Type: replace \nAbstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating image, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. The framework incorporates speaker separation, transcription, and large language models (LLMs) to produce structured, interpretable summaries of police-civilian encounters. We also employ a custom evaluation pipeline to assess transcription quality and behavior detection accuracy in high-stakes, real-world policing scenarios. Our methodology, computational techniques, and findings outline a practical approach for law enforcement review, training, and accountability processes while advancing the frontiers of knowledge discovery from complex police BWC data.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
      "titleJa": "On path to multimodal historical reasoning: histbench and histagent",
      "link": "https://arxiv.org/abs/2505.20246",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.20246v3 Announce Type: replace \nAbstract: Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents",
      "titleJa": "Riosworld: benchmarking the risk of multimodal computer-use agents",
      "link": "https://arxiv.org/abs/2506.00618",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.00618v3 Announce Type: replace \nAbstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \\textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \\textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback",
      "titleJa": "Automated skill discovery for language agents through exploration and iterative feedback",
      "link": "https://arxiv.org/abs/2506.04287",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.04287v2 Announce Type: replace \nAbstract: Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
      "titleJa": "Lars: latent reasoning skills for chain-of-thought reasoning",
      "link": "https://arxiv.org/abs/2312.04684",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2312.04684v4 Announce Type: replace-cross \nAbstract: Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.",
      "summary": "arXiv:2312.",
      "summaryJa": "Arxiv:2312.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language Models Closing the Gap with Human Vision ?",
      "titleJa": "Dual thinking and logical processing -- are multi-modal large language models closing the gap with human vision ?",
      "link": "https://arxiv.org/abs/2406.06967",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.06967v4 Announce Type: replace-cross \nAbstract: The dual thinking framework considers fast, intuitive, and slower logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ, and the latter is under-explored in current studies. We introduce a novel adversarial dataset to provide evidence for the dual thinking framework in human vision, which also facilitates the study of the qualitative behavior of deep learning models. Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows that the early stopping of visual processing can result in missing relevant information. MLLMs (Multi-modal Large Language Models) and VLMs (Vision Language Models) have made significant progress in correcting errors in intuitive processing in human vision and showed enhanced performance on images requiring logical processing. However, their improvements in logical processing have not kept pace with their advancements in intuitive processing. In contrast, segmentation models exhibit errors similar to those seen in intuitive human processing and lack understanding of sub-structures, as indicated by errors related to sub-components in identified instances. As AI (Artificial Intelligence)-based systems find increasing applications in safety-critical domains like autonomous driving, the integration of logical processing capabilities becomes essential. This not only enhances performance but also addresses the limitations of scaling-based approaches while ensuring robustness and reliability in real-world environments.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Efficient Mixture-of-Expert for Video-based Driver State and Physiological Multi-task Estimation in Conditional Autonomous Driving",
      "titleJa": "Efficient mixture-of-expert for video-based driver state and physiological multi-task estimation in conditional 自律的 driving",
      "link": "https://arxiv.org/abs/2410.21086",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.21086v2 Announce Type: replace-cross \nAbstract: Road safety remains a critical challenge worldwide, with approximately 1.35 million fatalities annually attributed to traffic accidents, often due to human errors. As we advance towards higher levels of vehicle automation, challenges still exist, as driving with automation can cognitively over-demand drivers if they engage in non-driving-related tasks (NDRTs), or lead to drowsiness if driving was the sole task. This calls for the urgent need for an effective Driver Monitoring System (DMS) that can evaluate cognitive load and drowsiness in SAE Level-2/3 autonomous driving contexts. In this study, we propose a novel multi-task DMS, termed VDMoE, which leverages RGB video input to monitor driver states non-invasively. By utilizing key facial features to minimize computational load and integrating remote Photoplethysmography (rPPG) for physiological insights, our approach enhances detection accuracy while maintaining efficiency. Additionally, we optimize the Mixture-of-Experts (MoE) framework to accommodate multi-modal inputs and improve performance across different tasks. A novel prior-inclusive regularization method is introduced to align model outputs with statistical priors, thus accelerating convergence and mitigating overfitting risks. We validate our method with the creation of a new dataset (MCDD), which comprises RGB video and physiological indicators from 42 participants, and two public datasets. Our findings demonstrate the effectiveness of VDMoE in monitoring driver states, contributing to safer autonomous driving systems. The code and data will be released.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system",
      "titleJa": "Falcon: feedback-driven adaptive long/short-term memory reinforced coding optimization system",
      "link": "https://arxiv.org/abs/2410.21349",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.21349v5 Announce Type: replace-cross \nAbstract: Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at https://github.com/titurte/FALCON.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation",
      "titleJa": "Incivility and rigidity: the risks of fine-tuning llms for political argumentation",
      "link": "https://arxiv.org/abs/2411.16813",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.16813v3 Announce Type: replace-cross \nAbstract: The incivility prevalent on platforms like Twitter (now X) and Reddit poses a challenge for developing AI systems that can support productive and rhetorically sound political argumentation. In this study, we report experiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of political discussions: high-variance, high-incivility Twitter replies to U.S. Congress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView. We systematically evaluate how these data sources and prompting strategies shape the rhetorical framing and deliberative quality of model-generated arguments. Our results show that Reddit-finetuned models produce safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies toxicity. Prompting reduces specific toxic behaviors, such as personal attacks, but fails to fully mitigate the influence of high-incivility training data. We introduce and validate a rhetorical evaluation rubric and provide practical guidelines for deploying LLMs in content authoring, moderation, and deliberation support.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting",
      "titleJa": "Fdllm: a dedicated detector for black-box llms fingerprinting",
      "link": "https://arxiv.org/abs/2501.16029",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.16029v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) are rapidly transforming the landscape of digital content creation. However, the prevalent black-box Application Programming Interface (API) access to many LLMs introduces significant challenges in accountability, governance, and security. LLM fingerprinting, which aims to identify the source model by analyzing statistical and stylistic features of generated text, offers a potential solution. Current progress in this area is hindered by a lack of dedicated datasets and the need for efficient, practical methods that are robust against adversarial manipulations. To address these challenges, we introduce FD-Dataset, a comprehensive bilingual fingerprinting benchmark comprising 90,000 text samples from 20 famous proprietary and open-source LLMs. Furthermore, we present FDLLM, a novel fingerprinting method that leverages parameter-efficient Low-Rank Adaptation (LoRA) to fine-tune a foundation model. This approach enables LoRA to extract deep, persistent features that characterize each source LLM. Through our analysis, we find that LoRA adaptation promotes the aggregation of outputs from the same LLM in representation space while enhancing the separation between different LLMs. This mechanism explains why LoRA proves particularly effective for LLM fingerprinting. Extensive empirical evaluations on FD-Dataset demonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than the strongest baseline. FDLLM also exhibits strong generalization to newly released models, achieving an average accuracy of 95% on unseen models. Notably, FDLLM remains consistently robust under various adversarial attacks, including polishing, translation, and synonym substitution. Experimental results show that FDLLM reduces the average attack success rate from 49.2% (LM-D) to 23.9%.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response",
      "titleJa": "Frida to the rescue! analyzing synthetic data effectiveness in object-based common sense reasoning for disaster response",
      "link": "https://arxiv.org/abs/2502.18452",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.18452v2 Announce Type: replace-cross \nAbstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these capabilities are often found only in larger models, which are frequently not reasonable to deploy on robotic systems. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our customized evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
      "titleJa": "Aligndistil: token-level language モデル alignment as adaptive 政策 distillation",
      "link": "https://arxiv.org/abs/2503.02832",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.02832v2 Announce Type: replace-cross \nAbstract: In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
      "titleJa": "Can we detect failures without failure data? uncertainty-aware runtime failure detection for imitation learning policies",
      "link": "https://arxiv.org/abs/2503.08558",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.08558v3 Announce Type: replace-cross \nAbstract: Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models",
      "titleJa": "Tale: a tool-augmented framework for reference-free evaluation of large language models",
      "link": "https://arxiv.org/abs/2504.07385",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.07385v2 Announce Type: replace-cross \nAbstract: As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement",
      "titleJa": "Calibrating pre-trained language classifiers on LLM-generated noisy labels via iterative refinement",
      "link": "https://arxiv.org/abs/2505.19675",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.19675v2 Announce Type: replace-cross \nAbstract: The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
      "titleJa": "Kvcache cache in the wild: characterizing and optimizing kvcache cache at a large クラウド provider",
      "link": "https://arxiv.org/abs/2506.02634",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.02634v3 Announce Type: replace-cross \nAbstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code",
      "titleJa": "Safegenbench: a benchmark framework for セキュリティ vulnerability detection in LLM-generated code",
      "link": "https://arxiv.org/abs/2506.05692",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.05692v3 Announce Type: replace-cross \nAbstract: The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes",
      "titleJa": "Sde-sql: enhancing text-to-sql generation in large language models via self-driven exploration with sql probes",
      "link": "https://arxiv.org/abs/2506.07245",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.07245v2 Announce Type: replace-cross \nAbstract: Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "PlantBert: An Open Source Language Model for Plant Science",
      "titleJa": "Plantbert: an オープンソース language モデル for plant science",
      "link": "https://arxiv.org/abs/2506.08897",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.08897v2 Announce Type: replace-cross \nAbstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving",
      "titleJa": "Using language and road manuals to inform map reconstruction for 自律的 driving",
      "link": "https://arxiv.org/abs/2506.10317",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.10317v2 Announce Type: replace-cross \nAbstract: Lane-topology prediction is a critical component of safe and reliable autonomous navigation. An accurate understanding of the road environment aids this task. We observe that this information often follows conventions encoded in natural language, through design codes that reflect the road structure and road names that capture the road functionality. We augment this information in a lightweight manner to SMERF, a map-prior-based online lane-topology prediction model, by combining structured road metadata from OSM maps and lane-width priors from Road design manuals with the road centerline encodings. We evaluate our method on two geo-diverse complex intersection scenarios. Our method shows improvement in both lane and traffic element detection and their association. We report results using four topology-aware metrics to comprehensively assess the model performance. These results demonstrate the ability of our approach to generalize and scale to diverse topologies and conditions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI",
      "titleJa": "The memory paradox: why our brains need knowledge in an age of ai",
      "link": "https://arxiv.org/abs/2506.11015",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11015v2 Announce Type: replace-cross \nAbstract: In the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory -- systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as \"grokking\" and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models -- biological \"schemata\" and neural manifolds -- that enable users to evaluate, refine, and guide AI output. The paper concludes with policy implications for education and workforce training in the age of large language models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Autonomous Computer Vision Development with Agentic AI",
      "titleJa": "自律的 コンピュータビジョン development with agentic ai",
      "link": "https://arxiv.org/abs/2506.11140",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11140v3 Announce Type: replace-cross \nAbstract: Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, \"provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)\"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment",
      "titleJa": "Learnalign: reasoning data selection for 強化学習 in large language models based on improved gradient alignment",
      "link": "https://arxiv.org/abs/2506.11480",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11480v2 Announce Type: replace-cross \nAbstract: Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data selection. To facilitate future work, we will release code.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration",
      "titleJa": "Sp-vla: a joint モデル scheduling and token pruning approach for vla モデル acceleration",
      "link": "https://arxiv.org/abs/2506.12723",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12723v2 Announce Type: replace-cross \nAbstract: Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Experimental results demonstrate that our method achieves up to 1.5$\\times$ acceleration with less than 3% drop in accuracy, outperforming existing approaches in multiple tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
      "titleJa": "Leverb: humanoid whole-body control with latent vision-language instruction",
      "link": "https://arxiv.org/abs/2506.13751",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13751v2 Announce Type: replace-cross \nAbstract: Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action \"vocabulary\" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning",
      "titleJa": "Lazyeviction: lagged kv eviction with attention pattern observation for efficient long reasoning",
      "link": "https://arxiv.org/abs/2506.15969",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15969v1 Announce Type: new \nAbstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by employing Chain-of-Thought (CoT). However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache size, particularly in tasks requiring long reasoning sequences, such as mathematics and programming. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens receive renewed attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, a lagged KV eviction framework designed to maintain reasoning performance while reducing KV memory. LazyEviction is an Observation Window-based Lagged Eviction Mechanism retaining latent recurring tokens by performing lagged evictions across decoding steps, which contains two key components: (1) Recurrence Interval Tracking for capturing temporal variations in token importance, and (2) an Maximum Recurrence Interval-Centric Eviction Policy that prioritizes eviction based on tokens' recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache size by 50% while maintaining comparable accuracy on mathematics reasoning datasets, outperforming state-of-the-art methods. Our findings highlight the importance of preserving recurring tokens, which are critical for maintaining knowledge continuity in multi-step reasoning tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation",
      "titleJa": "Quantum fisher-preconditioned 強化学習: from single-qubit control to rayleigh-fading link adaptation",
      "link": "https://arxiv.org/abs/2506.15753",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15753v1 Announce Type: cross \nAbstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a natural gradient-based algorithm for link adaptation that whitens policy updates using the full inverse quantum Fisher information with Tikhonov regularization. QPPG bridges classical and quantum geometry, achieving stable learning even under noise. Evaluated on classical and quantum environments, including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG converges 4 times faster than REINFORCE and sustains a 1 dB gain under uncertainty. It reaches a 90 percent return in one hundred episodes with high noise robustness, showcasing the advantages of full QFI-based preconditioning for scalable quantum reinforcement learning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion",
      "titleJa": "Investigating lagrangian neural networks for infinite horizon planning in quadrupedal locomotion",
      "link": "https://arxiv.org/abs/2506.16079",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16079v1 Announce Type: cross \nAbstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable framework for learning the system dynamics by utilizing inductive biases. While traditional dynamics models struggle with compounding errors over long horizons, LNNs intrinsically preserve the physical laws governing any system, enabling accurate and stable predictions essential for sustainable locomotion. This work evaluates LNNs for infinite horizon planning in quadrupedal robots through four dynamics models: (1) full-order forward dynamics (FD) training and inference, (2) diagonalized representation of Mass Matrix in full order FD, (3) full-order inverse dynamics (ID) training with FD inference, (4) reduced-order modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that LNNs bring improvements in sample efficiency (10x) and superior prediction accuracy (up to 2-10x) compared to baseline methods. Notably, the diagonalization approach of LNNs reduces computational complexity while retaining some interpretability, enabling real-time receding horizon control. These findings highlight the advantages of LNNs in capturing the underlying structure of system dynamics in quadrupeds, leading to improved performance and efficiency in locomotion planning and control. Additionally, our approach achieves a higher control frequency than previous LNN methods, demonstrating its potential for real-world deployment on quadrupeds.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Solving Zero-Sum Convex Markov Games",
      "titleJa": "Solving zero-sum convex markov games",
      "link": "https://arxiv.org/abs/2506.16120",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16120v1 Announce Type: cross \nAbstract: We contribute the first provable guarantees of global convergence to Nash equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using independent policy gradient methods. Convex Markov games, recently defined by Gemp et al. (2024), extend Markov decision processes to multi-agent settings with preferences that are convex over occupancy measures, offering a broad framework for modeling generic strategic interactions. However, even the fundamental min-max case of cMGs presents significant challenges, including inherent nonconvexity, the absence of Bellman consistency, and the complexity of the infinite horizon.\n  We follow a two-step approach. First, leveraging properties of hidden-convex--hidden-concave functions, we show that a simple nonconvex regularization transforms the min-max optimization problem into a nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this regularization can stabilize the iterates of independent policy gradient methods and ultimately lead them to converge to equilibria. Second, building on this reduction, we address the general constrained min-max problems under NC-pPL and two-sided pPL conditions, providing the first global convergence guarantees for stochastic nested and alternating gradient descent-ascent methods, which we believe may be of independent interest.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework",
      "titleJa": "When does divide and conquer work for long context LLM? a noise decomposition framework",
      "link": "https://arxiv.org/abs/2506.16411",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16411v1 Announce Type: cross \nAbstract: We investigate the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, we analyze when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Our experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, we also explain why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, we present a principled understanding framework and our results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection",
      "titleJa": "Improvement of nuclide detection through graph spectroscopic analysis framework and its application to nuclear facility upset detection",
      "link": "https://arxiv.org/abs/2506.16522",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16522v1 Announce Type: cross \nAbstract: We present a method to improve the detection limit for radionuclides using spectroscopic radiation detectors and the arrival time of each detected radiation quantum. We enable this method using a neural network with an attention mechanism. We illustrate the method on the detection of Cesium release from a nuclear facility during an upset, and our method shows $2\\times$ improvement over the traditional spectroscopic method. We hypothesize that our method achieves this performance increase by modulating its detection probability by the overall rate of probable detections, specifically by adapting detection thresholds based on temporal event distributions and local spectral features, and show evidence to this effect. We believe this method is applicable broadly and may be more successful for radionuclides with more complicated decay chains than Cesium; we also note that our method can generalize beyond the addition of arrival time and could integrate other data about each detection event, such as pulse quality, location in detector, or even combining the energy and time from detections in different detectors.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity",
      "titleJa": "Codediffuser: attention-enhanced diffusion 政策 via vlm-generated code for instruction ambiguity",
      "link": "https://arxiv.org/abs/2506.16652",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16652v1 Announce Type: cross \nAbstract: Natural language instructions for robotic manipulation tasks often exhibit ambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug tree\" may involve multiple valid actions if there are several mugs and branches to choose from. Existing language-conditioned policies typically rely on end-to-end models that jointly handle high-level semantic understanding and low-level action generation, which can result in suboptimal performance due to their lack of modularity and interpretability. To address these challenges, we introduce a novel robotic manipulation framework that can accomplish tasks specified by potentially ambiguous natural language. This framework employs a Vision-Language Model (VLM) to interpret abstract concepts in natural language instructions and generates task-specific code - an interpretable and executable intermediate representation. The generated code interfaces with the perception module to produce 3D attention maps that highlight task-relevant regions by integrating spatial and semantic information, effectively resolving ambiguities in instructions. Through extensive experiments, we identify key limitations of current imitation learning methods, such as poor adaptation to language and environmental variations. We show that our approach excels across challenging manipulation tasks involving language ambiguity, contact-rich manipulation, and multi-object interactions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning",
      "titleJa": "Boltzmann classifier: a thermodynamic-inspired approach to supervised learning",
      "link": "https://arxiv.org/abs/2505.06753",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.06753v2 Announce Type: replace \nAbstract: We present the Boltzmann classifier, a novel distance based probabilistic classification algorithm inspired by the Boltzmann distribution. Unlike traditional classifiers that produce hard decisions or uncalibrated probabilities, the Boltzmann classifier assigns class probabilities based on the average distance to the nearest neighbors within each class, providing interpretable, physically meaningful outputs. We evaluate the performance of the method across three application domains: molecular activity prediction, oxidation state classification of transition metal complexes, and breast cancer diagnosis. In the molecular activity task, the classifier achieved the highest accuracy in predicting active compounds against two protein targets, with strong correlations observed between the predicted probabilities and experimental pIC50 values. For metal complexes, the classifier accurately distinguished between oxidation states II and III for Fe, Mn, and Co, using only metal-ligand bond lengths extracted from crystallographic data, and demonstrated high consistency with known chemical trends. In the breast cancer dataset, the classifier achieved 97% accuracy, with low confidence predictions concentrated in inherently ambiguous cases. Across all tasks, the Boltzmann classifier performed competitively or better than standard models such as logistic regression, support vector machines, random forests, and k-nearest neighbors. Its probabilistic outputs were found to correlate with continuous physical or biological properties, highlighting its potential utility in both classification and regression contexts. The results suggest that the Boltzmann classifier is a robust and interpretable alternative to conventional machine learning approaches, particularly in scientific domains where underlying structure property relationships are important.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening",
      "titleJa": "Rewarding the unlikely: lifting grpo beyond distribution sharpening",
      "link": "https://arxiv.org/abs/2506.02355",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.02355v2 Announce Type: replace \nAbstract: Reinforcement learning is emerging as a primary driver for improving language model reasoning capabilities. A fundamental question is whether current reinforcement learning algorithms -- such as Group Relative Policy Optimization (GRPO), the de facto standard algorithm used to improve language model reasoning -- merely sharpen the base model's distribution around problems it can already solve. We investigate this question in the context of formal theorem proving, which has access to a perfect verifier. We identify a degenerate rank bias in GRPO in which highly probable trajectories are reinforced and rare ones are neglected. This results in distribution sharpening: the model can solve some problems with fewer samples, but underperforms simply sampling more solutions from the original model. To overcome GRPO's rank bias we introduce unlikeliness reward, a simple method for explicitly up-weighting rare but correct solutions. We show that unlikeliness reward mitigates rank bias and improves pass@$N$ across a large range of $N$ in both synthetic and real theorem proving settings. We also uncover an unexpected link between rank bias and a seemingly mundane hyperparameter -- the number of updates per batch -- that leads to a second, complementary mitigation. We combine our insights into a revised GRPO training recipe for formal theorem proving, yielding an open pipeline that achieves competitive performance to DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation at https://github.com/AndreHe02/rewarding-unlikely-release",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Safe Guaranteed Exploration for Non-linear Systems",
      "titleJa": "Safe guaranteed exploration for non-linear systems",
      "link": "https://arxiv.org/abs/2402.06562",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2402.06562v2 Announce Type: replace-cross \nAbstract: Safely exploring environments with a-priori unknown constraints is a fundamental challenge that restricts the autonomy of robots. While safety is paramount, guarantees on sufficient exploration are also crucial for ensuring autonomous task completion. To address these challenges, we propose a novel safe guaranteed exploration framework using optimal control, which achieves first-of-its-kind results: guaranteed exploration for non-linear systems with finite time sample complexity bounds, while being provably safe with arbitrarily high probability. The framework is general and applicable to many real-world scenarios with complex non-linear dynamics and unknown domains. We improve the efficiency of this general framework by proposing an algorithm, SageMPC, SAfe Guaranteed Exploration using Model Predictive Control. SageMPC leverages three key techniques: i) exploiting a Lipschitz bound, ii) goal-directed exploration, and iii) receding horizon style re-planning, all while maintaining the desired sample complexity, safety and exploration guarantees of the framework. Lastly, we demonstrate safe efficient exploration in challenging unknown environments using SageMPC with a car model.",
      "summary": "arXiv:2402.",
      "summaryJa": "Arxiv:2402.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative Recommender",
      "titleJa": "Hstu-blair: lightweight contrastive text embedding for generative recommender",
      "link": "https://arxiv.org/abs/2504.10545",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.10545v3 Announce Type: replace-cross \nAbstract: Recent advances in recommender systems have underscored the complementary strengths of generative modeling and pretrained language models. We propose HSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.\n  We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the Amazon Reviews 2023 dataset and the Steam dataset. We compare its performance against both the original HSTU-based recommender and a variant augmented with embeddings from OpenAI's state-of-the-art \\texttt{text-embedding-3-large} model. Despite the latter being trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- achieves better performance in nearly all cases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant on all but one metric, where it is marginally lower, and matches it on another. These findings highlight the effectiveness of contrastive text embeddings in compute-efficient recommendation settings.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning",
      "titleJa": "Reinflow: fine-tuning flow matching 政策 with online 強化学習",
      "link": "https://arxiv.org/abs/2505.22094",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.22094v5 Announce Type: replace-cross \nAbstract: We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy's deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project webpage: https://reinflow.github.io/",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 80
    },
    {
      "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning",
      "titleJa": "Compiler-r1: towards agentic compiler auto-tuning with 強化学習",
      "link": "https://arxiv.org/abs/2506.15701",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15701v1 Announce Type: cross \nAbstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction",
      "titleJa": "Monetv2: enhanced motion network for freehand 3d ultrasound reconstruction",
      "link": "https://arxiv.org/abs/2506.15835",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15835v1 Announce Type: cross \nAbstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the spatial relationships of anatomical structures, playing a crucial role in clinical diagnosis. Recently, deep-learning-based freehand 3D US has made significant advancements. It reconstructs volumes by estimating transformations between images without external tracking. However, image-only reconstruction poses difficulties in reducing cumulative drift and further improving reconstruction accuracy, particularly in scenarios involving complex motion trajectories. In this context, we propose an enhanced motion network (MoNetV2) to enhance the accuracy and generalizability of reconstruction under diverse scanning velocities and tactics. First, we propose a sensor-based temporal and multi-branch structure that fuses image and motion information from a velocity perspective to improve image-only reconstruction accuracy. Second, we devise an online multi-level consistency constraint that exploits the inherent consistency of scans to handle various scanning velocities and tactics. This constraint exploits both scan-level velocity consistency, path-level appearance consistency, and patch-level motion consistency to supervise inter-frame transformation estimation. Third, we distill an online multi-modal self-supervised strategy that leverages the correlation between network estimation and motion information to further reduce cumulative errors. Extensive experiments clearly demonstrate that MoNetV2 surpasses existing methods in both reconstruction quality and generalizability performance across three large datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
      "titleJa": "Mem1: learning to synergize memory and reasoning for efficient long-horizon agents",
      "link": "https://arxiv.org/abs/2506.15841",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15841v1 Announce Type: cross \nAbstract: Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "A Technical Study into 0.5B Reasoning Language Models",
      "titleJa": "A technical study into 0.5b reasoning language models",
      "link": "https://arxiv.org/abs/2506.13404",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13404v2 Announce Type: replace \nAbstract: The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
      "titleJa": "Xgen-mm (blip-3): a family of open large multimodal models",
      "link": "https://arxiv.org/abs/2408.08872",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.08872v3 Announce Type: replace-cross \nAbstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models",
      "titleJa": "V2x-vlm: end-to-end v2x cooperative 自律的 driving through large vision-language models",
      "link": "https://arxiv.org/abs/2408.09251",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.09251v3 Announce Type: replace-cross \nAbstract: Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "ShapeLib: Designing a library of programmatic 3D shape abstractions with Large Language Models",
      "titleJa": "Shapelib: designing a library of programmatic 3d shape abstractions with large language models",
      "link": "https://arxiv.org/abs/2502.08884",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.08884v2 Announce Type: replace-cross \nAbstract: We present ShapeLib, the first method that leverages the priors of LLMs to design libraries of programmatic 3D shape abstractions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover abstractions that match this design intent with a guided LLM workflow that first proposes, and then validates, different ways of applying and implementing functions. We learn recognition networks that map shapes to programs with these newly discovered abstractions by training on data produced by LLM authored synthetic data generation procedures. Across modeling domains (split by shape category), we find that LLMs, when thoughtfully combined with geometric reasoning, can be guided to author a library of abstraction functions that generalize to shapes outside of the seed set. This framework addresses a long-standing shape analysis problem of how to discover reusable abstraction functions while exposing interpretable, semantically aligned interfaces. We find that ShapeLib provides distinct advantages over prior alternative abstraction discovery works in terms of generalization, usability, and maintaining plausibility under manipulation. Finally, we demonstrate that ShapeLib's abstraction functions unlock a number of downstream applications, combining LLM reasoning over shape programs with geometry processing to support shape editing and generation.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets",
      "titleJa": "Robo2vlm: visual question answering from large-scale in-the-wild robot manipulation datasets",
      "link": "https://arxiv.org/abs/2505.15517",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.15517v2 Announce Type: replace-cross \nAbstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning",
      "titleJa": "Med-u1: incentivizing unified 医療 reasoning in llms via large-scale 強化学習",
      "link": "https://arxiv.org/abs/2506.12307",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12307v2 Announce Type: replace-cross \nAbstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. Our code is available here.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "Serving Large Language Models on Huawei CloudMatrix384",
      "titleJa": "Serving large language models on huawei cloudmatrix384",
      "link": "https://arxiv.org/abs/2506.12708",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12708v3 Announce Type: replace-cross \nAbstract: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "S$^2$GPT-PINNs: Sparse and Small models for PDEs",
      "titleJa": "S$^2$GPT-pinns: sparse and small models for pdes",
      "link": "https://arxiv.org/abs/2506.15687",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15687v1 Announce Type: new \nAbstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric partial differential equations (PDEs). Similar to Small Language Models (SLMs), S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and characterized by its compact architecture and minimal computational power. Leveraging a small amount of extremely high quality data via a mathematically rigorous greedy algorithm that is enabled by the large full-order models, S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to achieve extremely high efficiency via two levels of customizations. The first is knowledge distillation via task-specific activation functions that are transferred from Pre-Trained PINNs. The second is a judicious down-sampling when calculating the physics-informed loss of the network compressing the number of data sites by orders of magnitude to the size of the small model.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata",
      "titleJa": "Buildingbrep-11k: precise multi-storey b-rep building solids with rich layout metadata",
      "link": "https://arxiv.org/abs/2506.15718",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15718v1 Announce Type: new \nAbstract: With the rise of artificial intelligence, the automatic generation of building-scale 3-D objects has become an active research topic, yet training such models still demands large, clean and richly annotated datasets. We introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors) buildings (about 10 GB) produced by a shape-grammar-driven pipeline that encodes established building-design principles. Every sample consists of a geometrically exact B-rep solid-covering floors, walls, slabs and rule-based openings-together with a fast-loading .npy metadata file that records detailed per-floor parameters. The generator incorporates constraints on spatial scale, daylight optimisation and interior layout, and the resulting objects pass multi-stage filters that remove Boolean failures, undersized rooms and extreme aspect ratios, ensuring compliance with architectural standards. To verify the dataset's learnability we trained two lightweight PointNet baselines. (i) Multi-attribute regression. A single encoder predicts storey count, total rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100 unseen buildings it attains 0.37-storey MAE (87 \\% within $\\pm1$), 5.7-room MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same backbone we classify GOOD versus DEFECT; on a balanced 100-model set the network reaches 54 \\% accuracy, recalling 82 \\% of true defects at 53 \\% precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K is learnable yet non-trivial for both geometric regression and topological quality assessment",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU",
      "titleJa": "Mr. snuffleupagus at semeval-2025 task 4: unlearning factual knowledge from llms using adaptive rmu",
      "link": "https://arxiv.org/abs/2506.16548",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16548v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, their tendency to memorize training data raises concerns regarding privacy, copyright compliance, and security, particularly in cases involving Personally Identifiable Information (PII). Effective machine unlearning techniques are essential to mitigate these risks, yet existing methods remain underdeveloped for LLMs due to their open-ended output space. In this work, we apply the Adaptive Representation Misdirection Unlearning (RMU) technique to unlearn sensitive information from LLMs. Through extensive experiments, we analyze the effects of unlearning across different decoder layers to determine the most effective regions for sensitive information removal. Our technique ranked 4th on the official leaderboard of both 1B parameter and 7B parameter models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks",
      "titleJa": "Deeprtl2: a versatile モデル for rtl-related tasks",
      "link": "https://arxiv.org/abs/2506.15697",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15697v1 Announce Type: cross \nAbstract: The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code generation and understanding. While previous studies have demonstrated the efficacy of fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally critical to EDA workflows, have been largely overlooked. These tasks, including natural language code search, RTL code functionality equivalence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present DeepRTL2, a family of versatile LLMs that unifies both generation- and embedding-based tasks related to RTL. By simultaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse challenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the-art performance across all evaluated tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "Joint Optimization of Age of Information and Energy Consumption in NR-V2X System based on Deep Reinforcement Learning",
      "titleJa": "Joint optimization of age of information and energy consumption in nr-v2x system based on deep 強化学習",
      "link": "https://arxiv.org/abs/2407.08458",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2407.08458v2 Announce Type: replace \nAbstract: Autonomous driving may be the most important application scenario of next generation, the development of wireless access technologies enabling reliable and low-latency vehicle communication becomes crucial. To address this, 3GPP has developed Vehicle-to-Everything (V2X) specifications based on 5G New Radio (NR) technology, where Mode 2 Side-Link (SL) communication resembles Mode 4 in LTE-V2X, allowing direct communication between vehicles. This supplements SL communication in LTE-V2X and represents the latest advancement in cellular V2X (C-V2X) with improved performance of NR-V2X. However, in NR-V2X Mode 2, resource collisions still occur, and thus degrade the age of information (AOI). Therefore, a interference cancellation method is employed to mitigate this impact by combining NR-V2X with Non-Orthogonal multiple access (NOMA) technology. In NR-V2X, when vehicles select smaller resource reservation interval (RRI), higher-frequency transmissions take ore energy to reduce AoI. Hence, it is important to jointly consider AoI and communication energy consumption based on NR-V2X communication. Then, we formulate such an optimization problem and employ the Deep Reinforcement Learning (DRL) algorithm to compute the optimal transmission RRI and transmission power for each transmitting vehicle to reduce the energy consumption of each transmitting vehicle and the AoI of each receiving vehicle. Extensive simulations have demonstrated the performance of our proposed algorithm.",
      "summary": "arXiv:2407.",
      "summaryJa": "Arxiv:2407.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments",
      "titleJa": "Shakti: a 2.5 billion parameter small language モデル optimized for edge ai and low-resource environments",
      "link": "https://arxiv.org/abs/2410.11331",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.11331v2 Announce Type: replace-cross \nAbstract: We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 75
    },
    {
      "title": "LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge",
      "titleJa": "Llms struggle to perform counterfactual reasoning with parametric knowledge",
      "link": "https://arxiv.org/abs/2506.15732",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15732v1 Announce Type: new \nAbstract: Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information. In this work, we explore whether LLMs can combine knowledge in-context with their parametric knowledge through the lens of counterfactual reasoning. Through synthetic and real experiments in multi-hop reasoning problems, we show that LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. Moreover, we show that simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge. Ultimately, our work reveals important limitations of current LLM's abilities to re-purpose parametric knowledge in novel settings.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
      "titleJa": "$\\texttt{specs}$: faster test-time scaling through speculative drafts",
      "link": "https://arxiv.org/abs/2506.15733",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15733v1 Announce Type: new \nAbstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models",
      "titleJa": "The safety reminder: a soft prompt to reactivate delayed safety awareness in vision-language models",
      "link": "https://arxiv.org/abs/2506.15734",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15734v1 Announce Type: new \nAbstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "ContextBench: Modifying Contexts for Targeted Latent Activation",
      "titleJa": "Contextbench: modifying contexts for targeted latent activation",
      "link": "https://arxiv.org/abs/2506.15735",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15735v1 Announce Type: new \nAbstract: Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts",
      "titleJa": "Sysformer: safeguarding frozen large language models with adaptive system prompts",
      "link": "https://arxiv.org/abs/2506.15751",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15751v1 Announce Type: new \nAbstract: As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\\textbf{Sysformer}$, a trans$\\textbf{former}$ model that updates an initial $\\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search",
      "titleJa": "Deep 強化学習 xiangqi player with monte carlo tree search",
      "link": "https://arxiv.org/abs/2506.15880",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15880v1 Announce Type: new \nAbstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning",
      "titleJa": "Bayesian epistemology with weighted authority: a formal architecture for truth-promoting 自律的 scientific reasoning",
      "link": "https://arxiv.org/abs/2506.16015",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16015v1 Announce Type: new \nAbstract: The exponential expansion of scientific literature has surpassed the epistemic processing capabilities of both human experts and current artificial intelligence systems. This paper introduces Bayesian Epistemology with Weighted Authority (BEWA), a formally structured architecture that operationalises belief as a dynamic, probabilistically coherent function over structured scientific claims. Each claim is contextualised, author-attributed, and evaluated through a system of replication scores, citation weighting, and temporal decay. Belief updates are performed via evidence-conditioned Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports graph-based claim propagation, authorial credibility modelling, cryptographic anchoring, and zero-knowledge audit verification. By formalising scientific reasoning into a computationally verifiable epistemic network, BEWA advances the foundation for machine reasoning systems that promote truth utility, rational belief convergence, and audit-resilient integrity across dynamic scientific domains.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior",
      "titleJa": "Large language models are near-optimal decision-makers with a non-human learning behavior",
      "link": "https://arxiv.org/abs/2506.16163",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16163v1 Announce Type: new \nAbstract: Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Agentic Personalisation of Cross-Channel Marketing Experiences",
      "titleJa": "Agentic personalisation of cross-channel marketing experiences",
      "link": "https://arxiv.org/abs/2506.16429",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16429v1 Announce Type: new \nAbstract: Consumer applications provide ample opportunities to surface and communicate various forms of content to users. From promotional campaigns for new features or subscriptions, to evergreen nudges for engagement, or personalised recommendations; across e-mails, push notifications, and in-app surfaces. The conventional approach to orchestration for communication relies heavily on labour-intensive manual marketer work, and inhibits effective personalisation of content, timing, frequency, and copy-writing. We formulate this task under a sequential decision-making framework, where we aim to optimise a modular decision-making policy that maximises incremental engagement for any funnel event. Our approach leverages a Difference-in-Differences design for Individual Treatment Effect estimation, and Thompson sampling to balance the explore-exploit trade-off. We present results from a multi-service application, where our methodology has resulted in significant increases to a variety of goal events across several product features, and is currently deployed across 150 million users.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning",
      "titleJa": "Ml-master: towards ai-for-ai via integration of exploration and reasoning",
      "link": "https://arxiv.org/abs/2506.16499",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16499v1 Announce Type: new \nAbstract: As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System",
      "titleJa": "Advancing harmful content detection in organizational 研究: integrating large language models with elo rating system",
      "link": "https://arxiv.org/abs/2506.16575",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16575v1 Announce Type: new \nAbstract: Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Community-driven vision for a new Knowledge Resource for AI",
      "titleJa": "A community-driven vision for a new knowledge resource for ai",
      "link": "https://arxiv.org/abs/2506.16596",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16596v1 Announce Type: new \nAbstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Elevating Styled Mahjong Agents with Learning from Demonstration",
      "titleJa": "Elevating styled mahjong agents with learning from demonstration",
      "link": "https://arxiv.org/abs/2506.16995",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16995v1 Announce Type: new \nAbstract: A wide variety of bots in games enriches the gameplay experience and enhances replayability. Recent advancements in game artificial intelligence have predominantly focused on improving the proficiency of bots. Nevertheless, developing highly competent bots with a wide range of distinct play styles remains a relatively under-explored area. We select the Mahjong game environment as a case study. The high degree of randomness inherent in the Mahjong game and the prevalence of out-of-distribution states lead to suboptimal performance of existing offline learning and Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the gameplay histories of existing Mahjong agents and put forward a novel LfD algorithm that necessitates only minimal modifications to the Proximal Policy Optimization algorithm. The comprehensive empirical results illustrate that our proposed method not only significantly enhances the proficiency of the agents but also effectively preserves their unique play styles.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving",
      "titleJa": "Towards advanced mathematical reasoning for llms via first-order logic theorem proving",
      "link": "https://arxiv.org/abs/2506.17104",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17104v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models",
      "titleJa": "Mathematical proof as a litmus test: revealing failure modes of advanced large reasoning models",
      "link": "https://arxiv.org/abs/2506.17114",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17114v1 Announce Type: new \nAbstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Ignition Phase : Standard Training for Fast Adversarial Robustness",
      "titleJa": "Ignition phase : standard 学習 for fast adversarial robustness",
      "link": "https://arxiv.org/abs/2506.15685",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15685v1 Announce Type: cross \nAbstract: Adversarial Training (AT) is a cornerstone defense, but many variants overlook foundational feature representations by primarily focusing on stronger attack generation. We introduce Adversarial Evolution Training (AET), a simple yet powerful framework that strategically prepends an Empirical Risk Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM phase cultivates a favorable feature manifold, enabling more efficient and effective robustness acquisition. Empirically, AET achieves comparable or superior robustness more rapidly, improves clean accuracy, and cuts training costs by 8-25\\%. Its effectiveness is shown across multiple datasets, architectures, and when augmenting established AT methods. Our findings underscore the impact of feature pre-conditioning via standard training for developing more efficient, principled robust defenses. Code is available in the supplementary material.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs",
      "titleJa": "LLM web dynamics: tracing モデル collapse in a network of llms",
      "link": "https://arxiv.org/abs/2506.15690",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15690v1 Announce Type: cross \nAbstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "What Do Latent Action Models Actually Learn?",
      "titleJa": "What do latent action models actually learn?",
      "link": "https://arxiv.org/abs/2506.15691",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15691v1 Announce Type: cross \nAbstract: Latent action models (LAMs) aim to learn action-relevant changes from unlabeled videos by compressing changes between frames as latents. However, differences between video frames can be caused by controllable changes as well as exogenous noise, leading to an important concern -- do latents capture the changes caused by actions or irrelevant noise? This paper studies this issue analytically, presenting a linear model that encapsulates the essence of LAM learning, while being tractable.This provides several insights, including connections between LAM and principal component analysis (PCA), desiderata of the data-generating policy, and justification of strategies to encourage learning controllable changes using data augmentation, data cleaning, and auxiliary action-prediction. We also provide illustrative results based on numerical simulation, shedding light on the specific structure of observations, actions, and noise in data that influence LAM learning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap",
      "titleJa": "Blur: a benchmark for LLM unlearning robust to forget-retain overlap",
      "link": "https://arxiv.org/abs/2506.15699",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15699v1 Announce Type: cross \nAbstract: Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $\\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $\\texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $\\texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: https://huggingface.co/datasets/forgelab/BLUR",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding",
      "titleJa": "Learn from the past: fast sparse indexing for 大規模言語モデル decoding",
      "link": "https://arxiv.org/abs/2506.15704",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15704v1 Announce Type: cross \nAbstract: As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\\times$ speedup over full attention and 9.6$\\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models",
      "titleJa": "Generalisation bounds of zero-shot economic forecasting using time series foundation models",
      "link": "https://arxiv.org/abs/2506.15705",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15705v1 Announce Type: cross \nAbstract: This study investigates zero-shot forecasting capabilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to forecasting economic indicators under univariate conditions, bypassing the need for train bespoke econometric models using and extensive training datasets. Our experiments were conducted on a case study dataset, without additional customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos, TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our results demonstrate that appropriately engineered TSFMs can internalise rich economic dynamics, accommodate regime shifts, and deliver well-behaved uncertainty estimates out of the box, while matching state-of-the-art multivariate models on this domain. Our findings suggest that, without any fine-tuning, TSFMs can match or exceed classical models during stable economic conditions. However, they are vulnerable to degradation in performances during periods of rapid shocks. The findings offer guidance to practitioners on when zero-shot deployments are viable for macroeconomic monitoring and strategic planning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning",
      "titleJa": "Mdpo: multi-granularity direct preference optimization for mathematical reasoning",
      "link": "https://arxiv.org/abs/2506.15706",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15706v1 Announce Type: cross \nAbstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
      "titleJa": "Every rollout counts: optimal resource allocation for efficient test-time scaling",
      "link": "https://arxiv.org/abs/2506.15707",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15707v1 Announce Type: cross \nAbstract: Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies",
      "titleJa": "Alternates, assemble! selecting optimal alternates for citizens' assemblies",
      "link": "https://arxiv.org/abs/2506.15716",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15716v1 Announce Type: cross \nAbstract: An increasingly influential form of deliberative democracy centers on citizens' assemblies, where randomly selected people discuss policy questions. The legitimacy of these panels hinges on their representation of the broader population, but panelists often drop out, leading to an unbalanced composition. Although participant attrition is mitigated in practice by alternates, their selection is not taken into account by existing methods. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. We establish theoretical guarantees for our approach, including worst-case bounds on sample complexity (with implications for computational efficiency) and on loss when panelists' probabilities of dropping out are mis-estimated. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "daDPO: Distribution-Aware DPO for Distilling Conversational Abilities",
      "titleJa": "Dadpo: distribution-aware dpo for distilling conversational abilities",
      "link": "https://arxiv.org/abs/2506.15717",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15717v1 Announce Type: cross \nAbstract: Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation",
      "titleJa": "Unimate: a unified モデル for mechanical metamaterial generation, property prediction, and condition confirmation",
      "link": "https://arxiv.org/abs/2506.15722",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15722v1 Announce Type: cross \nAbstract: Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UNIMATE, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UNIMATE outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We opensource our proposed UNIMATE model and corresponding results at https://github.com/wzhan24/UniMate.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference",
      "titleJa": "Madakv: adaptive modality-perception kv cache eviction for efficient multimodal long-context 推論",
      "link": "https://arxiv.org/abs/2506.15724",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15724v1 Announce Type: cross \nAbstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture",
      "titleJa": "A study of hybrid and evolutionary metaheuristics for single hidden layer feedforward ニューラルネットワーク architecture",
      "link": "https://arxiv.org/abs/2506.15737",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15737v1 Announce Type: cross \nAbstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent (SGD) frequently encounters difficulties, including substantial computing expense and the risk of converging to local optima, attributable to its dependence on partial weight gradients. Therefore, this work investigates Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve local search efficiency. The findings indicate that the hybrid PSO-SGD technique decreases the median training MSE by 90 to 95 percent relative to conventional GA and PSO across various network sizes (e.g., from around 0.02 to approximately 0.001 in the Sphere function). RMHC attains substantial enhancements, reducing MSE by roughly 85 to 90 percent compared to GA. Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying subpar performance. These findings underscore that hybrid and evolutionary procedures significantly improve training efficiency and accuracy compared to conventional optimization methods and imply that the Building Block Hypothesis (BBH) may still be valid, indicating that advantageous weight structures are retained during evolutionary search.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Veracity: An Open-Source AI Fact-Checking System",
      "titleJa": "Veracity: an open-source ai fact-checking system",
      "link": "https://arxiv.org/abs/2506.15794",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15794v1 Announce Type: cross \nAbstract: The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images",
      "titleJa": "Cross-modality learning for predicting ihc biomarkers from h&e-stained whole-slide images",
      "link": "https://arxiv.org/abs/2506.15853",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15853v1 Announce Type: cross \nAbstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological analysis, offering reliable visualization of cellular morphology and tissue architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry (IHC) staining provides molecular insights by detecting specific proteins within tissues, enhancing diagnostic accuracy, and improving treatment planning. However, IHC staining is costly, time-consuming, and resource-intensive, requiring specialized expertise. To address these limitations, this study proposes HistoStainAlign, a novel deep learning framework that predicts IHC staining patterns directly from H&E whole-slide images (WSIs) by learning joint representations of morphological and molecular features. The framework integrates paired H&E and IHC embeddings through a contrastive training strategy, capturing complementary features across staining modalities without patch-level annotations or tissue registration. The model was evaluated on gastrointestinal and lung tissue WSIs with three commonly used IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI: 0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC stains. Embedding analyses demonstrated the robustness of the contrastive alignment in capturing meaningful cross-stain relationships. Comparisons with a baseline model further highlight the advantage of incorporating contrastive learning for improved stain pattern prediction. This study demonstrates the potential of computational approaches to serve as a pre-screening tool, helping prioritize cases for IHC staining and improving workflow efficiency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute",
      "titleJa": "Fractional reasoning via latent steering vectors improves 推論 time compute",
      "link": "https://arxiv.org/abs/2506.15882",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15882v1 Announce Type: cross \nAbstract: Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
      "titleJa": "Language models can perform single-utterance self-correction of perturbed reasoning",
      "link": "https://arxiv.org/abs/2506.15894",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15894v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent \"reasoning\" model work involves amplification of traits already meaningfully present in models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization",
      "titleJa": "Beyond audio and pose: a general-purpose framework for video synchronization",
      "link": "https://arxiv.org/abs/2506.15937",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15937v1 Announce Type: cross \nAbstract: Video synchronization-aligning multiple video streams capturing the same event from different angles-is crucial for applications such as reality TV show production, sports analysis, surveillance, and autonomous systems. Prior work has heavily relied on audio cues or specific visual events, limiting applicability in diverse settings where such signals may be unreliable or absent. Additionally, existing benchmarks for video synchronization lack generality and reproducibility, restricting progress in the field. In this work, we introduce VideoSync, a video synchronization framework that operates independently of specific feature extraction methods, such as human pose estimation, enabling broader applicability across different content types. We evaluate our system on newly composed datasets covering single-human, multi-human, and non-human scenarios, providing both the methodology and code for dataset creation to establish reproducible benchmarks. Our analysis reveals biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline, leading to inflated performance claims. We correct these biases and propose a more rigorous evaluation framework, demonstrating that VideoSync outperforms existing approaches, including SeSyn-Net, under fair experimental conditions. Additionally, we explore various synchronization offset prediction methods, identifying a convolutional neural network (CNN)-based model as the most effective. Our findings advance video synchronization beyond domain-specific constraints, making it more generalizable and robust for real-world applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction",
      "titleJa": "Autohformer: efficient hierarchical autoregressive トランスフォーマー for time series prediction",
      "link": "https://arxiv.org/abs/2506.16001",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16001v1 Announce Type: cross \nAbstract: Time series forecasting requires architectures that simultaneously achieve three competing objectives: (1) strict temporal causality for reliable predictions, (2) sub-quadratic complexity for practical scalability, and (3) multi-scale pattern recognition for accurate long-horizon forecasting. We introduce AutoHFormer, a hierarchical autoregressive transformer that addresses these challenges through three key innovations: 1) Hierarchical Temporal Modeling: Our architecture decomposes predictions into segment-level blocks processed in parallel, followed by intra-segment sequential refinement. This dual-scale approach maintains temporal coherence while enabling efficient computation. 2) Dynamic Windowed Attention: The attention mechanism employs learnable causal windows with exponential decay, reducing complexity while preserving precise temporal relationships. This design avoids both the anti-causal violations of standard transformers and the sequential bottlenecks of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system is adopted to capture time patterns at multiple scales. It combines fixed oscillating patterns for short-term variations with learnable decay rates for long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining consistent accuracy across 96-720 step horizons in most of cases. These breakthroughs establish new benchmarks for efficient and precise time series modeling. Implementations of our method and all baselines in hierarchical autoregressive mechanism are available at https://github.com/lizzyhku/Autotime.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "titleJa": "Evolm: in search of lost language モデル 学習 dynamics",
      "link": "https://arxiv.org/abs/2506.16029",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16029v1 Announce Type: cross \nAbstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling",
      "titleJa": "Dynscaling: efficient verifier-free 推論 scaling via dynamic and integrated sampling",
      "link": "https://arxiv.org/abs/2506.16043",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16043v1 Announce Type: cross \nAbstract: Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations",
      "titleJa": "Probing the robustness of large language models safety to latent perturbations",
      "link": "https://arxiv.org/abs/2506.16078",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16078v1 Announce Type: cross \nAbstract: Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks",
      "titleJa": "Gflowgr: fine-tuning generative recommendation frameworks with generative flow networks",
      "link": "https://arxiv.org/abs/2506.16114",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16114v1 Announce Type: cross \nAbstract: Generative recommendations (GR), which usually include item tokenizers and generative Large Language Models (LLMs), have demonstrated remarkable success across a wide range of scenarios. The majority of existing research efforts primarily concentrate on developing powerful item tokenizers or advancing LLM decoding strategies to attain superior performance. However, the critical fine-tuning step in GR frameworks, which is essential for adapting LLMs to recommendation data, remains largely unexplored. Current approaches predominantly rely on either the next-token prediction loss of supervised fine-tuning (SFT) or recommendationspecific direct preference optimization (DPO) strategies. Both methods ignore the exploration of possible positive unobserved samples, which is commonly referred to as the exposure bias problem. To mitigate this problem, this paper treats the GR as a multi-step generation task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The proposed framework integrates collaborative knowledge from traditional recommender systems to create an adaptive trajectory sampler and a comprehensive reward model. Leveraging the diverse generation property of GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR emerges as a promising approach to mitigate the exposure bias problem. Extensive empirical results on two real-world datasets and with two different GR backbones highlight the effectiveness and robustness of GFlowGR.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning",
      "titleJa": "Grpo-care: consistency-aware 強化学習 for multimodal reasoning",
      "link": "https://arxiv.org/abs/2506.16141",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16141v1 Announce Type: cross \nAbstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "PRISON: Unmasking the Criminal Potential of Large Language Models",
      "titleJa": "Prison: unmasking the criminal potential of large language models",
      "link": "https://arxiv.org/abs/2506.16150",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16150v1 Announce Type: cross \nAbstract: As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Under the Shadow of Babel: How Language Shapes Reasoning in LLMs",
      "titleJa": "Under the shadow of babel: how language shapes reasoning in llms",
      "link": "https://arxiv.org/abs/2506.16151",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16151v1 Announce Type: cross \nAbstract: Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "On using AI for EEG-based BCI applications: problems, current challenges and future trends",
      "titleJa": "On using ai for eeg-based bci applications: problems, current challenges and future trends",
      "link": "https://arxiv.org/abs/2506.16168",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16168v1 Announce Type: cross \nAbstract: Imagine unlocking the power of the mind to communicate, create, and even interact with the world around us. Recent breakthroughs in Artificial Intelligence (AI), especially in how machines \"see\" and \"understand\" language, are now fueling exciting progress in decoding brain signals from scalp electroencephalography (EEG). Prima facie, this opens the door to revolutionary brain-computer interfaces (BCIs) designed for real life, moving beyond traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a Brain-to-Internet of Things (BCIoT).\n  However, the journey is not as straightforward as it was for Computer Vision (CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based BCIs, particularly in building powerful foundational models, presents unique and intricate hurdles that could affect their reliability.\n  Here, we unfold a guided exploration of this dynamic and rapidly evolving research area. Rather than barely outlining a map of current endeavors and results, the goal is to provide a principled navigation of this hot and cutting-edge research landscape. We consider the basic paradigms that emerge from a causal perspective and the attendant challenges presented to AI-based models. Looking ahead, we then discuss promising research avenues that could overcome today's technological, methodological, and ethical limitations. Our aim is to lay out a clear roadmap for creating truly practical and effective EEG-based BCI solutions that can thrive in everyday environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Teacher to Student: Tracking Memorization Through Model Distillation",
      "titleJa": "From teacher to student: tracking memorization through モデル distillation",
      "link": "https://arxiv.org/abs/2506.16170",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16170v1 Announce Type: cross \nAbstract: Large language models (LLMs) are known to memorize parts of their training data, raising important concerns around privacy and security. While previous research has focused on studying memorization in pre-trained models, much less is known about how knowledge distillation (KD) affects memorization.In this study, we explore how different KD methods influence the memorization of fine-tuned task data when a large teacher model is distilled into smaller student variants.This study demonstrates that distilling a larger teacher model, fine-tuned on a dataset, into a smaller variant not only lowers computational costs and model size but also significantly reduces the memorization risks compared to standard fine-tuning approaches.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "CF-Seg: Counterfactuals meet Segmentation",
      "titleJa": "Cf-seg: counterfactuals meet segmentation",
      "link": "https://arxiv.org/abs/2506.16213",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16213v1 Announce Type: cross \nAbstract: Segmenting anatomical structures in medical images plays an important role in the quantitative assessment of various diseases. However, accurate segmentation becomes significantly more challenging in the presence of disease. Disease patterns can alter the appearance of surrounding healthy tissues, introduce ambiguous boundaries, or even obscure critical anatomical structures. As such, segmentation models trained on real-world datasets may struggle to provide good anatomical segmentation, leading to potential misdiagnosis. In this paper, we generate counterfactual (CF) images to simulate how the same anatomy would appear in the absence of disease without altering the underlying structure. We then use these CF images to segment structures of interest, without requiring any changes to the underlying segmentation model. Our experiments on two real-world clinical chest X-ray datasets show that the use of counterfactual images improves anatomical segmentation, thereby aiding downstream clinical decision-making.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping",
      "titleJa": "Synthetic als-eeg data augmentation for als 診断 using conditional wgan with weight clipping",
      "link": "https://arxiv.org/abs/2506.16243",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16243v1 Announce Type: cross \nAbstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and high-quality EEG data from ALS patients are scarce. This data scarcity, coupled with severe class imbalance between ALS and healthy control recordings, poses a challenge for training reliable machine learning classifiers. In this work, we address these issues by generating synthetic EEG signals for ALS patients using a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of ALS EEG signals and produce realistic synthetic samples. We preprocess and normalize EEG recordings, and train a CWGAN model to generate synthetic ALS signals. The CWGAN architecture and training routine are detailed, with key hyperparameters chosen for stable training. Qualitative evaluation of generated signals shows that they closely mimic real ALS EEG patterns. The CWGAN training converged with generator and discriminator loss curves stabilizing, indicating successful learning. The synthetic EEG signals appear realistic and have potential use as augmented data for training classifiers, helping to mitigate class imbalance and improve ALS detection accuracy. We discuss how this approach can facilitate data sharing and enhance diagnostic models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation",
      "titleJa": "Capsdt: diffusion-トランスフォーマー for capsule robot manipulation",
      "link": "https://arxiv.org/abs/2506.16263",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16263v1 Announce Type: cross \nAbstract: Vision-Language-Action (VLA) models have emerged as a prominent research area, showcasing significant potential across a variety of applications. However, their performance in endoscopy robotics, particularly endoscopy capsule robots that perform actions within the digestive system, remains unexplored. The integration of VLA models into endoscopy robots allows more intuitive and efficient interactions between human operators and medical devices, improving both diagnostic accuracy and treatment outcomes. In this work, we design CapsDT, a Diffusion Transformer model for capsule robot manipulation in the stomach. By processing interleaved visual inputs, and textual instructions, CapsDT can infer corresponding robotic control signals to facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot system, a capsule robot controlled by a robotic arm-held magnet, addressing different levels of four endoscopy tasks and creating corresponding capsule robot datasets within the stomach simulator. Comprehensive evaluations on various robotic tasks indicate that CapsDT can serve as a robust vision-language generalist, achieving state-of-the-art performance in various levels of endoscopy tasks while achieving a 26.25% success rate in real-world simulation manipulation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Artificial Intelligence for Atmospheric Sciences: A Research Roadmap",
      "titleJa": "人工知能 for atmospheric sciences: a 研究 roadmap",
      "link": "https://arxiv.org/abs/2506.16281",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16281v1 Announce Type: cross \nAbstract: Atmospheric sciences are crucial for understanding environmental phenomena ranging from air quality to extreme weather events, and climate change. Recent breakthroughs in sensing, communication, computing, and Artificial Intelligence (AI) have significantly advanced atmospheric sciences, enabling the generation of vast amounts of data through long-term Earth observations and providing powerful tools for analyzing atmospheric phenomena and predicting natural disasters. This paper contributes a critical interdisciplinary overview that bridges the fields of atmospheric science and computer science, highlighting the transformative potential of AI in atmospheric research. We identify key challenges associated with integrating AI into atmospheric research, including issues related to big data and infrastructure, and provide a detailed research roadmap that addresses both current and emerging challenges.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective",
      "titleJa": "Next-token prediction should be ambiguity-sensitive: a meta-learning perspective",
      "link": "https://arxiv.org/abs/2506.16288",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16288v1 Announce Type: cross \nAbstract: The rapid adaptation ability of auto-regressive foundation models is often attributed to the diversity of their pre-training data. This is because, from a Bayesian standpoint, minimizing prediction error in such settings requires integrating over all plausible latent hypotheses consistent with observations. While this behavior is desirable in principle, it often proves too ambitious in practice: under high ambiguity, the number of plausible latent alternatives makes Bayes-optimal prediction computationally intractable. Cognitive science has long recognized this limitation, suggesting that under such conditions, heuristics or information-seeking strategies are preferable to exhaustive inference. Translating this insight to next-token prediction, we hypothesize that low- and high-ambiguity predictions pose different computational demands, making ambiguity-agnostic next-token prediction a detrimental inductive bias. To test this, we introduce MetaHMM, a synthetic sequence meta-learning benchmark with rich compositional structure and a tractable Bayesian oracle. We show that Transformers indeed struggle with high-ambiguity predictions across model sizes. Motivated by cognitive theories, we propose a method to convert pre-trained models into Monte Carlo predictors that decouple task inference from token prediction. Preliminary results show substantial gains in ambiguous contexts through improved capacity allocation and test-time scalable inference, though challenges remain.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Analyzing the Influence of Knowledge Graph Information on Relation Extraction",
      "titleJa": "Analyzing the influence of knowledge graph information on relation extraction",
      "link": "https://arxiv.org/abs/2506.16343",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16343v1 Announce Type: cross \nAbstract: We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
      "titleJa": "Drag-and-drop llms: zero-shot prompt-to-weights",
      "link": "https://arxiv.org/abs/2506.16406",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16406v1 Announce Type: cross \nAbstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains up to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation",
      "titleJa": "Storywriter: a multi-agent framework for long story generation",
      "link": "https://arxiv.org/abs/2506.16445",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16445v1 Announce Type: cross \nAbstract: Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection",
      "titleJa": "Towards generalizable generic harmful speech datasets for implicit hate speech detection",
      "link": "https://arxiv.org/abs/2506.16476",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16476v1 Announce Type: cross \nAbstract: Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors",
      "titleJa": "Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of cnn detectors",
      "link": "https://arxiv.org/abs/2506.16497",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16497v1 Announce Type: cross \nAbstract: Face swapping manipulations in video streams represents an increasing threat in remote video communications, due to advances\n  in automated and real-time tools. Recent literature proposes to characterize and exploit visual artifacts introduced in video frames\n  by swapping algorithms when dealing with challenging physical scenes, such as face occlusions. This paper investigates the\n  effectiveness of this approach by benchmarking CNN-based data-driven models on two data corpora (including a newly collected\n  one) and analyzing generalization capabilities with respect to different acquisition sources and swapping algorithms. The results\n  confirm excellent performance of general-purpose CNN architectures when operating within the same data source, but a significant\n  difficulty in robustly characterizing occlusion-based visual cues across datasets. This highlights the need for specialized detection\n  strategies to deal with such artifacts.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Semantic To Instance: A Semi-Self-Supervised Learning Approach",
      "titleJa": "From semantic to instance: a semi-self-supervised learning approach",
      "link": "https://arxiv.org/abs/2506.16563",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16563v1 Announce Type: cross \nAbstract: Instance segmentation is essential for applications such as automated monitoring of plant health, growth, and yield. However, extensive effort is required to create large-scale datasets with pixel-level annotations of each object instance for developing instance segmentation models that restrict the use of deep learning in these areas. This challenge is more significant in images with densely packed, self-occluded objects, which are common in agriculture. To address this challenge, we propose a semi-self-supervised learning approach that requires minimal manual annotation to develop a high-performing instance segmentation model. We design GLMask, an image-mask representation for the model to focus on shape, texture, and pattern while minimizing its dependence on color features. We develop a pipeline to generate semantic segmentation and then transform it into instance-level segmentation. The proposed approach substantially outperforms the conventional instance segmentation models, establishing a state-of-the-art wheat head instance segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed methodology on the general-purpose Microsoft COCO dataset, achieving a significant performance improvement of over 12.6% mAP@50. This highlights that the utility of our proposed approach extends beyond precision agriculture and applies to other domains, specifically those with similar data characteristics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control",
      "titleJa": "Reimagination with test-time observation interventions: distractor-robust world モデル predictions for visual モデル predictive control",
      "link": "https://arxiv.org/abs/2506.16565",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16565v1 Announce Type: cross \nAbstract: World models enable robots to \"imagine\" future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning. Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training. Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification. In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable. Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction. Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution. Finally, ReOI \"reimagines\" future outcomes with the modified observation and reintroduces the distractors post-hoc to preserve visual consistency for downstream planning and verification. We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model. Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors. Notably, it improves task success rates by up to 3x in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
      "titleJa": "Measuring (a sufficient) world モデル in llms: a variance decomposition framework",
      "link": "https://arxiv.org/abs/2506.16584",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16584v1 Announce Type: cross \nAbstract: Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Energy-Based Transfer for Reinforcement Learning",
      "titleJa": "Energy-based transfer for 強化学習",
      "link": "https://arxiv.org/abs/2506.16590",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16590v1 Announce Type: cross \nAbstract: Reinforcement learning algorithms often suffer from poor sample efficiency, making them challenging to apply in multi-task or continual learning settings. Efficiency can be improved by transferring knowledge from a previously trained teacher policy to guide exploration in new but related tasks. However, if the new task sufficiently differs from the teacher's training task, the transferred guidance may be sub-optimal and bias exploration toward low-reward behaviors. We propose an energy-based transfer learning method that uses out-of-distribution detection to selectively issue guidance, enabling the teacher to intervene only in states within its training distribution. We theoretically show that energy scores reflect the teacher's state-visitation density and empirically demonstrate improved sample efficiency and performance across both single-task and multi-task settings.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images",
      "titleJa": "Hybrid attention network for accurate breast tumor segmentation in ultrasound images",
      "link": "https://arxiv.org/abs/2506.16592",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16592v1 Announce Type: cross \nAbstract: Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation",
      "titleJa": "History-augmented vision-language models for frontier-based zero-shot object navigation",
      "link": "https://arxiv.org/abs/2506.16623",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16623v1 Announce Type: cross \nAbstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in unseen environments, demanding sophisticated reasoning. While Vision-Language Models (VLMs) show potential, current ObjectNav methods often employ them superficially, primarily using vision-language embeddings for object-scene similarity checks rather than leveraging deeper reasoning. This limits contextual understanding and leads to practical issues like repetitive navigation behaviors. This paper introduces a novel zero-shot ObjectNav framework that pioneers the use of dynamic, history-aware prompting to more deeply integrate VLM reasoning into frontier-based exploration. Our core innovation lies in providing the VLM with action history context, enabling it to generate semantic guidance scores for navigation actions while actively avoiding decision loops. We also introduce a VLM-assisted waypoint generation mechanism for refining the final approach to detected objects. Evaluated on the HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and 24.8% Success weighted by Path Length (SPL). These results are comparable to state-of-the-art zero-shot methods, demonstrating the significant potential of our history-augmented VLM prompting strategy for more robust and context-aware robotic navigation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation",
      "titleJa": "Latent noise injection for private and statistically aligned synthetic data generation",
      "link": "https://arxiv.org/abs/2506.16636",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16636v1 Announce Type: cross \nAbstract: Synthetic Data Generation has become essential for scalable, privacy-preserving statistical analysis. While standard approaches based on generative models, such as Normalizing Flows, have been widely used, they often suffer from slow convergence in high-dimensional settings, frequently converging more slowly than the canonical $1/\\sqrt{n}$ rate when approximating the true data distribution.\n  To overcome these limitations, we propose a Latent Noise Injection method using Masked Autoregressive Flows (MAF). Instead of directly sampling from the trained model, our method perturbs each data point in the latent space and maps it back to the data domain. This construction preserves a one to one correspondence between observed and synthetic data, enabling synthetic outputs that closely reflect the underlying distribution, particularly in challenging high-dimensional regimes where traditional sampling struggles.\n  Our procedure satisfies local $(\\epsilon, \\delta)$-differential privacy and introduces a single perturbation parameter to control the privacy-utility trade-off. Although estimators based on individual synthetic datasets may converge slowly, we show both theoretically and empirically that aggregating across $K$ studies in a meta analysis framework restores classical efficiency and yields consistent, reliable inference. We demonstrate that with a well-calibrated perturbation parameter, Latent Noise Injection achieves strong statistical alignment with the original data and robustness against membership inference attacks. These results position our method as a compelling alternative to conventional flow-based sampling for synthetic data sharing in decentralized and privacy-sensitive domains, such as biomedical research.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "SemAgent: A Semantics Aware Program Repair Agent",
      "titleJa": "Semagent: a semantics aware program repair agent",
      "link": "https://arxiv.org/abs/2506.16650",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16650v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Minimalist Optimizer Design for LLM Pretraining",
      "titleJa": "A minimalist optimizer design for LLM pretraining",
      "link": "https://arxiv.org/abs/2506.16659",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16659v1 Announce Type: cross \nAbstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which require significant memory to maintain first- and second-moment matrices, known as optimizer states. While recent works such as GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What is the minimal amount of optimizer state that is truly necessary to retain state-of-the-art performance in LLM pretraining? In this work, we systematically investigate this question using a bottom-up approach. We find that two memory- and compute-efficient optimization techniques are particularly effective: (1) column-wise gradient normalization significantly boosts the performance of plain SGD without requiring momentum; and (2) adding first-order momentum only to the output layer - where gradient variance is highest - yields performance competitive with fully adaptive methods such as Muon. Based on these insights, we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new optimizer that combines column-normalized SGD with last-layer momentum, where column normalization refers to normalizing the gradient along the output dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira, and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art method APOLLO in terms of both perplexity and memory consumption. In addition, our method serves as a minimalist baseline for more sophisticated optimizer design.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation",
      "titleJa": "A simple contrastive framework of item tokenization for generative recommendation",
      "link": "https://arxiv.org/abs/2506.16683",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16683v1 Announce Type: cross \nAbstract: Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology",
      "titleJa": "From prompts to constructs: a dual-validity framework for LLM 研究 in psychology",
      "link": "https://arxiv.org/abs/2506.16697",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16697v1 Announce Type: cross \nAbstract: Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing \"I am anxious\"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Large Language Models as Psychological Simulators: A Methodological Guide",
      "titleJa": "Large language models as psychological simulators: a methodological guide",
      "link": "https://arxiv.org/abs/2506.16702",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16702v1 Announce Type: cross \nAbstract: Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models",
      "titleJa": "Reasongrm: enhancing generative reward models through large reasoning models",
      "link": "https://arxiv.org/abs/2506.16712",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16712v1 Announce Type: cross \nAbstract: Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data",
      "titleJa": "Tricon-sf: a triple-shuffle and contribution-aware serial federated learning framework for heterogeneous ヘルスケア data",
      "link": "https://arxiv.org/abs/2506.16723",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16723v1 Announce Type: cross \nAbstract: Serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead. However, even without centralized aggregation, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks. Additionally, ensuring resilience against semi-honest or malicious clients who may manipulate or misuse received models remains a grand challenge, particularly in privacy-sensitive domains such as healthcare. To address these challenges, we propose TriCon-SF, a novel serial federated learning framework that integrates triple shuffling and contribution awareness. TriCon-SF introduces three levels of randomization by shuffling model layers, data segments, and training sequences to break deterministic learning patterns and disrupt potential attack vectors, thereby enhancing privacy and robustness. In parallel, it leverages Shapley value methods to dynamically evaluate client contributions during training, enabling the detection of dishonest behavior and enhancing system accountability. Extensive experiments on non-IID healthcare datasets demonstrate that TriCon-SF outperforms standard serial and parallel federated learning in both accuracy and communication efficiency. Security analysis further supports its resilience against client-side privacy attacks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
      "titleJa": "Off-政策 actor-critic for adversarial observation robustness: virtual alternative 学習 via symmetric 政策 evaluation",
      "link": "https://arxiv.org/abs/2506.16753",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16753v1 Announce Type: cross \nAbstract: Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning",
      "titleJa": "Mist: jailbreaking black-box large language models via iterative semantic tuning",
      "link": "https://arxiv.org/abs/2506.16792",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16792v1 Announce Type: cross \nAbstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning",
      "titleJa": "Robust dynamic material handling via adaptive constrained evolutionary 強化学習",
      "link": "https://arxiv.org/abs/2506.16795",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16795v1 Announce Type: cross \nAbstract: Dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time for minimising makespan and tardiness. In real-world scenarios, historical task records are usually available, which enables the training of a decision policy on multiple instances consisting of historical records. Recently, reinforcement learning has been applied to solve DMH. Due to the occurrence of dynamic events such as new tasks, adaptability is highly required. Solving DMH is challenging since constraints including task delay should be satisfied. A feedback is received only when all tasks are served, which leads to sparse reward. Besides, making the best use of limited computational resources and historical records for training a robust policy is crucial. The time allocated to different problem instances would highly impact the learning process. To tackle those challenges, this paper proposes a novel adaptive constrained evolutionary reinforcement learning (ACERL) approach, which maintains a population of actors for diverse exploration. ACERL accesses each actor for tackling sparse rewards and constraint violation to restrict the behaviour of the policy. Moreover, ACERL adaptively selects the most beneficial training instances for improving the policy. Extensive experiments on eight training and eight unseen test instances demonstrate the outstanding performance of ACERL compared with several state-of-the-art algorithms. Policies trained by ACERL can schedule the vehicles while fully satisfying the constraints. Additional experiments on 40 unseen noised instances show the robust performance of ACERL. Cross-validation further presents the overall effectiveness of ACREL. Besides, a rigorous ablation study highlights the coordination and benefits of each ingredient of ACERL.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Towards Effective Complementary Security Analysis using Large Language Models",
      "titleJa": "Towards effective complementary セキュリティ analysis using large language models",
      "link": "https://arxiv.org/abs/2506.16899",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16899v1 Announce Type: cross \nAbstract: A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence",
      "titleJa": "Single-shot thermometry of simulated bose--einstein condensates using 人工知能",
      "link": "https://arxiv.org/abs/2506.16925",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16925v1 Announce Type: cross \nAbstract: Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D `pancake' condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating thermodynamic parameters for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Latent Concept Disentanglement in Transformer-based Language Models",
      "titleJa": "Latent concept disentanglement in トランスフォーマー-based language models",
      "link": "https://arxiv.org/abs/2506.16975",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16975v1 Announce Type: cross \nAbstract: When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs",
      "titleJa": "Texpert: a multi-level benchmark for evaluating latex code generation by llms",
      "link": "https://arxiv.org/abs/2506.16990",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16990v1 Announce Type: cross \nAbstract: LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers",
      "titleJa": "From concepts to components: concept-agnostic attention module discovery in transformers",
      "link": "https://arxiv.org/abs/2506.17052",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17052v1 Announce Type: cross \nAbstract: Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K benchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI",
      "titleJa": "LLM-based bot broadens the range of arguments in online discussions, even when transparently disclosed as ai",
      "link": "https://arxiv.org/abs/2506.17073",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17073v1 Announce Type: cross \nAbstract: A wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization. However, engagement in online political discussions often features a limited spectrum of views due to high levels of self-selection and the tendency of online platforms to facilitate exchanges primarily among like-minded individuals. This study examines whether an LLM-based bot can widen the scope of perspectives expressed by participants in online discussions through two pre-registered randomized experiments conducted in a chatroom. We evaluate the impact of a bot that actively monitors discussions, identifies missing arguments, and introduces them into the conversation. The results indicate that our bot significantly expands the range of arguments, as measured by both objective and subjective metrics. Furthermore, disclosure of the bot as AI does not significantly alter these effects. These findings suggest that LLM-based moderation tools can positively influence online political discourse.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation",
      "titleJa": "Mexa: towards general multimodal reasoning with dynamic multi-expert aggregation",
      "link": "https://arxiv.org/abs/2506.17113",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17113v1 Announce Type: cross \nAbstract: Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model",
      "titleJa": "Rapid and continuous trust evaluation for effective task コラボレーション through siamese モデル",
      "link": "https://arxiv.org/abs/2506.17128",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17128v1 Announce Type: cross \nAbstract: Trust is emerging as an effective tool to ensure the successful completion of collaborative tasks within collaborative systems. However, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources. To tackle this challenge, this paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) to facilitate effective task collaboration. First, the communication and computing resource attributes of the collaborator in a trusted state, along with historical collaboration data, are collected and represented using an attributed control flow graph (ACFG) that captures trust-related semantic information and serves as a reference for comparison with data collected during task execution. At each time slot of task execution, the collaborator's communication and computing resource attributes, as well as task completion effectiveness, are collected in real time and represented with an ACFG to convey their trust-related semantic information. A Siamese model, consisting of two shared-parameter Structure2vec networks, is then employed to learn the deep semantics of each pair of ACFGs and generate their embeddings. Finally, the similarity between the embeddings of each pair of ACFGs is calculated to determine the collaborator's trust value at each time slot. A real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to test the effectiveness of the proposed SRCTE framework. Experimental results demonstrate that SRCTE converges rapidly with only a small amount of data and achieves a high anomaly trust detection rate compared to the baseline algorithm.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification",
      "titleJa": "Medi: metadata-guided diffusion models for mitigating biases in tumor classification",
      "link": "https://arxiv.org/abs/2506.17140",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17140v1 Announce Type: cross \nAbstract: Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Do We Need Large VLMs for Spotting Soccer Actions?",
      "titleJa": "Do we need large vlms for spotting soccer actions?",
      "link": "https://arxiv.org/abs/2506.17144",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17144v1 Announce Type: cross \nAbstract: Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. In this work, we propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich, fine-grained descriptions and contextual cues such as excitement and tactical insights, contains enough information to reliably spot key actions in a match. To demonstrate this, we use the SoccerNet Echoes dataset, which provides timestamped commentary, and employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics. Each LLM evaluates sliding windows of commentary to identify actions like goals, cards, and substitutions, generating accurate timestamps for these events. Our experiments show that this language-centric approach performs effectively in detecting critical match events, providing a lightweight and training-free alternative to traditional video-based methods for action spotting.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Towards AI Search Paradigm",
      "titleJa": "Towards ai search paradigm",
      "link": "https://arxiv.org/abs/2506.17188",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17188v1 Announce Type: cross \nAbstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation",
      "titleJa": "Long-term traffic simulation with interleaved autoregressive motion and scenario generation",
      "link": "https://arxiv.org/abs/2506.17213",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17213v1 Announce Type: cross \nAbstract: An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
      "titleJa": "No free lunch: rethinking internal feedback for LLM reasoning",
      "link": "https://arxiv.org/abs/2506.17219",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17219v1 Announce Type: cross \nAbstract: Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry",
      "titleJa": "Alto: orchestrating distributed compound ai systems with nested ancestry",
      "link": "https://arxiv.org/abs/2403.04311",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2403.04311v2 Announce Type: replace \nAbstract: Compound AI applications chain together subcomponents such as generative language models, document retrievers, and embedding models. Applying traditional systems optimizations such as parallelism and pipelining in compound AI systems is difficult because each component has different constraints in terms of the granularity and type of data that it ingests. New data is often generated during intermediate computations, and text streams may be split into smaller, independent fragments (such as documents to sentences) which may then be re-aggregated at later parts of the computation. Due to this complexity, existing systems to serve compound AI queries do not fully take advantage of parallelism and pipelining opportunities.\n  We present Alto, a framework that automatically optimizes execution of compound AI queries through streaming and parallelism. Bento introduces a new abstraction called nested ancestry, a metadata hierarchy that allows the system to correctly track partial outputs and aggregate data across the heterogeneous constraints of the components of compound AI applications. This metadata is automatically inferred from the programming model, allowing developers to express complex dataflow patterns without needing to reason manually about the details of routing and aggregation. Implementations of four applications in Alto outperform or match implementations in LangGraph, a popular existing AI programming framework. Alto implementations match or improve latency by between 10-30%.",
      "summary": "arXiv:2403.",
      "summaryJa": "Arxiv:2403.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Embodied Intelligence System",
      "titleJa": "Laecips: large vision モデル assisted adaptive edge-クラウド コラボレーション for iot-based embodied intelligence system",
      "link": "https://arxiv.org/abs/2404.10498",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2404.10498v2 Announce Type: replace \nAbstract: Embodied intelligence (EI) enables manufacturing systems to flexibly perceive, reason, adapt, and operate within dynamic shop floor environments. In smart manufacturing, a representative EI scenario is robotic visual inspection, where industrial robots must accurately inspect components on rapidly changing, heterogeneous production lines. This task requires both high inference accuracy especially for uncommon defects and low latency to match production speeds, despite evolving lighting, part geometries, and surface conditions. To meet these needs, we propose LAECIPS, a large vision model-assisted adaptive edge-cloud collaboration framework for IoT-based embodied intelligence systems. LAECIPS decouples large vision models in the cloud from lightweight models on the edge, enabling plug-and-play model adaptation and continual learning. Through a hard input mining-based inference strategy, LAECIPS routes complex and uncertain inspection cases to the cloud while handling routine tasks at the edge, achieving both high accuracy and low latency. Experiments conducted on a real-world robotic semantic segmentation system for visual inspection demonstrate significant improvements in accuracy, processing latency, and communication overhead compared to state-of-the-art methods. LAECIPS provides a practical and scalable foundation for embodied intelligence in smart manufacturing, especially in adaptive robotic inspection and quality control scenarios.",
      "summary": "arXiv:2404.",
      "summaryJa": "Arxiv:2404.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Cost-effective Instruction Learning for Pathology Vision and Language Analysis",
      "titleJa": "Cost-effective instruction learning for pathology vision and language analysis",
      "link": "https://arxiv.org/abs/2407.17734",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2407.17734v2 Announce Type: replace \nAbstract: The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology.",
      "summary": "arXiv:2407.",
      "summaryJa": "Arxiv:2407.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Automatic dataset shift identification to support safe deployment of medical imaging AI",
      "titleJa": "Automatic データセット shift identification to support safe deployment of 医療 imaging ai",
      "link": "https://arxiv.org/abs/2411.07940",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.07940v3 Announce Type: replace \nAbstract: Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, the root causes of dataset shifts are diverse, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework for imaging datasets, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts using five large publicly available datasets.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Infrastructure for AI Agents",
      "titleJa": "Infrastructure for ai agents",
      "link": "https://arxiv.org/abs/2501.10114",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.10114v3 Announce Type: replace \nAbstract: AI agents plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. Much research on making agents useful and safe focuses on directly modifying their behaviour, such as by training them to follow user instructions. Direct behavioural modifications are useful, but do not fully address how heterogeneous agents will interact with each other and other actors. Rather, we will need external protocols and systems to shape such interactions. For instance, agents will need more efficient protocols to communicate with each other and form agreements. Attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of \\textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
      "titleJa": "Rethinking external slow-thinking: from snowball errors to probability of correct reasoning",
      "link": "https://arxiv.org/abs/2501.15602",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.15602v3 Announce Type: replace \nAbstract: Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code at https://github.com/ZyGan1999/Snowball-Errors-and-Probability.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Nature Language Model: Deciphering the Language of Nature for Scientific Discovery",
      "titleJa": "Nature language モデル: deciphering the language of nature for scientific discovery",
      "link": "https://arxiv.org/abs/2502.07527",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.07527v3 Announce Type: replace \nAbstract: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization",
      "titleJa": "Planning of heuristics: strategic planning on large language models with monte carlo tree search for automating heuristic optimization",
      "link": "https://arxiv.org/abs/2502.11422",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.11422v3 Announce Type: replace \nAbstract: Heuristics have achieved great success in solving combinatorial optimization problems~(COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Since Large Language Models~(LLMs) possess strong capabilities to understand and generate content with a knowledge base that covers various domains, they offer potential ways to automatically optimize heuristics. To this end, we propose Planning of Heuristics~(PoH), an optimization method that integrates LLM self-reflection with Monte Carlo Tree Search, a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics~(states) and improve them based on the improvement suggestions~(actions) and evaluation results~(rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop Scheduling Problem. The experimental results show that PoH outperforms hand-crafted heuristics and other Automatic Heuristic Design methods based on LLMs, and achieves the state-of-the-art performance in automating heuristic optimization with LLMs to solve tested COPs, especially with large sizes.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Synthesizing Composite Hierarchical Structure from Symbolic Music Corpora",
      "titleJa": "Synthesizing composite hierarchical structure from symbolic music corpora",
      "link": "https://arxiv.org/abs/2502.15849",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.15849v4 Announce Type: replace \nAbstract: Western music is an innately hierarchical system of interacting levels of structure, from fine-grained melody to high-level form. In order to analyze music compositions holistically and at multiple granularities, we propose a unified, hierarchical meta-representation of musical structure called the structural temporal graph (STG). For a single piece, the STG is a data structure that defines a hierarchy of progressively finer structural musical features and the temporal relationships between them. We use the STG to enable a novel approach for deriving a representative structural summary of a music corpus, which we formalize as a nested NP-hard combinatorial optimization problem extending the Generalized Median Graph problem. Our approach first applies simulated annealing to develop a measure of structural distance between two music pieces rooted in graph isomorphism. Our approach then combines the formal guarantees of SMT solvers with nested simulated annealing over structural distances to produce a structurally sound, representative centroid STG for an entire corpus of STGs from individual pieces. To evaluate our approach, we conduct experiments verifying that structural distance accurately differentiates between music pieces, and that derived centroids accurately structurally characterize their corpora.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
      "titleJa": "Enhancing mathematical reasoning in large language models with self-consistency-based hallucination detection",
      "link": "https://arxiv.org/abs/2504.09440",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.09440v3 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
      "titleJa": "Robust finite-memory 政策 gradients for hidden-モデル pomdps",
      "link": "https://arxiv.org/abs/2505.09518",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.09518v2 Announce Type: replace \nAbstract: Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs.We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale",
      "titleJa": "Swarmthinkers: learning physically consistent atomic kmc transitions at scale",
      "link": "https://arxiv.org/abs/2505.20094",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.20094v2 Announce Type: replace \nAbstract: Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness",
      "titleJa": "Revisiting multi-agent debate as test-time scaling: a systematic study of conditional effectiveness",
      "link": "https://arxiv.org/abs/2505.22960",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.22960v2 Announce Type: replace \nAbstract: The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning",
      "titleJa": "The ai imperative: scaling high-quality peer review in 機械学習",
      "link": "https://arxiv.org/abs/2506.08134",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.08134v2 Announce Type: replace \nAbstract: Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence",
      "titleJa": "Embodied web agents: bridging physical-digital realms for integrated agent intelligence",
      "link": "https://arxiv.org/abs/2506.15677",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15677v2 Announce Type: replace \nAbstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation",
      "titleJa": "Efficient and flexible ニューラルネットワーク 学習 through layer-wise feedback propagation",
      "link": "https://arxiv.org/abs/2308.12053",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2308.12053v3 Announce Type: replace-cross \nAbstract: Gradient-based optimization has been a cornerstone of machine learning that enabled the vast advances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can become limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications - neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) - we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function. Our code is available at https://github.com/leanderweber/layerwise-feedback-propagation.",
      "summary": "arXiv:2308.",
      "summaryJa": "Arxiv:2308.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Adaptive Experimental Design for Policy Learning",
      "titleJa": "Adaptive experimental design for 政策 learning",
      "link": "https://arxiv.org/abs/2401.03756",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2401.03756v4 Announce Type: replace-cross \nAbstract: This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.",
      "summary": "arXiv:2401.",
      "summaryJa": "Arxiv:2401.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention",
      "titleJa": "Efficient event-based object detection: a hybrid ニューラルネットワーク with spatial and temporal attention",
      "link": "https://arxiv.org/abs/2403.10173",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2403.10173v4 Announce Type: replace-cross \nAbstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy-efficient and low latency event-based data processing, they often fall short of Artificial Neural Networks (ANNs) in accuracy and flexibility. Here, we introduce Attention-based Hybrid SNN-ANN backbones for event-based object detection to leverage the strengths of both SNN and ANN architectures. A novel Attention-based SNN-ANN bridge module captures sparse spatial and temporal relations from the SNN layer and converts them into dense feature maps for the ANN part of the backbone. Additionally, we present a variant that integrates DWConvL-STMs to the ANN blocks to capture slower dynamics. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics. Experimental results demonstrate that our proposed method surpasses SNN-based approaches by significant margins, with results comparable to existing ANN and RNN-based methods. Unlike ANN-only networks, the hybrid setup allows us to implement the SNN blocks on digital neuromorphic hardware to investigate the feasibility of our approach. Extensive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. Our hybrid SNN-ANN architectures pave the way for ANN-like performance at a drastically reduced parameter, latency, and power budget.",
      "summary": "arXiv:2403.",
      "summaryJa": "Arxiv:2403.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "ChatDBG: Augmenting Debugging with Large Language Models",
      "titleJa": "Chatdbg: augmenting debugging with large language models",
      "link": "https://arxiv.org/abs/2403.16354",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2403.16354v5 Announce Type: replace-cross \nAbstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\". To handle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.",
      "summary": "arXiv:2403.",
      "summaryJa": "Arxiv:2403.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Survey of Automatic Hallucination Evaluation on Natural Language Generation",
      "titleJa": "A survey of automatic hallucination evaluation on natural language generation",
      "link": "https://arxiv.org/abs/2404.12041",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2404.12041v3 Announce Type: replace-cross \nAbstract: The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.",
      "summary": "arXiv:2404.",
      "summaryJa": "Arxiv:2404.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "LogProber: Disentangling confidence from contamination in LLM responses",
      "titleJa": "Logprober: disentangling confidence from contamination in LLM responses",
      "link": "https://arxiv.org/abs/2408.14352",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.14352v3 Announce Type: replace-cross \nAbstract: In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical. In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Core Knowledge Deficits in Multi-Modal Language Models",
      "titleJa": "Core knowledge deficits in multi-modal language models",
      "link": "https://arxiv.org/abs/2410.10855",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.10855v4 Announce Type: replace-cross \nAbstract: While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Learning to Route LLMs with Confidence Tokens",
      "titleJa": "Learning to route llms with confidence tokens",
      "link": "https://arxiv.org/abs/2410.13284",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.13284v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning",
      "titleJa": "A implies b: circuit analysis in llms for propositional logical reasoning",
      "link": "https://arxiv.org/abs/2411.04105",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.04105v4 Announce Type: replace-cross \nAbstract: Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts",
      "titleJa": "Multi-preference optimization: generalizing dpo via set-level contrasts",
      "link": "https://arxiv.org/abs/2412.04628",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2412.04628v4 Announce Type: replace-cross \nAbstract: Direct Preference Optimization (DPO) has become a popular approach for aligning language models using pairwise preferences. However, in practical post-training pipelines, on-policy generation typically yields multiple candidate responses per prompt, which are scored by a reward model to guide learning. In this setting, we propose $\\textbf{Multi-Preference Optimization (MPO)}$, a generalization of DPO that optimizes over entire sets of responses by extending the Bradley-Terry model to groupwise comparisons between chosen and rejected sets. To further enhance learning, MPO employs deviation-based weighting, which emphasizes outlier responses that deviate most from the mean reward, effectively inducing a self-paced curriculum. We theoretically prove that MPO reduces alignment bias at a rate of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}}\\right)$ with respect to the number of responses per query. Empirically, MPO achieves state-of-the-art performance on the UltraFeedback benchmark and yields up to $\\sim 17.5\\%$ improvement over the state-of-the-art baseline in length-controlled win rate on AlpacaEval2, establishing a new baseline for preference-based alignment",
      "summary": "arXiv:2412.",
      "summaryJa": "Arxiv:2412.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning",
      "titleJa": "Each rank could be an expert: single-ranked mixture of experts lora for multi-task learning",
      "link": "https://arxiv.org/abs/2501.15103",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.15103v2 Announce Type: replace-cross \nAbstract: Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank as an independent expert}. With a \\textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning",
      "titleJa": "Rethinking fine-tuning when scaling test-time compute: limiting confidence improves mathematical reasoning",
      "link": "https://arxiv.org/abs/2502.07154",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.07154v3 Announce Type: replace-cross \nAbstract: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization",
      "titleJa": "Conformal 推論 under high-dimensional covariate shifts via likelihood-ratio regularization",
      "link": "https://arxiv.org/abs/2502.13030",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.13030v4 Announce Type: replace-cross \nAbstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
      "titleJa": "From rag to memory: non-parametric continual learning for large language models",
      "link": "https://arxiv.org/abs/2502.14802",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.14802v2 Announce Type: replace-cross \nAbstract: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models",
      "titleJa": "Batayan: a filipino nlp benchmark for evaluating large language models",
      "link": "https://arxiv.org/abs/2502.14911",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.14911v2 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models",
      "titleJa": "Dynamic knowledge integration for evidence-driven counter-argument generation with large language models",
      "link": "https://arxiv.org/abs/2503.05328",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.05328v2 Announce Type: replace-cross \nAbstract: This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
      "titleJa": "Efficient but vulnerable: benchmarking and defending LLM batch prompting attack",
      "link": "https://arxiv.org/abs/2503.15551",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.15551v2 Announce Type: replace-cross \nAbstract: Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
      "titleJa": "Pr-attack: coordinated prompt-rag attacks on retrieval-augmented generation in large language models via bilevel optimization",
      "link": "https://arxiv.org/abs/2504.07717",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.07717v3 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations",
      "titleJa": "Aerialvg: a challenging benchmark for aerial visual grounding by exploring positional relations",
      "link": "https://arxiv.org/abs/2504.07836",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.07836v3 Announce Type: replace-cross \nAbstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
      "titleJa": "Perceptionlm: open-access data and models for detailed visual understanding",
      "link": "https://arxiv.org/abs/2504.13180",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.13180v2 Announce Type: replace-cross \nAbstract: Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a video. We make our work fully reproducible by providing data, training recipes, code & models. https://github.com/facebookresearch/perception_models",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
      "titleJa": "Assessing tenstorrent's risc-v matmul acceleration capabilities",
      "link": "https://arxiv.org/abs/2505.06085",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.06085v3 Announce Type: replace-cross \nAbstract: The increasing demand for generative AI as Large Language Models (LLMs) services has driven the need for specialized hardware architectures that optimize computational efficiency and energy consumption. This paper evaluates the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic linear algebra kernels at reduced numerical precision, a fundamental operation in LLM computations. We present a detailed characterization of Grayskull's execution model, gridsize, matrix dimensions, data formats, and numerical precision impact computational efficiency. Furthermore, we compare Grayskull's performance against state-of-the-art architectures with tensor acceleration, including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100). Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a competitive trade-off between power consumption and computational throughput, reaching a peak of 1.55 TFLOPs/Watt with BF16.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks",
      "titleJa": "Mask-pinns: regulating feature distributions in physics-informed neural networks",
      "link": "https://arxiv.org/abs/2505.06331",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.06331v2 Announce Type: replace-cross \nAbstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws directly into the loss function. However, effective training of PINNs remains challenging due to internal covariate shift, which destabilizes feature distributions and impairs model expressiveness. While normalization techniques like Batch Normalization and Layer Normalization are standard remedies in deep learning, they disrupt the pointwise input-output mappings critical to preserving the physical consistency in PINNs. In this work, we introduce Mask-PINNs, a novel architecture that regulates internal feature distributions through a smooth, learnable mask function applied pointwise across hidden layers. Unlike conventional normalization methods, the proposed mask function preserves the deterministic nature of input-output relationships while suppressing activation drift and saturation. Theoretically, we demonstrate that Mask-PINNs control feature spread near initialization by attenuating gradient variance growth through a tailored modulation mechanism. Empirically, we validate the method on multiple PDE benchmarks across diverse activation functions. Our results show consistent improvements in prediction accuracy, convergence stability, and robustness, with relative L2 errors reduced by up to two orders of magnitude over baseline models. Furthermore, we demonstrate that Mask-PINNs enable the effective use of wider networks, overcoming a key limitation in existing PINN frameworks.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
      "titleJa": "Learning dynamics in continual pre-学習 for large language models",
      "link": "https://arxiv.org/abs/2505.07796",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.07796v2 Announce Type: replace-cross \nAbstract: Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
      "titleJa": "Dynamic risk assessments for offensive cybersecurity agents",
      "link": "https://arxiv.org/abs/2505.18384",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.18384v2 Announce Type: replace-cross \nAbstract: Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models",
      "titleJa": "More thinking, less seeing? assessing amplified hallucination in multimodal reasoning models",
      "link": "https://arxiv.org/abs/2505.21523",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.21523v3 Announce Type: replace-cross \nAbstract: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation",
      "titleJa": "Graphrag-bench: challenging domain-specific reasoning for evaluating graph retrieval-augmented generation",
      "link": "https://arxiv.org/abs/2506.02404",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.02404v3 Announce Type: replace-cross \nAbstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \\((ii)\\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
      "titleJa": "Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing",
      "link": "https://arxiv.org/abs/2506.09965",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.09965v2 Announce Type: replace-cross \nAbstract: As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy",
      "titleJa": "Tardis stride: a spatio-temporal road image データセット and world モデル for autonomy",
      "link": "https://arxiv.org/abs/2506.11302",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11302v3 Announce Type: replace-cross \nAbstract: World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Semantic Preprocessing for LLM-based Malware Analysis",
      "titleJa": "Semantic preprocessing for LLM-based malware analysis",
      "link": "https://arxiv.org/abs/2506.12113",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12113v2 Announce Type: replace-cross \nAbstract: In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction",
      "titleJa": "Breastdcedl: curating a comprehensive dce-mri データセット and developing a トランスフォーマー implementation for breast cancer treatment response prediction",
      "link": "https://arxiv.org/abs/2506.12190",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12190v2 Announce Type: replace-cross \nAbstract: Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+/HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments",
      "titleJa": "Screen hijack: visual poisoning of vlm agents in mobile environments",
      "link": "https://arxiv.org/abs/2506.13205",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13205v2 Announce Type: replace-cross \nAbstract: With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models",
      "titleJa": "Adaptive guidance accelerates 強化学習 of reasoning models",
      "link": "https://arxiv.org/abs/2506.13923",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13923v2 Announce Type: replace-cross \nAbstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance in two main ways: (1) by compressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B parameters on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\\text{Guide}$ -- a new class of online training algorithms. $\\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the \"off-policy\" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework",
      "titleJa": "Capturing polysemanticity with prism: a multi-concept feature description framework",
      "link": "https://arxiv.org/abs/2506.15538",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15538v2 Announce Type: replace-cross \nAbstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Current feature description methods face two critical challenges: limited robustness and the flawed assumption that each neuron encodes only a single concept (monosemanticity), despite growing evidence that neurons are often polysemantic. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework that captures the inherent complexity of neural network features. Unlike prior approaches that assign a single description per feature, PRISM provides more nuanced descriptions for both polysemantic and monosemantic features. We apply PRISM to language models and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
      "titleJa": "Mle-star: 機械学習 engineering agent via search and targeted refinement",
      "link": "https://arxiv.org/abs/2506.15692",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15692v1 Announce Type: new \nAbstract: Agents based on large language models (LLMs) for machine learning engineering (MLE) can automatically implement ML models via code generation. However, existing approaches to build such agents often rely heavily on inherent LLM knowledge and employ coarse exploration strategies that modify the entire code structure at once. This limits their ability to select effective task-specific models and perform deep exploration within specific components, such as experimenting extensively with feature engineering options. To overcome these, we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first leverages external knowledge by using a search engine to retrieve effective models from the web, forming an initial solution, then iteratively refines it by exploring various strategies targeting specific ML components. This exploration is guided by ablation studies analyzing the impact of individual code blocks. Furthermore, we introduce a novel ensembling method using an effective strategy suggested by MLE-STAR. Our experimental results show that MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench, significantly outperforming the best alternative.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models",
      "titleJa": "Simugen: multi-modal agentic framework for constructing block diagram-based simulation models",
      "link": "https://arxiv.org/abs/2506.15695",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15695v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) have shown impressive performance in mathematical reasoning and code generation. However, LLMs still struggle in the simulation domain, particularly in generating Simulink models, which are essential tools in engineering and scientific research. Our preliminary experiments indicate that LLM agents often fail to produce reliable and complete Simulink simulation code from text-only inputs, likely due to the lack of Simulink-specific data in their pretraining. To address this challenge, we propose SimuGen, a multimodal agent-based framework that automatically generates accurate Simulink simulation code by leveraging both the visual Simulink diagram and domain knowledge. SimuGen coordinates several specialized agents, including an investigator, unit test reviewer, code generator, executor, debug locator, and report writer, supported by a domain-specific knowledge base. This collaborative and modular design enables interpretable, robust, and reproducible Simulink simulation generation. Our source code is publicly available at https://github.com/renxinxing123/SimuGen_beta.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction",
      "titleJa": "Coc: chain-of-cancer based on cross-modal autoregressive traction for survival prediction",
      "link": "https://arxiv.org/abs/2506.15696",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15696v1 Announce Type: new \nAbstract: Survival prediction aims to evaluate the risk level of cancer patients. Existing methods primarily rely on pathology and genomics data, either individually or in combination. From the perspective of cancer pathogenesis, epigenetic changes, such as methylation data, could also be crucial for this task. Furthermore, no previous endeavors have utilized textual descriptions to guide the prediction. To this end, we are the first to explore the use of four modalities, including three clinical modalities and language, for conducting survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT) to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and inter-learning. We encode the clinical data as the raw features, which remain domain-specific knowledge for intra-learning. In terms of inter-learning, we use language to prompt the raw features and introduce an Autoregressive Mutual Traction module for synergistic representation. This tailored framework facilitates joint learning among multiple modalities. Our approach is evaluated across five public cancer datasets, and extensive experiments validate the effectiveness of our methods and proposed designs, leading to producing \\sota results. Codes will be released.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention",
      "titleJa": "Adaptive two sided laplace transforms: a learnable, interpretable, and scalable replacement for self-attention",
      "link": "https://arxiv.org/abs/2506.15714",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15714v1 Announce Type: new \nAbstract: We propose an innovative, learnable two-sided short-time Laplace transform (STLT) mechanism to supplant the traditional self attention in transformer-based LLMs. Our STLT introduces trainable parameters for each Laplace node, enabling end-to-end learning of decay rates , oscillatory frequencies, and window bandwidth T. This flexibility allows the model to dynamically adapt token relevance half lives and frequency responses during training. By selecting S learnable nodes and leveraging fast recursive convolution, we achieve an effective complexity of in time and memory. We further incorporate an efficient FFT-based computation of the relevance matrix and an adaptive node allocation mechanism to dynamically adjust the number of active Laplace nodes. Empirical results on language modeling (WikiText\\-103, Project Gutenberg), machine translation (WMT'14 En\\-De), and long document question answering (NarrativeQA) demonstrate that our learnable STLT achieves perplexities and scores on par with or better than existing efficient transformers while naturally extending to context lengths exceeding 100k tokens or more limited only by available hardware. Ablation studies confirm the importance of learnable parameters and adaptive node allocation. The proposed approach combines interpretability, through explicit decay and frequency parameters, with scalability and robustness, offering a pathway towards ultra-long-sequence language modeling without the computational bottleneck of self-attention.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration",
      "titleJa": "Bohdi: heterogeneous LLM fusion with automatic data exploration",
      "link": "https://arxiv.org/abs/2506.15721",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15721v1 Announce Type: new \nAbstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) reliance on real data from limited domain for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) fixed data allocation proportions across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities. Our code is available at https://github.com/gjq100/Bohdi.git.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Descriptor-based Foundation Models for Molecular Property Prediction",
      "titleJa": "Descriptor-based foundation models for molecular property prediction",
      "link": "https://arxiv.org/abs/2506.15792",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15792v1 Announce Type: new \nAbstract: Fast and accurate prediction of molecular properties with machine learning is pivotal to scientific advancements across myriad domains. Foundation models in particular have proven especially effective, enabling accurate training on small, real-world datasets. This study introduces CheMeleon, a novel molecular foundation model pre-trained on deterministic molecular descriptors from the Mordred package, leveraging a Directed Message-Passing Neural Network to predict these descriptors in a noise-free setting. Unlike conventional approaches relying on noisy experimental data or biased quantum mechanical simulations, CheMeleon uses low-noise molecular descriptors to learn rich molecular representations. Evaluated on 58 benchmark datasets from Polaris and MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks, outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop (36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%) and other foundation models. However, it struggles to distinguish activity cliffs like many of the tested models. The t-SNE projection of CheMeleon's learned representations demonstrates effective separation of chemical series, highlighting its ability to capture structural nuances. These results underscore the potential of descriptor-based pre-training for scalable and effective molecular property prediction, opening avenues for further exploration of descriptor sets and unlabeled datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Hidden Breakthroughs in Language Model Training",
      "titleJa": "Hidden breakthroughs in language モデル 学習",
      "link": "https://arxiv.org/abs/2506.15872",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15872v1 Announce Type: new \nAbstract: Loss curves are smooth during most of model training, so visible discontinuities stand out as possible conceptual breakthroughs. Studying these breakthroughs enables a deeper understanding of learning dynamics, but only when they are properly identified. This paper argues that similar breakthroughs occur frequently throughout training but they are obscured by a loss metric that collapses all variation into a single scalar. To find these hidden transitions, we introduce POLCA, a method for decomposing changes in loss along arbitrary bases of the low-rank training subspace. We use our method to identify clusters of samples that share similar changes in loss during training, disaggregating the overall loss into that of smaller groups of conceptually similar data. We validate our method on synthetic arithmetic and natural language tasks, showing that POLCA recovers clusters that represent interpretable breakthroughs in the model's capabilities. We demonstrate the promise of these hidden phase transitions as a tool for unsupervised interpretability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond",
      "titleJa": "On the theoretical understanding of identifiable sparse autoencoders and beyond",
      "link": "https://arxiv.org/abs/2506.15963",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15963v1 Announce Type: new \nAbstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting features learned by large language models (LLMs). It aims to recover complex superposed polysemantic features into interpretable monosemantic ones through feature reconstruction via sparsely activated neural networks. Despite the wide applications of SAEs, it remains unclear under what conditions an SAE can fully recover the ground truth monosemantic features from the superposed polysemantic ones. In this paper, through theoretical analysis, we for the first time propose the necessary and sufficient conditions for identifiable SAEs (SAEs that learn unique and ground truth monosemantic features), including 1) extreme sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3) enough hidden dimensions of SAEs. Moreover, when the identifiable conditions are not fully met, we propose a reweighting strategy to improve the identifiability. Specifically, following the theoretically suggested weight selection principle, we prove that the gap between the loss functions of SAE reconstruction and monosemantic feature reconstruction can be narrowed, so that the reweighted SAEs have better reconstruction of the ground truth monosemantic features than the uniformly weighted ones. In experiments, we validate our theoretical findings and show that our weighted SAE significantly improves feature monosemanticity and interpretability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Scalable Factorization Approach for High-Order Structured Tensor Recovery",
      "titleJa": "A scalable factorization approach for high-order structured tensor recovery",
      "link": "https://arxiv.org/abs/2506.16032",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16032v1 Announce Type: new \nAbstract: Tensor decompositions, which represent an $N$-order tensor using approximately $N$ factors of much smaller dimensions, can significantly reduce the number of parameters. This is particularly beneficial for high-order tensors, as the number of entries in a tensor grows exponentially with the order. Consequently, they are widely used in signal recovery and data analysis across domains such as signal processing, machine learning, and quantum physics. A computationally and memory-efficient approach to these problems is to optimize directly over the factors using local search algorithms such as gradient descent, a strategy known as the factorization approach in matrix and tensor optimization. However, the resulting optimization problems are highly nonconvex due to the multiplicative interactions between factors, posing significant challenges for convergence analysis and recovery guarantees.\n  In this paper, we present a unified framework for the factorization approach to solving various tensor decomposition problems. Specifically, by leveraging the canonical form of tensor decompositions--where most factors are constrained to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient descent (RGD) to optimize these orthonormal factors on the Stiefel manifold. Under a mild condition on the loss function, we establish a Riemannian regularity condition for the factorized objective and prove that RGD converges to the ground-truth tensor at a linear rate when properly initialized. Notably, both the initialization requirement and the convergence rate scale polynomially rather than exponentially with $N$, improving upon existing results for Tucker and tensor-train format tensors.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach",
      "titleJa": "Joint user priority and power scheduling for qos-aware wmmse precoding: a constrained-actor attentive-critic approach",
      "link": "https://arxiv.org/abs/2506.16074",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16074v1 Announce Type: new \nAbstract: 6G wireless networks are expected to support diverse quality-of-service (QoS) demands while maintaining high energy efficiency. Weighted Minimum Mean Square Error (WMMSE) precoding with fixed user priorities and transmit power is widely recognized for enhancing overall system performance but lacks flexibility to adapt to user-specific QoS requirements and time-varying channel conditions. To address this, we propose a novel constrained reinforcement learning (CRL) algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy network to dynamically allocate user priorities and power for WMMSE precoding. Specifically, CAAC integrates a Constrained Stochastic Successive Convex Approximation (CSSCA) method to optimize the policy, enabling more effective handling of energy efficiency goals and satisfaction of stochastic non-convex QoS constraints compared to traditional and existing CRL methods. Moreover, CAAC employs lightweight attention-enhanced Q-networks to evaluate policy updates without prior environment model knowledge. The network architecture not only enhances representational capacity but also boosts learning efficiency. Simulation results show that CAAC outperforms baselines in both energy efficiency and QoS satisfaction.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs",
      "titleJa": "Efficient and プライバシー-preserving soft prompt transfer for llms",
      "link": "https://arxiv.org/abs/2506.16196",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16196v1 Announce Type: new \nAbstract: Prompting has become a dominant paradigm for adapting large language models (LLMs). While discrete (textual) prompts are widely used for their interpretability, soft (parameter) prompts have recently gained traction in APIs. This is because they can encode information from more training samples while minimizing the user's token usage, leaving more space in the context window for task-specific input. However, soft prompts are tightly coupled to the LLM they are tuned on, limiting their generalization to other LLMs. This constraint is particularly problematic for efficiency and privacy: (1) tuning prompts on each LLM incurs high computational costs, especially as LLMs continue to grow in size. Additionally, (2) when the LLM is hosted externally, soft prompt tuning often requires sharing private data with the LLM provider. For instance, this is the case with the NVIDIA NeMo API. To address these issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that enables private tuning of soft prompts on a small model and subsequently transfers these prompts to a larger LLM. POST uses knowledge distillation to derive a small model directly from the large LLM to improve prompt transferability, tunes the soft prompt locally, optionally with differential privacy guarantees, and transfers it back to the larger LLM using a small public dataset. Our experiments show that POST reduces computational costs, preserves privacy, and effectively transfers high-utility soft prompts.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management",
      "titleJa": "From pixels to csi: distilling latent dynamics for efficient wireless resource management",
      "link": "https://arxiv.org/abs/2506.16216",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16216v1 Announce Type: new \nAbstract: In this work, we aim to optimize the radio resource management of a communication system between a remote controller and its device, whose state is represented through image frames, without compromising the performance of the control task. We propose a novel machine learning (ML) technique to jointly model and predict the dynamics of the control system as well as the wireless propagation environment in latent space. Our method leverages two coupled joint-embedding predictive architectures (JEPAs): a control JEPA models the control dynamics and guides the predictions of a wireless JEPA, which captures the dynamics of the device's channel state information (CSI) through cross-modal conditioning. We then train a deep reinforcement learning (RL) algorithm to derive a control policy from latent control dynamics and a power predictor to estimate scheduling intervals with favorable channel conditions based on latent CSI representations. As such, the controller minimizes the usage of radio resources by utilizing the coupled JEPA networks to imagine the device's trajectory in latent space. We present simulation results on synthetic multimodal data and show that our proposed approach reduces transmit power by over 50% while maintaining control performance comparable to baseline methods that do not account for wireless optimization.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design",
      "titleJa": "Active mri 買収 with diffusion guided bayesian experimental design",
      "link": "https://arxiv.org/abs/2506.16237",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16237v1 Announce Type: new \nAbstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging (MRI) in clinical settings is to accelerate acquisition times without significantly degrading image quality. This objective requires a balance between under-sampling the raw k-space measurements for faster acquisitions and gathering sufficient raw information for high-fidelity image reconstruction and analysis tasks. To achieve this balance, we propose to use sequential Bayesian experimental design (BED) to provide an adaptive and task-dependent selection of the most informative measurements. Measurements are sequentially augmented with new samples selected to maximize information gain on a posterior distribution over target images. Selection is performed via a gradient-based optimization of a design parameter that defines a subsampling pattern. In this work, we introduce a new active BED procedure that leverages diffusion-based generative models to handle the high dimensionality of the images and employs stochastic optimization to select among a variety of patterns while meeting the acquisition process constraints and budget. So doing, we show how our setting can optimize, not only standard image reconstruction, but also any associated image analysis task. The versatility and performance of our approach are demonstrated on several MRI acquisitions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Optimal Online Bookmaking for Any Number of Outcomes",
      "titleJa": "Optimal online bookmaking for any number of outcomes",
      "link": "https://arxiv.org/abs/2506.16253",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16253v1 Announce Type: new \nAbstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates betting odds on the possible outcomes of an event. In each betting round, the bookmaker can adjust the odds based on the cumulative betting behavior of gamblers, aiming to maximize profit while mitigating potential loss. We show that for any event and any number of betting rounds, in a worst-case setting over all possible gamblers and outcome realizations, the bookmaker's optimal loss is the largest root of a simple polynomial. Our solution shows that bookmakers can be as fair as desired while avoiding financial risk, and the explicit characterization reveals an intriguing relation between the bookmaker's regret and Hermite polynomials. We develop an efficient algorithm that computes the optimal bookmaking strategy: when facing an optimal gambler, the algorithm achieves the optimal loss, and in rounds where the gambler is suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a notion that is related to subgame perfect Nash equilibrium. The key technical contribution to achieve these results is an explicit characterization of the Bellman-Pareto frontier, which unifies the dynamic programming updates for Bellman's value function with the multi-criteria optimization framework of the Pareto frontier in the context of vector repeated games.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Data-Driven Policy Mapping for Safe RL-based Energy Management Systems",
      "titleJa": "Data-driven 政策 mapping for safe rl-based energy management systems",
      "link": "https://arxiv.org/abs/2506.16352",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16352v1 Announce Type: new \nAbstract: Increasing global energy demand and renewable integration complexity have placed buildings at the center of sustainable energy management. We present a three-step reinforcement learning(RL)-based Building Energy Management System (BEMS) that combines clustering, forecasting, and constrained policy learning to address scalability, adaptability, and safety challenges. First, we cluster non-shiftable load profiles to identify common consumption patterns, enabling policy generalization and transfer without retraining for each new building. Next, we integrate an LSTM based forecasting module to anticipate future states, improving the RL agents' responsiveness to dynamic conditions. Lastly, domain-informed action masking ensures safe exploration and operation, preventing harmful decisions. Evaluated on real-world data, our approach reduces operating costs by up to 15% for certain building types, maintains stable environmental performance, and quickly classifies and optimizes new buildings with limited data. It also adapts to stochastic tariff changes without retraining. Overall, this framework delivers scalable, robust, and cost-effective building energy management.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models",
      "titleJa": "Goalladder: incremental goal discovery with vision-language models",
      "link": "https://arxiv.org/abs/2506.16396",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16396v1 Announce Type: new \nAbstract: Natural language can offer a concise and human-interpretable means of specifying reinforcement learning (RL) tasks. The ability to extract rewards from a language instruction can enable the development of robotic systems that can learn from human guidance; however, it remains a challenging problem, especially in visual environments. Existing approaches that employ large, pretrained language models either rely on non-visual environment representations, require prohibitively large amounts of feedback, or generate noisy, ill-shaped reward functions. In this paper, we propose a novel method, $\\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL agents from a single language instruction in visual environments. GoalLadder works by incrementally discovering states that bring the agent closer to completing a task specified in natural language. To do so, it queries a VLM to identify states that represent an improvement in agent's task progress and to rank them using pairwise comparisons. Unlike prior work, GoalLadder does not trust VLM's feedback completely; instead, it uses it to rank potential goal states using an ELO-based rating system, thus reducing the detrimental effects of noisy VLM feedback. Over the course of training, the agent is tasked with minimising the distance to the top-ranked goal in a learned embedding space, which is trained on unlabelled visual data. This key feature allows us to bypass the need for abundant and accurate feedback typically required to train a well-shaped reward function. We demonstrate that GoalLadder outperforms existing related methods on classic control and robotic manipulation environments with the average final success rate of $\\sim$95% compared to only $\\sim$45% of the best competitor.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras",
      "titleJa": "An efficient neuromorphic approach for collision avoidance combining stack-cnn with event cameras",
      "link": "https://arxiv.org/abs/2506.16436",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16436v1 Announce Type: new \nAbstract: Space debris poses a significant threat, driving research into active and passive mitigation strategies. This work presents an innovative collision avoidance system utilizing event-based cameras - a novel imaging technology well-suited for Space Situational Awareness (SSA) and Space Traffic Management (STM). The system, employing a Stack-CNN algorithm (previously used for meteor detection), analyzes real-time event-based camera data to detect faint moving objects. Testing on terrestrial data demonstrates the algorithm's ability to enhance signal-to-noise ratio, offering a promising approach for on-board space imaging and improving STM/SSA operations.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
      "titleJa": "Sparselora: accelerating LLM fine-tuning with contextual sparsity",
      "link": "https://arxiv.org/abs/2506.16500",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16500v1 Announce Type: new \nAbstract: Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Robust Reward Modeling via Causal Rubrics",
      "titleJa": "Robust reward modeling via causal rubrics",
      "link": "https://arxiv.org/abs/2506.16507",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16507v1 Announce Type: new \nAbstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches",
      "titleJa": "Aligning asr evaluation with human and LLM judgments: intelligibility metrics using phonetic, semantic, and nli approaches",
      "link": "https://arxiv.org/abs/2506.16528",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16528v1 Announce Type: new \nAbstract: Traditional ASR metrics like WER and CER fail to capture intelligibility, especially for dysarthric and dysphonic speech, where semantic alignment matters more than exact word matches. ASR systems struggle with these speech types, often producing errors like phoneme repetitions and imprecise consonants, yet the meaning remains clear to human listeners. We identify two key challenges: (1) Existing metrics do not adequately reflect intelligibility, and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR transcripts of dysarthric speech remains underexplored. To address this, we propose a novel metric integrating Natural Language Inference (NLI) scores, semantic similarity, and phonetic similarity. Our ASR evaluation metric achieves a 0.890 correlation with human judgments on Speech Accessibility Project data, surpassing traditional methods and emphasizing the need to prioritize intelligibility over error-based measures.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification",
      "titleJa": "Isonet: causal analysis of multimodal transformers for neuromuscular gesture classification",
      "link": "https://arxiv.org/abs/2506.16744",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16744v1 Announce Type: new \nAbstract: Hand gestures are a primary output of the human motor system, yet the decoding of their neuromuscular signatures remains a bottleneck for basic neuroscience and assistive technologies such as prosthetics. Traditional human-machine interface pipelines rely on a single biosignal modality, but multimodal fusion can exploit complementary information from sensors. We systematically compare linear and attention-based fusion strategies across three architectures: a Multimodal MLP, a Multimodal Transformer, and a Hierarchical Transformer, evaluating performance on scenarios with unimodal and multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2 (sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force). Across both datasets, the Hierarchical Transformer with attention-based fusion consistently achieved the highest accuracy, surpassing the multimodal and best single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7% on HD-sEMG. To investigate how modalities interact, we introduce an Isolation Network that selectively silences unimodal or cross-modal attention pathways, quantifying each group of token interactions' contribution to downstream decisions. Ablations reveal that cross-modal interactions contribute approximately 30% of the decision signal across transformer layers, highlighting the importance of attention-driven fusion in harnessing complementary modality information. Together, these findings reveal when and how multimodal fusion would enhance biosignal classification and also provides mechanistic insights of human muscle activities. The study would be beneficial in the design of sensor arrays for neurorobotic systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs",
      "titleJa": "Predicting new 研究 directions in materials science using large language models and concept graphs",
      "link": "https://arxiv.org/abs/2506.16824",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16824v1 Announce Type: new \nAbstract: Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models",
      "titleJa": "Reward-agnostic prompt optimization for text-to-image diffusion models",
      "link": "https://arxiv.org/abs/2506.16853",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16853v1 Announce Type: new \nAbstract: We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) \\textit{without} requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a \"hint\") as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects. RATTPO surpasses other test-time search baselines in search efficiency, using up to 3.5 times less inference budget, and, given sufficient inference budget, achieves performance comparable to learning-based baselines that require reward-specific fine-tuning. The code is available at https://github.com/seminkim/RATTPO.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images",
      "titleJa": "From lab to factory: pitfalls and guidelines for self-/unsupervised defect detection on low-quality industrial images",
      "link": "https://arxiv.org/abs/2506.16890",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16890v1 Announce Type: new \nAbstract: The detection and localization of quality-related problems in industrially mass-produced products has historically relied on manual inspection, which is costly and error-prone. Machine learning has the potential to replace manual handling. As such, the desire is to facilitate an unsupervised (or self-supervised) approach, as it is often impossible to specify all conceivable defects ahead of time. A plethora of prior works have demonstrated the aptitude of common reconstruction-, embedding-, and synthesis-based methods in laboratory settings. However, in practice, we observe that most methods do not handle low data quality well or exude low robustness in unfavorable, but typical real-world settings. For practitioners it may be very difficult to identify the actual underlying problem when such methods underperform. Worse, often-reported metrics (e.g., AUROC) are rarely suitable in practice and may give misleading results. In our setting, we attempt to identify subtle anomalies on the surface of blasted forged metal parts, using rather low-quality RGB imagery only, which is a common industrial setting. We specifically evaluate two types of state-of-the-art models that allow us to identify and improve quality issues in production data, without having to obtain new data. Our contribution is to provide guardrails for practitioners that allow them to identify problems related to, e.g., (lack of) robustness or invariance, in either the chosen model or the data reliably in similar scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of likelihood-based approaches and outline a framework for proper empirical risk estimation that is more suitable for real-world scenarios.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics",
      "titleJa": "Rocketstack: a level-aware deep recursive ensemble learning framework with exploratory feature fusion and モデル pruning dynamics",
      "link": "https://arxiv.org/abs/2506.16965",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16965v1 Announce Type: new \nAbstract: Ensemble learning remains a cornerstone of machine learning, with stacking used to integrate predictions from multiple base learners through a meta-model. However, deep stacking remains rare, as most designs prioritize horizontal diversity over recursive depth due to model complexity, feature redundancy, and computational burden. To address these challenges, RocketStack, a level-aware recursive ensemble framework, is introduced and explored up to ten stacking levels, extending beyond prior architectures. The framework incrementally prunes weaker learners at each level, enabling deeper stacking without excessive complexity. To mitigate early performance saturation, mild Gaussian noise is added to out-of-fold (OOF) scores before pruning, and compared against strict OOF pruning. Further both per-level and periodic feature compressions are explored using attention-based selection, Simple, Fast, Efficient (SFE) filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class), linear-trend tests confirmed rising accuracy with depth in most variants, and the top performing meta-model at each level increasingly outperformed the strongest standalone ensemble. In the binary subset, periodic SFE with mild OOF-score randomization reached 97.08% at level 10, 5.14% above the strict-pruning configuration and cut runtime by 10.5% relative to no compression. In the multi-class subset, periodic attention selection reached 98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing runtime by 56.1% and feature dimensionality by 74% compared to no compression. These findings highlight mild randomization as an effective regularizer and periodic compression as a stabilizer. Echoing the design of multistage rockets in aerospace (prune, compress, propel) RocketStack achieves deep recursive ensembling with tractable complexity.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning",
      "titleJa": "Bread: branched rollouts from expert anchors bridge sft & rl for reasoning",
      "link": "https://arxiv.org/abs/2506.17211",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17211v1 Announce Type: new \nAbstract: Small language models (SLMs) struggle to learn complex reasoning behaviors, especially when high-quality traces are scarce or difficult to learn from. The standard training approach combines a supervised fine-tuning (SFT) stage, often to distill capabilities of a larger model, followed by a reinforcement learning (RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we investigate the fundamental limitations of this SFT + RL paradigm and propose methods to overcome them. Under a suitable theoretical model, we demonstrate that the SFT + RL strategy can fail completely when (1) the expert's traces are too difficult for the small model to express, or (2) the small model's initialization has exponentially small likelihood of success. To address these, we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via partial expert guidance and branched rollouts. When self-generated traces fail, BREAD adaptively inserts short expert prefixes/hints, allowing the small model to complete the rest of the reasoning path, and ensuring that each update includes at least one successful trace. This mechanism both densifies the reward signal and induces a natural learning curriculum. BREAD requires fewer than 40% of ground-truth traces, consistently outperforming standard GRPO while speeding up the training by about 3 times. Importantly, we demonstrate that BREAD helps the model solve problems that are otherwise unsolvable by the SFT + RL strategy, highlighting how branched rollouts and expert guidance can substantially boost SLM reasoning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms",
      "titleJa": "Compilation, optimization, error mitigation, and 機械学習 in quantum algorithms",
      "link": "https://arxiv.org/abs/2506.15760",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15760v1 Announce Type: cross \nAbstract: This paper discusses the compilation, optimization, and error mitigation of quantum algorithms, essential steps to execute real-world quantum algorithms. Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take advantage of existing high-performance computing power with quantum-enabled exponential speedups. The proposed approximate quantum Fourier transform (AQFT) for quantum algorithm optimization improves the circuit execution on top of an exponential speed-ups the quantum Fourier transform has provided.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Implicit neural representations for accurate estimation of the standard model of white matter",
      "titleJa": "Implicit neural representations for accurate estimation of the standard モデル of white matter",
      "link": "https://arxiv.org/abs/2506.15762",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15762v1 Announce Type: cross \nAbstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive investigation of tissue microstructure. The Standard Model (SM) of white matter aims to disentangle dMRI signal contributions from intra- and extra-axonal water compartments. However, due to the model its high-dimensional nature, extensive acquisition protocols with multiple b-values and diffusion tensor shapes are typically required to mitigate parameter degeneracies. Even then, accurate estimation remains challenging due to noise. This work introduces a novel estimation framework based on implicit neural representations (INRs), which incorporate spatial regularization through the sinusoidal encoding of the input coordinates. The INR method is evaluated on both synthetic and in vivo datasets and compared to parameter estimates using cubic polynomials, supervised neural networks, and nonlinear least squares. Results demonstrate superior accuracy of the INR method in estimating SM parameters, particularly in low signal-to-noise conditions. Additionally, spatial upsampling of the INR can represent the underlying dataset anatomically plausibly in a continuous way, which is unattainable with linear or cubic interpolation. The INR is fully unsupervised, eliminating the need for labeled training data. It achieves fast inference ($\\sim$6 minutes), is robust to both Gaussian and Rician noise, supports joint estimation of SM kernel parameters and the fiber orientation distribution function with spherical harmonics orders up to at least 8 and non-negativity constraints, and accommodates spatially varying acquisition protocols caused by magnetic gradient non-uniformities. The combination of these properties along with the possibility to easily adapt the framework to other dMRI models, positions INRs as a potentially important tool for analyzing and interpreting diffusion MRI data.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Superconducting Qubit Readout Using Next-Generation Reservoir Computing",
      "titleJa": "Superconducting qubit readout using next-generation reservoir computing",
      "link": "https://arxiv.org/abs/2506.15771",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15771v1 Announce Type: cross \nAbstract: Quantum processors require rapid and high-fidelity simultaneous measurements of many qubits. While superconducting qubits are among the leading modalities toward a useful quantum processor, their readout remains a bottleneck. Traditional approaches to processing measurement data often struggle to account for crosstalk present in frequency-multiplexed readout, the preferred method to reduce the resource overhead. Recent approaches to address this challenge use neural networks to improve the state-discrimination fidelity. However, they are computationally expensive to train and evaluate, resulting in increased latency and poor scalability as the number of qubits increases. We present an alternative machine learning approach based on next-generation reservoir computing that constructs polynomial features from the measurement signals and maps them to the corresponding qubit states. This method is highly parallelizable, avoids the costly nonlinear activation functions common in neural networks, and supports real-time training, enabling fast evaluation, adaptability, and scalability. Despite its lower computational complexity, our reservoir approach is able to maintain high qubit-state-discrimination fidelity. Relative to traditional methods, our approach achieves error reductions of up to 50% and 11% on single- and five-qubit datasets, respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit dataset. Compared with recent machine-learning methods, evaluating our model requires 100x fewer multiplications for single-qubit and 2.5x fewer for five-qubit models. This work demonstrates that reservoir computing can enhance qubit-state discrimination while maintaining scalability for future quantum processors.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation",
      "titleJa": "プライバシー-preserving in connected and 自律的 vehicles through vision to text transformation",
      "link": "https://arxiv.org/abs/2506.15854",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15854v1 Announce Type: cross \nAbstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77\\% and Detail Density by around 50\\% compared to existing approaches.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls",
      "titleJa": "Contactless precision steering of particles in a fluid inside a cube with rotating walls",
      "link": "https://arxiv.org/abs/2506.15958",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15958v1 Announce Type: cross \nAbstract: Contactless manipulation of small objects is essential for biomedical and chemical applications, such as cell analysis, assisted fertilisation, and precision chemistry. Established methods, including optical, acoustic, and magnetic tweezers, are now complemented by flow control techniques that use flow-induced motion to enable precise and versatile manipulation. However, trapping multiple particles in fluid remains a challenge. This study introduces a novel control algorithm capable of steering multiple particles in flow. The system uses rotating disks to generate flow fields that transport particles to precise locations. Disk rotations are governed by a feedback control policy based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid dynamics equations with path objectives into a single loss function. Our experiments, conducted in both simulations and with the physical device, demonstrate the capability of the approach to transport two beads simultaneously to predefined locations, advancing robust contactless particle manipulation for biomedical applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Feedback-driven recurrent quantum neural network universality",
      "titleJa": "Feedback-driven recurrent quantum ニューラルネットワーク universality",
      "link": "https://arxiv.org/abs/2506.16332",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16332v1 Announce Type: cross \nAbstract: Quantum reservoir computing uses the dynamics of quantum systems to process temporal data, making it particularly well-suited for learning with noisy intermediate-scale quantum devices. Early experimental proposals, such as the restarting and rewinding protocols, relied on repeating previous steps of the quantum map to avoid backaction. However, this approach compromises real-time processing and increases computational overhead. Recent developments have introduced alternative protocols that address these limitations. These include online, mid-circuit measurement, and feedback techniques, which enable real-time computation while preserving the input history. Among these, the feedback protocol stands out for its ability to process temporal information with comparatively fewer components. Despite this potential advantage, the theoretical foundations of feedback-based quantum reservoir computing remain underdeveloped, particularly with regard to the universality and the approximation capabilities of this approach. This paper addresses this issue by presenting a recurrent quantum neural network architecture that extends a class of existing feedforward models to a dynamic, feedback-driven reservoir setting. We provide theoretical guarantees for variational recurrent quantum neural networks, including approximation bounds and universality results. Notably, our analysis demonstrates that the model is universal with linear readouts, making it both powerful and experimentally accessible. These results pave the way for practical and theoretically grounded quantum reservoir computing with real-time processing capabilities.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System",
      "titleJa": "Initial investigation of LLM-assisted development of rule-based clinical nlp system",
      "link": "https://arxiv.org/abs/2506.16628",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16628v1 Announce Type: cross \nAbstract: Despite advances in machine learning (ML) and large language models (LLMs), rule-based natural language processing (NLP) systems remain active in clinical settings due to their interpretability and operational efficiency. However, their manual development and maintenance are labor-intensive, particularly in tasks with large linguistic variability. To overcome these limitations, we proposed a novel approach employing LLMs solely during the rule-based systems development phase. We conducted the initial experiments focusing on the first two steps of developing a rule-based NLP pipeline: find relevant snippets from the clinical note; extract informative keywords from the snippets for the rule-based named entity recognition (NER) component. Our experiments demonstrated exceptional recall in identifying clinically relevant text snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER. This study sheds light on a promising new direction for NLP development, enabling semi-automated or automated development of rule-based systems with significantly faster, more cost-effective, and transparent execution compared with deep learning model-based solutions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test",
      "titleJa": "Enhancing expressivity of quantum neural networks based on the swap test",
      "link": "https://arxiv.org/abs/2506.16938",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16938v1 Announce Type: cross \nAbstract: Parameterized quantum circuits represent promising architectures for machine learning applications, yet many lack clear connections to classical models, potentially limiting their ability to translate the wide success of classical neural networks to the quantum realm. We examine a specific type of quantum neural network (QNN) built exclusively from SWAP test circuits, and discuss its mathematical equivalence to a classical two-layer feedforward network with quadratic activation functions under amplitude encoding. Our analysis across classical real-world and synthetic datasets reveals that while this architecture can successfully learn many practical tasks, it exhibits fundamental expressivity limitations due to violating the universal approximation theorem, particularly failing on harder problems like the parity check function. To address this limitation, we introduce a circuit modification using generalized SWAP test circuits that effectively implements classical neural networks with product layers. This enhancement enables successful learning of parity check functions in arbitrary dimensions which we analytically argue to be impossible for the original architecture beyond two dimensions regardless of network size. Our results establish a framework for enhancing QNN expressivity through classical task analysis and demonstrate that our SWAP test-based architecture offers broad representational capacity, suggesting potential promise also for quantum learning tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models",
      "titleJa": "Mucar: benchmarking multilingual cross-modal ambiguity resolution for multimodal large language models",
      "link": "https://arxiv.org/abs/2506.17046",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17046v1 Announce Type: cross \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong image-text alignment capability, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models--encompassing both open-source and proprietary architectures--reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions",
      "titleJa": "Empowering near-field communications in low-altitude economy with LLM: fundamentals, potentials, solutions, and future directions",
      "link": "https://arxiv.org/abs/2506.17067",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17067v1 Announce Type: cross \nAbstract: The low-altitude economy (LAE) is gaining significant attention from academia and industry. Fortunately, LAE naturally aligns with near-field communications in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles, while the additional distance dimension boosts overall spectrum efficiency. However, near-field communications in LAE still face several challenges, such as the increase in signal processing complexity and the necessity of distinguishing between far and near-field users. Inspired by the large language models (LLM) with powerful ability to handle complex problems, we apply LLM to solve challenges of near-field communications in LAE. The objective of this article is to provide a comprehensive analysis and discussion on LLM-empowered near-field communications in LAE. Specifically, we first introduce fundamentals of LLM and near-field communications, including the key advantages of LLM and key characteristics of near-field communications. Then, we reveal the opportunities and challenges of near-field communications in LAE. To address these challenges, we present a LLM-based scheme for near-field communications in LAE, and provide a case study which jointly distinguishes far and near-field users and designs multi-user precoding matrix. Finally, we outline and highlight several future research directions and open issues.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Deep-Reinforcement-Learning-Based AoI-Aware Resource Allocation for RIS-Aided IoV Networks",
      "titleJa": "Deep-reinforcement-learning-based aoi-aware resource allocation for ris-aided iov networks",
      "link": "https://arxiv.org/abs/2406.11245",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.11245v2 Announce Type: replace \nAbstract: Reconfigurable Intelligent Surface (RIS) is a pivotal technology in communication, offering an alternative path that significantly enhances the link quality in wireless communication environments. In this paper, we propose a RIS-assisted internet of vehicles (IoV) network, considering the vehicle-to-everything (V2X) communication method. In addition, in order to improve the timeliness of vehicle-to-infrastructure (V2I) links and the stability of vehicle-to-vehicle (V2V) links, we introduce the age of information (AoI) model and the payload transmission probability model. Therefore, with the objective of minimizing the AoI of V2I links and prioritizing transmission of V2V links payload, we construct this optimization problem as an Markov decision process (MDP) problem in which the BS serves as an agent to allocate resources and control phase-shift for the vehicles using the soft actor-critic (SAC) algorithm, which gradually converges and maintains a high stability. A AoI-aware joint vehicular resource allocation and RIS phase-shift control scheme based on SAC algorithm is proposed and simulation results show that its convergence speed, cumulative reward, AoI performance, and payload transmission probability outperforms those of proximal policy optimization (PPO), deep deterministic policy gradient (DDPG), twin delayed deep deterministic policy gradient (TD3) and stochastic algorithms.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations",
      "titleJa": "Medha: efficiently serving multi-million context length LLM 推論 requests without approximations",
      "link": "https://arxiv.org/abs/2409.17264",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2409.17264v4 Announce Type: replace \nAbstract: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.",
      "summary": "arXiv:2409.",
      "summaryJa": "Arxiv:2409.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework",
      "titleJa": "Cos-dpo: conditioned one-shot multi-objective fine-tuning framework",
      "link": "https://arxiv.org/abs/2410.08316",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.08316v3 Announce Type: replace \nAbstract: In LLM alignment and many other ML applications, one often faces the Multi-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model with datasets labeled w.r.t. different objectives simultaneously. To address the challenge, we propose a Conditioned One-Shot fine-tuning framework (COS-DPO) that extends the Direct Preference Optimization technique, originally developed for efficient LLM alignment with preference data, to accommodate the MOFT settings. By direct conditioning on the weight across auxiliary objectives, our Weight-COS-DPO method enjoys an efficient one-shot training process for profiling the Pareto front and is capable of achieving comprehensive trade-off solutions even in the post-training stage. Based on our theoretical findings on the linear transformation properties of the loss function, we further propose the Temperature-COS-DPO method that augments the temperature parameter to the model input, enhancing the flexibility of post-training control over the trade-offs between the main and auxiliary objectives. We demonstrate the effectiveness and efficiency of the COS-DPO framework through its applications to various tasks, including the Learning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for large-scale ML deployments.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Training Multi-Layer Binary Neural Networks With Local Binary Error Signals",
      "titleJa": "学習 multi-layer binary neural networks with local binary error signals",
      "link": "https://arxiv.org/abs/2412.00119",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2412.00119v3 Announce Type: replace \nAbstract: Binary Neural Networks (BNNs) significantly reduce computational complexity and memory usage in machine and deep learning by representing weights and activations with just one bit. However, most existing training algorithms for BNNs rely on quantization-aware floating-point Stochastic Gradient Descent (SGD), limiting the full exploitation of binary operations to the inference phase only. In this work, we propose, for the first time, a fully binary and gradient-free training algorithm for multi-layer BNNs, eliminating the need for back-propagated floating-point gradients. Specifically, the proposed algorithm relies on local binary error signals and binary weight updates, employing integer-valued hidden weights that serve as a synaptic metaplasticity mechanism, thereby enhancing its neurobiological plausibility. Our proposed solution enables the training of binary multi-layer perceptrons by using exclusively XNOR, Popcount, and increment/decrement operations. Experimental results on multi-class classification benchmarks show test accuracy improvements of up to +35.47% over the only existing fully binary single-layer state-of-the-art solution. Compared to full-precision SGD, our solution improves test accuracy by up to +35.30% under the same total memory demand, while also reducing computational cost by two to three orders of magnitude in terms of the total number of Boolean gates. The proposed algorithm is made available to the scientific community as a public repository.",
      "summary": "arXiv:2412.",
      "summaryJa": "Arxiv:2412.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping",
      "titleJa": "Ladder-residual: parallelism-aware architecture for accelerating large モデル 推論 with communication overlapping",
      "link": "https://arxiv.org/abs/2501.06589",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.06589v5 Announce Type: replace \nAbstract: Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 29% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens. We release our code for training and inference for easier replication of experiments.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Celo: Training Versatile Learned Optimizers on a Compute Diet",
      "titleJa": "Celo: 学習 versatile learned optimizers on a compute diet",
      "link": "https://arxiv.org/abs/2501.12670",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.12670v2 Announce Type: replace \nAbstract: Learned optimization has emerged as a promising alternative to hand-crafted optimizers, with the potential to discover stronger learned update rules that enable faster, hyperparameter-free training of neural networks. A critical element for practically useful learned optimizers, that can be used off-the-shelf after meta-training, is strong meta-generalization: the ability to apply the optimizers to new tasks. Recent state-of-the-art work in learned optimizers, VeLO (Metz et al., 2022), requires a large number of highly diverse meta-training tasks along with massive computational resources, 4000 TPU months, to achieve meta-generalization. This makes further improvements to such learned optimizers impractical. In this work, we identify several key elements in learned optimizer architectures and meta-training procedures that can lead to strong meta-generalization. We also propose evaluation metrics to reliably assess quantitative performance of an optimizer at scale on a set of evaluation tasks. Our proposed approach, Celo, makes a significant leap in improving the meta-generalization performance of learned optimizers and also outperforms tuned state-of-the-art optimizers on a diverse set of out-of-distribution tasks, despite being meta-trained for just 24 GPU hours.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "On Almost Surely Safe Alignment of Large Language Models at Inference-Time",
      "titleJa": "On almost surely safe alignment of large language models at 推論-time",
      "link": "https://arxiv.org/abs/2502.01208",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.01208v3 Announce Type: replace \nAbstract: We introduce a novel inference-time alignment approach for LLMs that aims to generate safe responses almost surely, i.e., with probability approaching one. Our approach models the generation of safe responses as a constrained Markov Decision Process (MDP) within the LLM's latent space. We augment a safety state that tracks the evolution of safety constraints and dynamically penalize unsafe generations to ensure the generation of safe responses. Consequently, we demonstrate formal safety guarantees w.r.t. the given cost model upon solving the MDP in the latent space with sufficiently large penalties. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate that InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. Our findings contribute to the advancement of safer LLM deployment through alignment at inference-time, thus presenting a promising alternative to resource-intensive, overfitting-prone alignment techniques like RLHF.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models",
      "titleJa": "Membership 推論 attack should move on to distributional statistics for distilled generative models",
      "link": "https://arxiv.org/abs/2502.02970",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.02970v3 Announce Type: replace \nAbstract: To detect unauthorized data usage in training large-scale generative models (e.g., ChatGPT or Midjourney), membership inference attacks (MIA) have proven effective in distinguishing a single training instance (a member) from a single non-training instance (a non-member). This success is mainly credited to a memorization effect: models tend to perform better on a member than a non-member. However, we find that standard MIAs fail against distilled generative models (i.e., student models) that are increasingly deployed in practice for efficiency (e.g., ChatGPT 4o-mini). Trained exclusively on data generated from a large-scale model (a teacher model), the student model lacks direct exposure to any members (teacher's training data), nullifying the memorization effect that standard MIAs rely on. This finding reveals a serious privacy loophole, where generation-service providers could deploy a student model whose teacher was potentially trained on unauthorized data, yet claim the deployed model is clean because it was not directly trained on such data. Hence, are distilled models inherently unauditable for upstream privacy violations, and should we discard them when we care about privacy? We contend no, as we uncover a memory chain connecting the student and teacher's member data: the distribution of student-generated data aligns more closely with the distribution of the teacher's members than with non-members, thus we can detect unauthorized data usage even when direct instance-level memorization is absent. This leads us to posit that MIAs on distilled generative models should shift from instance-level scores to distribution-level statistics. We further propose three principles of distribution-based MIAs for detecting unauthorized training data through distilled generative models, and validate our position through an exemplar framework. We lastly discuss the implications of our position.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance Analysis",
      "titleJa": "Dvfs-aware dnn 推論 on gpus: latency modeling and performance analysis",
      "link": "https://arxiv.org/abs/2502.06295",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.06295v2 Announce Type: replace \nAbstract: The rapid development of deep neural networks (DNNs) is inherently accompanied by the problem of high computational costs. To tackle this challenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising technology for balancing the latency and energy consumption of DNN inference by adjusting the computing frequency of processors. However, most existing models of DNN inference time are based on the CPU-DVFS technique, and directly applying the CPU-DVFS model to DNN inference on GPUs will lead to significant errors in optimizing latency and energy consumption. In this paper, we propose a DVFS-aware latency model to precisely characterize DNN inference time on GPUs. We first formulate the DNN inference time based on extensive experiment results for different devices and analyze the impact of fitting parameters. Then by dividing DNNs into multiple blocks and obtaining the actual inference time, the proposed model is further verified. Finally, we compare our proposed model with the CPU-DVFS model in two specific cases. Evaluation results demonstrate that local inference optimization with our proposed model achieves a reduction of no less than 66% and 69% in inference time and energy consumption respectively. In addition, cooperative inference with our proposed model can improve the partition policy and reduce the energy consumption compared to the CPU-DVFS model.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Provably Efficient Online RLHF with One-Pass Reward Modeling",
      "titleJa": "Provably efficient online rlhf with one-pass reward modeling",
      "link": "https://arxiv.org/abs/2502.07193",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.07193v2 Announce Type: replace \nAbstract: Reinforcement Learning from Human Feedback (RLHF) has shown remarkable success in aligning Large Language Models (LLMs) with human preferences. Traditional RLHF approaches rely on a fixed dataset, which often suffers from limited coverage. To this end, online RLHF has emerged as a promising direction, enabling iterative data collection and model improvement. Despite its potential, this paradigm faces a key bottleneck: the requirement to continuously integrate new data into the historical dataset and re-optimize the model from scratch at each iteration, resulting in computational and storage costs that grow linearly with the number of iterations. In this work, we address this challenge by proposing a one-pass reward modeling method that does not require storing the historical data and can be computed in constant time. Specifically, we first formalize RLHF as a contextual preference bandit problem and design an online mirror descent algorithm with a tailored local norm to replace the standard maximum likelihood estimation for reward modeling. We then apply our method to various online RLHF settings, including passive data collection, active data collection, and deployment-time adaptation. We provide theoretical guarantees showing that our method improves both statistical and computational efficiency. Finally, we provide practical algorithms and conduct experiments using Llama-3-8B-Instruct and Qwen2.5-7B-Instruct models on the Ultrafeedback-binarized and Mixture2 datasets, validating the effectiveness of our proposed method.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness",
      "titleJa": "Multi-agent multi-armed bandits with minimum reward guarantee fairness",
      "link": "https://arxiv.org/abs/2502.15240",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.15240v2 Announce Type: replace \nAbstract: We investigate the problem of maximizing social welfare while ensuring fairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem, a centralized decision-maker takes actions over time, generating random rewards for various agents. Our goal is to maximize the sum of expected cumulative rewards, a.k.a. social welfare, while ensuring that each agent receives an expected reward that is at least a constant fraction of the maximum possible expected reward.\n  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound (UCB) technique to achieve sublinear regret bounds for both fairness and social welfare. The fairness regret measures the positive difference between the minimum reward guarantee and the expected reward of a given policy, whereas the social welfare regret measures the difference between the social welfare of the optimal fair policy and that of the given policy.\n  We show that RewardFairUCB algorithm achieves instance-independent social welfare regret guarantees of $\\tilde{O}(T^{1/2})$ and a fairness regret upper bound of $\\tilde{O}(T^{3/4})$. We also give the lower bound of $\\Omega(\\sqrt{T})$ for both social welfare and fairness regret. We evaluate RewardFairUCB's performance against various baseline and heuristic algorithms using simulated data and real world data, highlighting trade-offs between fairness and social welfare regrets.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
      "titleJa": "Robust hallucination detection in llms via adaptive token selection",
      "link": "https://arxiv.org/abs/2504.07863",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.07863v2 Announce Type: replace \nAbstract: Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "LLM Agents for Bargaining with Utility-based Feedback",
      "titleJa": "LLM agents for bargaining with utility-based feedback",
      "link": "https://arxiv.org/abs/2505.22998",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.22998v2 Announce Type: replace \nAbstract: Bargaining, a critical aspect of real-world interactions, presents challenges for large language models (LLMs) due to limitations in strategic depth and adaptation to complex human factors. Existing benchmarks often fail to capture this real-world complexity. To address this and enhance LLM capabilities in realistic bargaining, we introduce a comprehensive framework centered on utility-based feedback. Our contributions are threefold: (1) BargainArena, a novel benchmark dataset with six intricate scenarios (e.g., deceptive practices, monopolies) to facilitate diverse strategy modeling; (2) human-aligned, economically-grounded evaluation metrics inspired by utility theory, incorporating agent utility and negotiation power, which implicitly reflect and promote opponent-aware reasoning (OAR); and (3) a structured feedback mechanism enabling LLMs to iteratively refine their bargaining strategies. This mechanism can positively collaborate with in-context learning (ICL) prompts, including those explicitly designed to foster OAR. Experimental results show that LLMs often exhibit negotiation strategies misaligned with human preferences, and that our structured feedback mechanism significantly improves their performance, yielding deeper strategic and opponent-aware reasoning.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Kinetics: Rethinking Test-Time Scaling Laws",
      "titleJa": "Kinetics: rethinking test-time scaling laws",
      "link": "https://arxiv.org/abs/2506.05333",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.05333v3 Announce Type: replace \nAbstract: We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Domain Specific Benchmarks for Evaluating Multimodal Large Language Models",
      "titleJa": "Domain specific benchmarks for evaluating multimodal large language models",
      "link": "https://arxiv.org/abs/2506.12958",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12958v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly being deployed across disciplines due to their advanced reasoning and problem solving capabilities. To measure their effectiveness, various benchmarks have been developed that measure aspects of LLM reasoning, comprehension, and problem-solving. While several surveys address LLM evaluation and benchmarks, a domain-specific analysis remains underexplored in the literature. This paper introduces a taxonomy of seven key disciplines, encompassing various domains and application areas where LLMs are extensively utilized. Additionally, we provide a comprehensive review of LLM benchmarks and survey papers within each domain, highlighting the unique capabilities of LLMs and the challenges faced in their application. Finally, we compile and categorize these benchmarks by domain to create an accessible resource for researchers, aiming to pave the way for advancements toward artificial general intelligence (AGI)",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
      "titleJa": "Calibrated predictive lower bounds on time-to-unsafe-sampling in llms",
      "link": "https://arxiv.org/abs/2506.13593",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13593v2 Announce Type: replace \nAbstract: We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model (LLM) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned LLMs, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the LPB, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems",
      "titleJa": "Quantum-informed contrastive learning with dynamic mixup augmentation for class-imbalanced expert systems",
      "link": "https://arxiv.org/abs/2506.13987",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13987v2 Announce Type: replace \nAbstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Solving a class of stochastic optimal control problems by physics-informed neural networks",
      "titleJa": "Solving a class of stochastic optimal control problems by physics-informed neural networks",
      "link": "https://arxiv.org/abs/2402.15592",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2402.15592v2 Announce Type: replace-cross \nAbstract: The aim of this work is to develop a deep learning method for solving high-dimensional stochastic control problems based on the Hamilton--Jacobi--Bellman (HJB) equation and physics-informed learning. Our approach is to parameterize the feedback control and the value function using a decoupled neural network with multiple outputs. We train this network by using a loss function with penalty terms that enforce the HJB equation along the sampled trajectories generated by the controlled system. More significantly, numerical results on various applications are carried out to demonstrate that the proposed approach is efficient and applicable.",
      "summary": "arXiv:2402.",
      "summaryJa": "Arxiv:2402.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Unraveling the Interplay between Carryover Effects and Reward Autocorrelations in Switchback Experiments",
      "titleJa": "Unraveling the interplay between carryover effects and reward autocorrelations in switchback experiments",
      "link": "https://arxiv.org/abs/2403.17285",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2403.17285v4 Announce Type: replace-cross \nAbstract: A/B testing has become the gold standard for policy evaluation in modern technological industries. Motivated by the widespread use of switchback experiments in A/B testing, this paper conducts a comprehensive comparative analysis of various switchback designs in Markovian environments. Unlike many existing works which derive the optimal design based on specific and relatively simple estimators, our analysis covers a range of state-of-the-art estimators developed in the reinforcement learning (RL) literature. It reveals that the effectiveness of different switchback designs depends crucially on (i) the size of the carryover effect and (ii) the auto-correlations among reward errors over time. Meanwhile, these findings are estimator-agnostic, i.e., they apply to most RL estimators. Based on these insights, we provide a workflow to offer guidelines for practitioners on designing switchback experiments in A/B testing.",
      "summary": "arXiv:2403.",
      "summaryJa": "Arxiv:2403.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "A Sparse Tensor Generator with Efficient Feature Extraction",
      "titleJa": "A sparse tensor generator with efficient feature extraction",
      "link": "https://arxiv.org/abs/2405.04944",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.04944v3 Announce Type: replace-cross \nAbstract: Sparse tensor operations are increasingly important in diverse applications such as social networks, deep learning, diagnosis, crime, and review analysis. However, a major obstacle in sparse tensor research is the lack of large-scale sparse tensor datasets. Another challenge lies in analyzing sparse tensor features, which are essential not only for understanding the nonzero pattern but also for selecting the most suitable storage format, decomposition algorithm, and reordering methods. However, due to the large size of real-world tensors, even extracting these features can be computationally expensive without careful optimization. To address these limitations, we have developed a smart sparse tensor generator that replicates key characteristics of real sparse tensors. Additionally, we propose efficient methods for extracting a comprehensive set of sparse tensor features. The effectiveness of our generator is validated through the quality of extracted features and the performance of decomposition on the generated tensors. Both the sparse tensor feature extractor and the tensor generator are open source with all the artifacts available at https://github.com/sparcityeu/FeaTensor and https://github.com/sparcityeu/GenTensor, respectively.",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Reconfigurable Intelligent Surface Assisted VEC Based on Multi-Agent Reinforcement Learning",
      "titleJa": "Reconfigurable intelligent surface assisted vec based on multi-agent 強化学習",
      "link": "https://arxiv.org/abs/2406.11318",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.11318v2 Announce Type: replace-cross \nAbstract: Vehicular edge computing (VEC) is an emerging technology that enables vehicles to perform high-intensity tasks by executing tasks locally or offloading them to nearby edge devices. However, obstacles such as buildings may degrade the communications and incur communication interruptions, and thus the vehicle may not meet the requirement for task offloading. Reconfigurable intelligent surfaces (RIS) is introduced to support vehicle communication and provide an alternative communication path. The system performance can be improved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC system where tasks arrive randomly, we design a control scheme that considers offloading power, local power allocation and phase-shift optimization. To solve this non-convex problem, we propose a new deep reinforcement learning (DRL) framework that employs modified multi-agent deep deterministic policy gradient (MADDPG) approach to optimize the power allocation for vehicle users (VUs) and block coordinate descent (BCD) algorithm to optimize the phase-shift of the RIS. Simulation results show that our proposed scheme outperforms the centralized deep deterministic policy gradient (DDPG) scheme and random scheme.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation",
      "titleJa": "Alphatrans: a neuro-symbolic compositional approach for repository-level code translation and validation",
      "link": "https://arxiv.org/abs/2410.24117",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.24117v5 Announce Type: replace-cross \nAbstract: Code translation transforms programs from one programming language (PL) to another. Several rule-based transpilers have been designed to automate code translation between different pairs of PLs. However, the rules can become obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies have explored the automation of code translation using Large Language Models (LLMs). One key observation is that such techniques may work well for crafted benchmarks but fail to generalize to the scale and complexity of real-world projects with dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-level code translation. AlphaTrans translates both source and test code, and employs multiple levels of validation to ensure the translation preserves the functionality of the source program. To break down the problem for LLMs, AlphaTrans leverages program analysis to decompose the program into fragments and translates them in the reverse call order. We leveraged AlphaTrans to translate ten real-world open-source projects consisting of  classes, methods, and tests. AlphaTrans breaks down these projects into 17874 fragments and translates the entire repository. 96.40% of the translated fragments are syntactically correct, and AlphaTrans validates the translations' runtime behavior and functional correctness for 27.03% and 25.14% of fragments. On average, the integrated translation and validation take 34 hours to translate a project, showing its scalability in practice. For the incorrect translations, AlphaTrans generates a report including existing translation, stack trace, test errors, or assertion failures. We provided these artifacts to two developers to fix the translation bugs in four projects. They were able to fix the issues in 20.1 hours on average and achieve all passing tests.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Problem Space Transformations for Out-of-Distribution Generalisation in Behavioural Cloning",
      "titleJa": "Problem space transformations for out-of-distribution generalisation in behavioural cloning",
      "link": "https://arxiv.org/abs/2411.04056",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.04056v2 Announce Type: replace-cross \nAbstract: The combination of behavioural cloning and neural networks has driven significant progress in robotic manipulation. As these algorithms may require a large number of demonstrations for each task of interest, they remain fundamentally inefficient in complex scenarios, in which finite datasets can hardly cover the state space. One of the remaining challenges is thus out-of-distribution (OOD) generalisation, i.e. the ability to predict correct actions for states with a low likelihood with respect to the state occupancy induced by the dataset. This issue is aggravated when the system to control is treated as a black-box, ignoring its physical properties. This work characterises widespread properties of robotic manipulation, specifically pose equivariance and locality. We investigate the effect of the choice of problem space on OOD performance of BC policies and how transformations arising from characteristic properties of manipulation could be employed for its improvement. We empirically demonstrate that these transformations allow behaviour cloning policies, using either standard MLP-based one-step action prediction or diffusion-based action-sequence prediction, to generalise better to OOD problem instances.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models",
      "titleJa": "On domain-adaptive post-学習 for multimodal large language models",
      "link": "https://arxiv.org/abs/2411.19930",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.19930v3 Announce Type: replace-cross \nAbstract: Adapting general multimodal large language models (MLLMs) to specific domains, such as scientific and industrial fields, is highly significant in promoting their practical applications. This paper systematically investigates domain adaptation of MLLMs via post-training, focusing on data synthesis, training pipeline, and task evaluation. (1) Data Synthesis: Using only open-source models, we develop a generate-then-filter pipeline that curates diverse visual instruction tasks based on domain-specific image-caption pairs. The resulting data surpass the data synthesized by manual rules or strong closed-source models in enhancing domain-specific performance. (2) Training Pipeline: Unlike general MLLMs that typically adopt a two-stage training paradigm, we find that a single-stage approach is more effective for domain adaptation. (3) Task Evaluation: We conduct extensive experiments in high-impact domains such as biomedicine, food, and remote sensing, by post-training a variety of MLLMs and then evaluating MLLM performance on various domain-specific tasks. Finally, we fully open-source our models, code, and data to encourage future research in this area.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "DeepGDel: Deep Learning-based Gene Deletion Prediction Framework for Growth-Coupled Production in Genome-Scale Metabolic Models",
      "titleJa": "Deepgdel: ディープラーニング-based gene deletion prediction framework for growth-coupled production in genome-scale metabolic models",
      "link": "https://arxiv.org/abs/2504.06316",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.06316v4 Announce Type: replace-cross \nAbstract: In genome-scale constraint-based metabolic models, gene deletion strategies are crucial for achieving growth-coupled production, where cell growth and target metabolite production are simultaneously achieved. While computational methods for calculating gene deletions have been widely explored and contribute to developing gene deletion strategy databases, current approaches are limited in leveraging new data-driven paradigms, such as machine learning, for more efficient strain design. Therefore, it is necessary to propose a fundamental framework for this objective. In this study, we first formulate the problem of gene deletion strategy prediction and then propose a framework for predicting gene deletion strategies for growth-coupled production in genome-scale metabolic models. The proposed framework leverages deep learning algorithms to learn and integrate sequential gene and metabolite data representation, enabling the automatic gene deletion strategy prediction. Computational experiment results demonstrate the feasibility of the proposed framework, showing substantial improvements over baseline methods. Specifically, the proposed framework achieves a 14.69%, 22.52%, and 13.03% increase in overall accuracy across three metabolic models of different scales under study, while maintaining balanced precision and recall in predicting gene deletion statuses. The source code and examples for the framework are publicly available at https://github.com/MetNetComp/DeepGDel.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction",
      "titleJa": "Statistical learning for heterogeneous treatment effects: pretraining, prognosis, and prediction",
      "link": "https://arxiv.org/abs/2505.00310",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.00310v2 Announce Type: replace-cross \nAbstract: Robust estimation of heterogeneous treatment effects is a fundamental challenge for optimal decision-making in domains ranging from personalized medicine to educational policy. In recent years, predictive machine learning has emerged as a valuable toolbox for causal estimation, enabling more flexible effect estimation. However, accurately estimating conditional average treatment effects (CATE) remains a major challenge, particularly in the presence of many covariates. In this article, we propose pretraining strategies that leverage a phenomenon in real-world applications: factors that are prognostic of the outcome are frequently also predictive of treatment effect heterogeneity. In medicine, for example, components of the same biological signaling pathways frequently influence both baseline risk and treatment response. Specifically, we demonstrate our approach within the R-learner framework, which estimates the CATE by solving individual prediction problems based on a residualized loss. We use this structure to incorporate side information and develop models that can exploit synergies between risk prediction and causal effect estimation. In settings where these synergies are present, this cross-task learning enables more accurate signal detection, yields lower estimation error, reduced false discovery rates, and higher power for detecting heterogeneity.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks",
      "titleJa": "Code graph モデル (cgm): a graph-integrated 大規模言語モデル for repository-level software engineering tasks",
      "link": "https://arxiv.org/abs/2505.16901",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.16901v3 Announce Type: replace-cross \nAbstract: Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Statistical Inference under Performativity",
      "titleJa": "Statistical 推論 under performativity",
      "link": "https://arxiv.org/abs/2505.18493",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.18493v2 Announce Type: replace-cross \nAbstract: Performativity of predictions refers to the phenomena that prediction-informed decisions may influence the target they aim to predict, which is widely observed in policy-making in social sciences and economics. In this paper, we initiate the study of statistical inference under performativity. Our contribution is two-fold. First, we build a central limit theorem for estimation and inference under performativity, which enables inferential purposes in policy-making such as constructing confidence intervals or testing hypotheses. Second, we further leverage the derived central limit theorem to investigate prediction-powered inference (PPI) under performativity, which is based on a small labeled dataset and a much larger dataset of machine-learning predictions. This enables us to obtain more precise estimation and improved confidence regions for the model parameter (i.e., policy) of interest in performative prediction. We demonstrate the power of our framework by numerical experiments. To the best of our knowledge, this paper is the first one to establish statistical inference under performativity, which brings up new challenges and inference settings that we believe will add significant values to policy-making, statistics, and machine learning.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots",
      "titleJa": "From experts to a generalist: toward general whole-body control for humanoid robots",
      "link": "https://arxiv.org/abs/2506.12779",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12779v2 Announce Type: replace-cross \nAbstract: Achieving general agile whole-body control on humanoid robots remains a major challenge due to diverse motion demands and data conflicts. While existing frameworks excel in training single motion-specific policies, they struggle to generalize across highly varied behaviors due to conflicting control requirements and mismatched data distributions. In this work, we propose BumbleBee (BB), an expert-generalist learning framework that combines motion clustering and sim-to-real adaptation to overcome these challenges. BB first leverages an autoencoder-based clustering method to group behaviorally similar motions using motion features and motion descriptions. Expert policies are then trained within each cluster and refined with real-world data through iterative delta action modeling to bridge the sim-to-real gap. Finally, these experts are distilled into a unified generalist controller that preserves agility and robustness across all motion types. Experiments on two simulations and a real humanoid robot demonstrate that BB achieves state-of-the-art general whole-body control, setting a new benchmark for agile, robust, and generalizable humanoid performance in the real world.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Min-p, Max Exaggeration: A Critical Analysis of Min-p Sampling in Language Models",
      "titleJa": "Min-p, max exaggeration: a critical analysis of min-p sampling in language models",
      "link": "https://arxiv.org/abs/2506.13681",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13681v2 Announce Type: replace-cross \nAbstract: Sampling from language models impacts the quality and diversity of outputs, affecting both research and real-world applications. Recently, Nguyen et al. 2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs\" introduced a new sampler called min-p, claiming it achieves superior quality and diversity over established samplers such as basic, top-k, and top-p sampling. The significance of these claims was underscored by the paper's recognition as the 18th highest-scoring submission to ICLR 2025 and selection for an Oral presentation. This paper conducts a comprehensive re-examination of the evidence supporting min-p and reaches different conclusions from the original paper's four lines of evidence. First, the original paper's human evaluations omitted data, conducted statistical tests incorrectly, and described qualitative feedback inaccurately; our reanalysis demonstrates min-p did not outperform baselines in quality, diversity, or a trade-off between quality and diversity; in response to our findings, the authors of the original paper conducted a new human evaluation using a different implementation, task, and rubric that nevertheless provides further evidence min-p does not improve over baselines. Second, comprehensively sweeping the original paper's NLP benchmarks reveals min-p does not surpass baselines when controlling for the number of hyperparameters. Third, the original paper's LLM-as-a-Judge evaluations lack methodological clarity and appear inconsistently reported. Fourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars) were found to be unsubstantiated, leading to their removal; the revised adoption claim remains misleading. We conclude that evidence presented in the original paper fails to support claims that min-p improves quality, diversity, or a trade-off between quality and diversity.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 65
    },
    {
      "title": "Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints",
      "titleJa": "Advancing stochastic 3-sat solvers by dissipating oversatisfied constraints",
      "link": "https://arxiv.org/abs/2506.15774",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15774v1 Announce Type: new \nAbstract: We introduce and benchmark a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT that drastically outperforms existing solvers in the notoriously difficult realm of critically hard instances. Our construction is based on the crucial observation that well established previous approaches such as WalkSAT are prone to get stuck in local minima that are distinguished from true solutions by a larger number of oversatisfied combinatorial constraints. To address this issue, the proposed algorithm, coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their unfavorable abundance so as to render them critical. We analyze and benchmark our algorithm on a randomly generated sample of hard but satisfiable 3-SAT instances with varying problem sizes up to N=15000. Quite remarkably, we find that DOCSAT outperforms both WalkSAT and other well known algorithms including the complete solver Kissat, even when comparing its ability to solve the hardest quintile of the sample to the average performance of its competitors. The essence of DOCSAT may be seen as a way of harnessing statistical structure beyond the primary cost function of a combinatorial problem to avoid or escape local minima traps in stochastic local search, which opens avenues for generalization to other optimization problems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Graphics4Science: Computer Graphics for Scientific Impacts",
      "titleJa": "Graphics4science: computer graphics for scientific impacts",
      "link": "https://arxiv.org/abs/2506.15786",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15786v1 Announce Type: cross \nAbstract: Computer graphics, often associated with films, games, and visual effects, has long been a powerful tool for addressing scientific challenges--from its origins in 3D visualization for medical imaging to its role in modern computational modeling and simulation. This course explores the deep and evolving relationship between computer graphics and science, highlighting past achievements, ongoing contributions, and open questions that remain. We show how core methods, such as geometric reasoning and physical modeling, provide inductive biases that help address challenges in both fields, especially in data-scarce settings. To that end, we aim to reframe graphics as a modeling language for science by bridging vocabulary gaps between the two communities. Designed for both newcomers and experts, Graphics4Science invites the graphics community to engage with science, tackle high-impact problems where graphics expertise can make a difference, and contribute to the future of scientific discovery. Additional details are available on the course website: https://graphics4science.github.io",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal",
      "titleJa": "Veigar: view-consistent explicit inpainting and geometry alignment for 3d object removal",
      "link": "https://arxiv.org/abs/2506.15821",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15821v1 Announce Type: cross \nAbstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization",
      "titleJa": "Cp$^2$: leveraging geometry for conformal prediction via canonicalization",
      "link": "https://arxiv.org/abs/2506.16189",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16189v1 Announce Type: cross \nAbstract: We study the problem of conformal prediction (CP) under geometric data shifts, where data samples are susceptible to transformations such as rotations or flips. While CP endows prediction models with post-hoc uncertainty quantification and formal coverage guarantees, their practicality breaks under distribution shifts that deteriorate model performance. To address this issue, we propose integrating geometric information--such as geometric pose--into the conformal procedure to reinstate its guarantees and ensure robustness under geometric shifts. In particular, we explore recent advancements on pose canonicalization as a suitable information extractor for this purpose. Evaluating the combined approach across discrete and continuous shifts and against equivariant and augmentation-based baselines, we find that integrating geometric information with CP yields a principled way to address geometric shifts while maintaining broad applicability to black-box predictors.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "SycnMapV2: Robust and Adaptive Unsupervised Segmentation",
      "titleJa": "Sycnmapv2: robust and adaptive unsupervised segmentation",
      "link": "https://arxiv.org/abs/2506.16297",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16297v1 Announce Type: cross \nAbstract: Human vision excels at segmenting visual cues without the need for explicit training, and it remains remarkably robust even as noise severity increases. In contrast, existing AI algorithms struggle to maintain accuracy under similar conditions. Here, we present SyncMapV2, the first to solve unsupervised segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop observed in SOTA methods.This superior performance extends across various types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training, supervision, or loss functions. It is based on a learning paradigm that uses self-organizing dynamical equations combined with concepts from random networks. Moreover,unlike conventional methods that require re-initialization for each new input, SyncMapV2 adapts online, mimicking the continuous adaptability of human vision. Thus, we go beyond the accurate and robust results, and present the first algorithm that can do all the above online, adapting to input rather than re-initializing. In adaptability tests, SyncMapV2 demonstrates near-zero performance degradation, which motivates and fosters a new generation of robust and adaptive intelligence in the near future.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details",
      "titleJa": "Hunyuan3d 2.5: towards high-fidelity 3d assets generation with ultimate details",
      "link": "https://arxiv.org/abs/2506.16504",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16504v1 Announce Type: cross \nAbstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Instituto de Telecomunica\\c{c}\\~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning",
      "titleJa": "Instituto de telecomunica\\c{c}\\~oes at iwslt 2025: aligning small-scale speech and language models for speech-to-text learning",
      "link": "https://arxiv.org/abs/2506.17019",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17019v1 Announce Type: cross \nAbstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "TransDreamerV3: Implanting Transformer In DreamerV3",
      "titleJa": "Transdreamerv3: implanting トランスフォーマー in dreamerv3",
      "link": "https://arxiv.org/abs/2506.17103",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17103v1 Announce Type: cross \nAbstract: This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder. The model is designed to improve memory and decision-making capabilities in complex environments. We conducted experiments on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved performance over DreamerV3, particularly in the Atari-Freeway and Crafter tasks. While issues in the Minecraft task and limited training across all tasks were noted, TransDreamerV3 displays advancement in world model-based reinforcement learning, leveraging transformer architectures.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting",
      "titleJa": "Part$^{2}$gs: part-aware modeling of articulated objects using 3d gaussian splatting",
      "link": "https://arxiv.org/abs/2506.17212",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17212v1 Announce Type: cross \nAbstract: Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable parts.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "MonoSOWA: Scalable monocular 3D Object detector Without human Annotations",
      "titleJa": "Monosowa: scalable monocular 3d object detector without human annotations",
      "link": "https://arxiv.org/abs/2501.09481",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.09481v3 Announce Type: replace-cross \nAbstract: Inferring object 3D position and orientation from a single RGB camera is a foundational task in computer vision with many important applications. Traditionally, 3D object detection methods are trained in a fully-supervised setup, requiring LiDAR and vast amounts of human annotations, which are laborious, costly, and do not scale well with the ever-increasing amounts of data being captured.\n  We present a novel method to train a 3D object detector from a single RGB camera without domain-specific human annotations, making orders of magnitude more data available for training. The method uses newly proposed Local Object Motion Model to disentangle object movement source between subsequent frames, is approximately 700 times faster than previous work and compensates camera focal length differences to aggregate multiple datasets.\n  The method is evaluated on three public datasets, where despite using no human labels, it outperforms prior work by a significant margin. It also shows its versatility as a pre-training tool for fully-supervised training and shows that combining pseudo-labels from multiple datasets can achieve comparable accuracy to using human labels from a single dataset. The source code and model are available at https://github.com/jskvrna/MonoSOWA.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations",
      "titleJa": "Rl2grid: benchmarking 強化学習 in power grid operations",
      "link": "https://arxiv.org/abs/2503.23101",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.23101v2 Announce Type: replace-cross \nAbstract: Reinforcement learning (RL) can provide adaptive and scalable controllers essential for power grid decarbonization. However, RL methods struggle with power grids' complex dynamics, long-horizon goals, and hard physical constraints. For these reasons, we present RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on RTE France's power simulation framework, RL2Grid standardizes tasks, state and action spaces, and reward structures for a systematic evaluation and comparison of RL algorithms. Moreover, we integrate operational heuristics and design safety constraints based on human expertise to ensure alignment with physical requirements. By establishing reference performance metrics for classic RL baselines on RL2Grid's tasks, we highlight the need for novel methods capable of handling real systems and discuss future directions for RL-based grid control.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Essential-Web v1.0: 24T tokens of organized web data",
      "titleJa": "Essential-web v1.0: 24t tokens of organized web data",
      "link": "https://arxiv.org/abs/2506.14111",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14111v2 Announce Type: replace-cross \nAbstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3",
      "titleJa": "Enhancing document-level question answering via multi-hop retrieval-augmented generation with llama 3",
      "link": "https://arxiv.org/abs/2506.16037",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16037v1 Announce Type: cross \nAbstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework tailored for complex question answering tasks, addressing challenges in multi-hop reasoning and contextual understanding across lengthy documents. Built upon LLaMA 3, the framework integrates a dense retrieval module with advanced context fusion and multi-hop reasoning mechanisms, enabling more accurate and coherent response generation. A joint optimization strategy combining retrieval likelihood and generation cross-entropy improves the model's robustness and adaptability. Experimental results show that the proposed system outperforms existing retrieval-augmented and generative baselines, confirming its effectiveness in delivering precise, contextually grounded answers.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments",
      "titleJa": "Prmpt2adpt: prompt-based zero-shot domain adaptation for resource-constrained environments",
      "link": "https://arxiv.org/abs/2506.16994",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16994v1 Announce Type: cross \nAbstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world vision systems, especially in resource-constrained environments like drones, where memory and computation are limited. Existing prompt-driven UDA methods typically rely on large vision-language models and require full access to source-domain data during adaptation, limiting their applicability. In this work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain adaptation framework built around a teacher-student paradigm guided by prompt-based feature alignment. At the core of our method is a distilled and fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A small set of low-level source features is aligned to the target domain semantics-specified only through a natural language prompt-via Prompt-driven Instance Normalization (PIN). These semantically steered features are used to briefly fine-tune the detection head of the teacher model. The adapted teacher then generates high-quality pseudo-labels, which guide the on-the-fly adaptation of a compact student model. Experiments on the MDS-A dataset demonstrate that Prmpt2Adpt achieves competitive detection performance compared to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x faster inference speed using few source images-making it a practical and scalable solution for real-time adaptation in low-resource domains.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization",
      "titleJa": "Dreamcube: 3d panorama generation via multi-plane synchronization",
      "link": "https://arxiv.org/abs/2506.17206",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17206v1 Announce Type: cross \nAbstract: 3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "EF21 with Bells & Whistles: Six Algorithmic Extensions of Modern Error Feedback",
      "titleJa": "Ef21 with bells & whistles: six algorithmic extensions of modern error feedback",
      "link": "https://arxiv.org/abs/2110.03294",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2110.03294v2 Announce Type: replace \nAbstract: First proposed by Seide (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is $O(1/T^{2/3})$, the rate of gradient descent in the same regime is $O(1/T)$). Recently, Richt\\'arik et al. (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum, and bidirectional compression. To the best of our knowledge, several of these techniques have not been previously analyzed in combination with EF, and in cases where prior analysis exists -- such as for bidirectional compression -- our theoretical convergence guarantees significantly improve upon existing results.",
      "summary": "arXiv:2110.",
      "summaryJa": "Arxiv:2110.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis",
      "titleJa": "A statistical evaluation of indoor lorawan environment-aware propagation for 6g: mlr, anova, and residual distribution analysis",
      "link": "https://arxiv.org/abs/2504.16688",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.16688v3 Announce Type: replace-cross \nAbstract: Modeling path loss in indoor LoRaWAN technology deployments is inherently challenging due to structural obstructions, occupant density and activities, and fluctuating environmental conditions. This study proposes a two-stage approach to capture and analyze these complexities using an extensive dataset of 1,328,334 field measurements collected over six months in a single-floor office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First, we implement a multiple linear regression model that includes traditional propagation metrics (distance, structural walls) and an extension with proposed environmental variables (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure). Using analysis of variance, we demonstrate that adding these environmental factors can reduce unexplained variance by 42.32 percent. Secondly, we examine residual distributions by fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy, Student's t, and Gaussian Mixture Models (GMMs) with 2 to 5 components. Our results show that a four-component Gaussian Mixture Model captures the residual heterogeneity of indoor signal propagation most accurately, significantly outperforming single-distribution approaches. Given the push toward ultra-reliable, context-aware communications in 6G networks, our analysis shows that environment-aware modeling can substantially improve LoRaWAN network design in dynamic indoor IoT deployments.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 60
    },
    {
      "title": "OAgents: An Empirical Study of Building Effective Agents",
      "titleJa": "Oagents: an empirical study of building effective agents",
      "link": "https://arxiv.org/abs/2506.15741",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15741v1 Announce Type: new \nAbstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Linear-Time Primitives for Algorithm Development in Graphical Causal Inference",
      "titleJa": "Linear-time primitives for アルゴリズム development in graphical causal 推論",
      "link": "https://arxiv.org/abs/2506.15758",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15758v1 Announce Type: new \nAbstract: We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents",
      "titleJa": "Osworld-human: benchmarking the efficiency of computer-use agents",
      "link": "https://arxiv.org/abs/2506.16042",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16042v1 Announce Type: new \nAbstract: Generative AI is being leveraged to solve a variety of computer-use tasks involving desktop applications. State-of-the-art systems have focused solely on improving accuracy on leading benchmarks. However, these systems are practically unusable due to extremely high end-to-end latency (e.g., tens of minutes) for tasks that typically take humans just a few minutes to complete. To understand the cause behind this and to guide future developments of computer agents, we conduct the first study on the temporal performance of computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We find that large model calls for planning and reflection account for the majority of the overall latency, and as an agent uses more steps to complete a task, each successive step can take 3x longer than steps at the beginning of a task. We then construct OSWorld-Human, a manually annotated version of the original OSWorld dataset that contains a human-determined trajectory for each task. We evaluate 16 agents on their efficiency using OSWorld-Human and found that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than necessary.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies",
      "titleJa": "Consistency verification in ontology-based process models with parameter interdependencies",
      "link": "https://arxiv.org/abs/2506.16087",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16087v1 Announce Type: new \nAbstract: The formalization of process knowledge using ontologies enables consistent modeling of parameter interdependencies in manufacturing. These interdependencies are typically represented as mathematical expressions that define relations between process parameters, supporting tasks such as calculation, validation, and simulation. To support cross-context application and knowledge reuse, such expressions are often defined in a generic form and applied across multiple process contexts. This highlights the necessity of a consistent and semantically coherent model to ensure the correctness of data retrieval and interpretation. Consequently, dedicated mechanisms are required to address key challenges such as selecting context-relevant data, ensuring unit compatibility between variables and data elements, and verifying the completeness of input data required for evaluating mathematical expressions. This paper presents a set of verification mechanisms for a previously developed ontology-based process model that integrates standardized process semantics, data element definitions, and formal mathematical constructs. The approach includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a unit consistency check based on expected-unit annotations and semantic classification, and (iii) a data completeness check to validate the evaluability of interdependencies. The applicability of the approach is demonstrated with a use case from Resin Transfer Molding (RTM), supporting the development of machine-interpretable and verifiable engineering models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction",
      "titleJa": "Geometric learning in black-box optimization: a gnn framework for アルゴリズム performance prediction",
      "link": "https://arxiv.org/abs/2506.16144",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16144v1 Announce Type: new \nAbstract: Automated algorithm performance prediction in numerical blackbox optimization often relies on problem characterizations, such as exploratory landscape analysis features. These features are typically used as inputs to machine learning models and are represented in a tabular format. However, such approaches often overlook algorithm configurations, a key factor influencing performance. The relationships between algorithm operators, parameters, problem characteristics, and performance outcomes form a complex structure best represented as a graph. This work explores the use of heterogeneous graph data structures and graph neural networks to predict the performance of optimization algorithms by capturing the complex dependencies between problems, algorithm configurations, and performance outcomes. We focus on two modular frameworks, modCMA-ES and modDE, which decompose two widely used derivative-free optimization algorithms: the covariance matrix adaptation evolution strategy (CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576 modDE variants on 24 BBOB problems across six runtime budgets and two problem dimensions. Achieving up to 36.6% improvement in MSE over traditional tabular-based methods, this work highlights the potential of geometric learning in black-box optimization.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Approximation Fixpoint Theory with Refined Approximation Spaces",
      "titleJa": "Approximation fixpoint theory with refined approximation spaces",
      "link": "https://arxiv.org/abs/2506.16294",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16294v1 Announce Type: new \nAbstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various semantics of non-monotonic reasoning formalisms in knowledge representation such as Logic Programming and Answer Set Programming. Many semantics of such non-monotonic formalisms can be characterized as suitable fixpoints of a non-monotonic operator on a suitable lattice. Instead of working on the original lattice, AFT operates on intervals in such lattice to approximate or construct the fixpoints of interest. While AFT has been applied successfully across a broad range of non-monotonic reasoning formalisms, it is confronted by its limitations in other, relatively simple, examples. In this paper, we overcome those limitations by extending consistent AFT to deal with approximations that are more refined than intervals. Therefore, we introduce a more general notion of approximation spaces, showcase the improved expressiveness and investigate relations between different approximation spaces.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring",
      "titleJa": "The role of explanation styles and perceived accuracy on decision making in predictive process monitoring",
      "link": "https://arxiv.org/abs/2506.16617",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16617v1 Announce Type: new \nAbstract: Predictive Process Monitoring (PPM) often uses deep learning models to predict the future behavior of ongoing processes, such as predicting process outcomes. While these models achieve high accuracy, their lack of interpretability undermines user trust and adoption. Explainable AI (XAI) aims to address this challenge by providing the reasoning behind the predictions. However, current evaluations of XAI in PPM focus primarily on functional metrics (such as fidelity), overlooking user-centered aspects such as their effect on task performance and decision-making. This study investigates the effects of explanation styles (feature importance, rule-based, and counterfactual) and perceived AI accuracy (low or high) on decision-making in PPM. We conducted a decision-making experiment, where users were presented with the AI predictions, perceived accuracy levels, and explanations of different styles. Users' decisions were measured both before and after receiving explanations, allowing the assessment of objective metrics (Task Performance and Agreement) and subjective metrics (Decision Confidence). Our findings show that perceived accuracy and explanation style have a significant effect.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics",
      "titleJa": "Interpretable low-dimensional modeling of spatiotemporal agent states for decision making in football tactics",
      "link": "https://arxiv.org/abs/2506.16696",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16696v1 Announce Type: new \nAbstract: Understanding football tactics is crucial for managers and analysts. Previous research has proposed models based on spatial and kinematic equations, but these are computationally expensive. Also, Reinforcement learning approaches use player positions and velocities but lack interpretability and require large datasets. Rule-based models align with expert knowledge but have not fully considered all players' states. This study explores whether low-dimensional, rule-based models using spatiotemporal data can effectively capture football tactics. Our approach defines interpretable state variables for both the ball-holder and potential pass receivers, based on criteria that explore options like passing. Through discussions with a manager, we identified key variables representing the game state. We then used StatsBomb event data and SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost model to predict pass success. The analysis revealed that the distance between the player and the ball, as well as the player's space score, were key factors in determining successful passes. Our interpretable low-dimensional modeling facilitates tactical analysis through the use of intuitive variables and provides practical value as a tool to support decision-making in football.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Incentivizing High-quality Participation From Federated Learning Agents",
      "titleJa": "Incentivizing high-quality participation from federated learning agents",
      "link": "https://arxiv.org/abs/2506.16731",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16731v1 Announce Type: new \nAbstract: Federated learning (FL) provides a promising paradigm for facilitating collaboration between multiple clients that jointly learn a global model without directly sharing their local data. However, existing research suffers from two caveats: 1) From the perspective of agents, voluntary and unselfish participation is often assumed. But self-interested agents may opt out of the system or provide low-quality contributions without proper incentives; 2) From the mechanism designer's perspective, the aggregated models can be unsatisfactory as the existing game-theoretical federated learning approach for data collection ignores the potential heterogeneous effort caused by contributed data. To alleviate above challenges, we propose an incentive-aware framework for agent participation that considers data heterogeneity to accelerate the convergence process. Specifically, we first introduce the notion of Wasserstein distance to explicitly illustrate the heterogeneous effort and reformulate the existing upper bound of convergence. To induce truthful reporting from agents, we analyze and measure the generalization error gap of any two agents by leveraging the peer prediction mechanism to develop score functions. We further present a two-stage Stackelberg game model that formalizes the process and examines the existence of equilibrium. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed mechanism.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers",
      "titleJa": "強化学習 for hybrid charging stations planning and operation considering fixed and mobile chargers",
      "link": "https://arxiv.org/abs/2506.16764",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16764v1 Announce Type: new \nAbstract: The success of vehicle electrification, which brings significant societal and environmental benefits, is contingent upon the availability of efficient and adaptable charging infrastructure. Traditional fixed-location charging stations often face issues like underutilization or congestion due to the dynamic nature of charging demand. Mobile chargers have emerged as a flexible solution, capable of relocating to align with these demand fluctuations. This paper addresses the optimal planning and operation of hybrid charging infrastructures, integrating both fixed and mobile chargers within urban road networks. We introduce the Hybrid Charging Station Planning and Operation (HCSPO) problem, which simultaneously optimizes the location and configuration of fixed charging stations and schedules mobile chargers for dynamic operations. Our approach incorporates a charging demand prediction model grounded in Model Predictive Control (MPC) to enhance decision-making. To solve the HCSPO problem, we propose a deep reinforcement learning method, augmented with heuristic scheduling techniques, to effectively bridge the planning of fixed chargers with the real-time operation of mobile chargers. Extensive case studies using real-world urban scenarios demonstrate that our method significantly improves the availability of charging infrastructure and reduces user inconvenience compared to existing solutions and baselines.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario",
      "titleJa": "Ai's blind spots: geographic knowledge and diversity deficit in generated urban scenario",
      "link": "https://arxiv.org/abs/2506.16898",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16898v1 Announce Type: new \nAbstract: Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fr\\'echet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for \"United States\" instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {\\color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines",
      "titleJa": "Real-time black-box optimization for dynamic discrete environments using embedded ising machines",
      "link": "https://arxiv.org/abs/2506.16924",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16924v1 Announce Type: new \nAbstract: Many real-time systems require the optimization of discrete variables. Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms perform optimization by repeatedly taking actions and observing the corresponding instant rewards without any prior knowledge. Recently, a BBO method using an Ising machine has been proposed to find the best action that is represented by a combination of discrete values and maximizes the instant reward in static environments. In contrast, dynamic environments, where real-time systems operate, necessitate MAB algorithms that maximize the average reward over multiple trials. However, due to the enormous number of actions resulting from the combinatorial nature of discrete optimization, conventional MAB algorithms cannot effectively optimize dynamic, discrete environments. Here, we show a heuristic MAB method for dynamic, discrete environments by extending the BBO method, in which an Ising machine effectively explores the actions while considering interactions between variables and changes in dynamic environments. We demonstrate the dynamic adaptability of the proposed method in a wireless communication system with moving users.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models",
      "titleJa": "A quantile regression approach for remaining useful life estimation with state space models",
      "link": "https://arxiv.org/abs/2506.17018",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17018v1 Announce Type: new \nAbstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively enhancing efficiency through accurate equipment Remaining Useful Life (RUL) prediction, thus optimizing maintenance scheduling and reducing unexpected failures and premature interventions. This paper introduces a novel RUL estimation approach leveraging State Space Models (SSM) for efficient long-term sequence modeling. To handle model uncertainty, Simoultaneous Quantile Regression (SQR) is integrated into the SSM, enabling multiple quantile estimations. The proposed method is benchmarked against traditional sequence modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset. Results demonstrate superior accuracy and computational efficiency of SSM models, underscoring their potential for high-stakes industrial applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Dispositions and Roles of Generically Dependent Entities",
      "titleJa": "Dispositions and roles of generically dependent entities",
      "link": "https://arxiv.org/abs/2506.17085",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17085v1 Announce Type: new \nAbstract: BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants (like software or datasets). In this paper, we argue that this is a severe limitation, which prevents, for example, the adequate representation of the functions of computer models or the various roles of datasets during the execution of these models. We discuss the aspects of BFO 2020 that prevent the representation of realizable entities of generically dependent continuants. Two approaches to address the issue are presented: (a) the use of defined classes and (b) a proposal of changes that allow BFO to support functions, dispositions, and roles of generically dependent continuants.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Are Bias Evaluation Methods Biased ?",
      "titleJa": "Are バイアス evaluation methods biased ?",
      "link": "https://arxiv.org/abs/2506.17111",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17111v1 Announce Type: new \nAbstract: The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI",
      "titleJa": "Chain-of-trust: a progressive trust evaluation framework enabled by 生成AI",
      "link": "https://arxiv.org/abs/2506.17130",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17130v1 Announce Type: new \nAbstract: In collaborative systems with complex tasks relying on distributed resources, trust evaluation of potential collaborators has emerged as an effective mechanism for task completion. However, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment. In this paper, a novel progressive trust evaluation framework, namely chain-of-trust, is proposed to make better use of misaligned device attribute data. This framework, designed for effective task completion, divides the trust evaluation process into multiple chained stages based on task decomposition. At each stage, based on the task completion process, the framework only gathers the latest device attribute data relevant to that stage, leading to reduced trust evaluation complexity and overhead. By leveraging advanced in-context learning, few-shot learning, and reasoning capabilities, generative AI is then employed to analyze and interpret the collected data to produce correct evaluation results quickly. Only devices deemed trustworthy at this stage proceed to the next round of trust evaluation. The framework ultimately determines devices that remain trustworthy across all stages. Experimental results demonstrate that the proposed framework achieves high accuracy in trust evaluation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree",
      "titleJa": "Cast: enhancing code retrieval-augmented generation with structural chunking via abstract syntax tree",
      "link": "https://arxiv.org/abs/2506.15655",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15655v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning from M-Tuple Dominant Positive and Unlabeled Data",
      "titleJa": "Learning from m-tuple dominant positive and unlabeled data",
      "link": "https://arxiv.org/abs/2506.15686",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15686v1 Announce Type: cross \nAbstract: Label Proportion Learning (LLP) addresses the classification problem where multiple instances are grouped into bags and each bag contains information about the proportion of each class. However, in practical applications, obtaining precise supervisory information regarding the proportion of instances in a specific class is challenging. To better align with real-world application scenarios and effectively leverage the proportional constraints of instances within tuples, this paper proposes a generalized learning framework \\emph{MDPU}. Specifically, we first mathematically model the distribution of instances within tuples of arbitrary size, under the constraint that the number of positive instances is no less than that of negative instances. Then we derive an unbiased risk estimator that satisfies risk consistency based on the empirical risk minimization (ERM) method. To mitigate the inevitable overfitting issue during training, a risk correction method is introduced, leading to the development of a corrected risk estimator. The generalization error bounds of the unbiased risk estimator theoretically demonstrate the consistency of the proposed method. Extensive experiments on multiple datasets and comparisons with other relevant baseline methods comprehensively validate the effectiveness of the proposed learning framework.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism",
      "titleJa": "Cellular traffic prediction via deep state space models with attention mechanism",
      "link": "https://arxiv.org/abs/2506.15688",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15688v1 Announce Type: cross \nAbstract: Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors, which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modelling. Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation",
      "titleJa": "Minifinetuning: low-data generation domain adaptation through corrective self-distillation",
      "link": "https://arxiv.org/abs/2506.15702",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15702v1 Announce Type: cross \nAbstract: Finetuning language models for a new domain inevitably leads to the deterioration of their general performance. This becomes more pronounced the more limited the finetuning data resource.\n  We introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay. MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples.\n  Employing corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like degeneralization mitigation properties, and is composable with either for a combined effect.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance",
      "titleJa": "Federated incomplete multi-view clustering with globally fused graph guidance",
      "link": "https://arxiv.org/abs/2506.15703",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15703v1 Announce Type: cross \nAbstract: Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress, most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG. Our code is publicly available at https://github.com/PaddiHunter/FIMCFG.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification",
      "titleJa": "Refined causal graph structure learning via curvature for brain disease classification",
      "link": "https://arxiv.org/abs/2506.15708",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15708v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called CGB (Causal Graphs for Brains) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Studying and Improving Graph Neural Network-based Motif Estimation",
      "titleJa": "Studying and improving graph ニューラルネットワーク-based motif estimation",
      "link": "https://arxiv.org/abs/2506.15709",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15709v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Shadow defense against gradient inversion attack in federated learning",
      "titleJa": "Shadow defense against gradient inversion attack in federated learning",
      "link": "https://arxiv.org/abs/2506.15711",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15711v1 Announce Type: cross \nAbstract: Federated learning (FL) has emerged as a transformative framework for privacy-preserving distributed training, allowing clients to collaboratively train a global model without sharing their local data. This is especially crucial in sensitive fields like healthcare, where protecting patient data is paramount. However, privacy leakage remains a critical challenge, as the communication of model updates can be exploited by potential adversaries. Gradient inversion attacks (GIAs), for instance, allow adversaries to approximate the gradients used for training and reconstruct training images, thus stealing patient privacy. Existing defense mechanisms obscure gradients, yet lack a nuanced understanding of which gradients or types of image information are most vulnerable to such attacks. These indiscriminate calibrated perturbations result in either excessive privacy protection degrading model accuracy, or insufficient one failing to safeguard sensitive information. Therefore, we introduce a framework that addresses these challenges by leveraging a shadow model with interpretability for identifying sensitive areas. This enables a more targeted and sample-specific noise injection. Specially, our defensive strategy achieves discrepancies of 3.73 in PSNR and 0.2 in SSIM compared to the circumstance without defense on the ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover, it minimizes adverse effects on model performance, with less than 1\\% F1 reduction compared to SOTA methods. Our extensive experiments, conducted across diverse types of medical images, validate the generalization of the proposed framework. The stable defense improvements for FedAvg are consistently over 1.5\\% times in LPIPS and SSIM. It also offers a universal defense against various GIA types, especially for these sensitive areas in images.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "NeuronSeek: On Stability and Expressivity of Task-driven Neurons",
      "titleJa": "Neuronseek: on stability and expressivity of task-driven neurons",
      "link": "https://arxiv.org/abs/2506.15715",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15715v1 Announce Type: cross \nAbstract: Drawing inspiration from our human brain that designs different neurons for different tasks, recent advances in deep learning have explored modifying a network's neurons to develop so-called task-driven neurons. Prototyping task-driven neurons (referred to as NeuronSeek) employs symbolic regression (SR) to discover the optimal neuron formulation and construct a network from these optimized neurons. Along this direction, this work replaces symbolic regression with tensor decomposition (TD) to discover optimal neuronal formulations, offering enhanced stability and faster convergence. Furthermore, we establish theoretical guarantees that modifying the aggregation functions with common activation functions can empower a network with a fixed number of parameters to approximate any continuous function with an arbitrarily small error, providing a rigorous mathematical foundation for the NeuronSeek framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD framework not only achieves superior stability, but also is competitive relative to the state-of-the-art models across diverse benchmarks. The code is available at https://github.com/HanyuPei22/NeuronSeek.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Graph Diffusion that can Insert and Delete",
      "titleJa": "Graph diffusion that can insert and delete",
      "link": "https://arxiv.org/abs/2506.15725",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15725v1 Announce Type: cross \nAbstract: Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains",
      "titleJa": "Recbayes: recurrent bayesian ad hoc teamwork in large partially observable domains",
      "link": "https://arxiv.org/abs/2506.15756",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15756v1 Announce Type: cross \nAbstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under partial observability, a setting where agents are deployed on-the-fly to environments where pre-existing teams operate, that never requires, at any stage, access to the states of the environment or the actions of its teammates. We show that by relying on a recurrent Bayesian classifier trained using past experiences, an ad hoc agent is effectively able to identify known teams and tasks being performed from observations alone. Unlike recent approaches such as PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some stage fully observable states of the environment, actions of teammates, or both, or approaches such as ATPO (Ribeiro et al., 2023) that require the environments to be small enough to be tabularly modelled (Ribeiro et al., 2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes is both able to handle arbitrarily large spaces while never relying on either states and teammates' actions. Our results in benchmark domains from the multi-agent systems literature, adapted for partial observability and scaled up to 1M states and 2^125 observations, show that RecBayes is effective at identifying known teams and tasks being performed from partial observations alone, and as a result, is able to assist the teams in solving the tasks effectively.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "TRUST: Transparent, Robust and Ultra-Sparse Trees",
      "titleJa": "Trust: transparent, robust and ultra-sparse trees",
      "link": "https://arxiv.org/abs/2506.15791",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15791v1 Announce Type: cross \nAbstract: Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products",
      "titleJa": "Linearithmic clean-up for vector-symbolic key-value memory with kroneker rotation products",
      "link": "https://arxiv.org/abs/2506.15793",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15793v1 Announce Type: cross \nAbstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is the ``clean-up'' step, which decodes the noisy vectors retrieved from the architecture. Clean-up typically compares noisy vectors against a ``codebook'' of prototype vectors, incurring computational complexity that is quadratic or similar. We present a new codebook representation that supports efficient clean-up, based on Kroneker products of rotation-like matrices. The resulting clean-up time complexity is linearithmic, i.e. $\\mathcal{O}(N\\,\\text{log}\\,N)$, where $N$ is the vector dimension and also the number of vectors in the codebook. Clean-up space complexity is $\\mathcal{O}(N)$. Furthermore, the codebook is not stored explicitly in computer memory: It can be represented in $\\mathcal{O}(\\text{log}\\,N)$ space, and individual vectors in the codebook can be materialized in $\\mathcal{O}(N)$ time and space. At the same time, asymptotic memory capacity remains comparable to standard approaches. Computer experiments confirm these results, demonstrating several orders of magnitude more scalability than baseline VSA techniques.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma",
      "titleJa": "Unsupervised ディープラーニング モデル for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma",
      "link": "https://arxiv.org/abs/2506.15803",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15803v1 Announce Type: cross \nAbstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in radiotherapy, offering several advantages over conventional intensitymodulated proton therapy (IMPT). However, identifying the optimal energy layer (EL) sequence remains computationally intensive due to the large number of possible energy layer transitions. This study proposes an unsupervised deep learning framework for fast and effective EL pre-selection, aiming to minimize energy layer switch time while preserving high plan quality. Approach. We introduce a novel data representation method, spot-count representation, which encodes the number of proton spots intersecting the target and organs at risk (OARs) in a matrix structured by sorted gantry angles and energy layers. This representation is the input of a UNet-based architecture, SPArcdl, which is trained to optimize a tri-objective function: maximizing target coverage, minimizing OAR exposure, and reducing energy switching time. The model is evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked against plans generated by SPArcparticle swarm. Main results. SPArcdl produces EL pre-selection that significantly improves both plan quality and delivery efficiency. Compared to SPArc particle swarm, it enhances the conformity index by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's inference time is within 1 second. Significance. SPArcdl is a fast and effective tool for generating high-quality PAT plans by strategically pre-selecting energy layers to reduce delivery time while maintaining excellent dosimetric performance.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Finance Language Model Evaluation (FLaME)",
      "titleJa": "Finance language モデル evaluation (flame)",
      "link": "https://arxiv.org/abs/2506.15846",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15846v1 Announce Type: cross \nAbstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Uncertainty Estimation by Human Perception versus Neural Models",
      "titleJa": "Uncertainty estimation by human perception versus neural models",
      "link": "https://arxiv.org/abs/2506.15850",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15850v1 Announce Type: cross \nAbstract: Modern neural networks (NNs) often achieve high predictive accuracy but remain poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers",
      "titleJa": "Mor: better handling diverse queries with a mixture of sparse, dense, and human retrievers",
      "link": "https://arxiv.org/abs/2506.15862",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15862v1 Announce Type: cross \nAbstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction",
      "titleJa": "Kg-fgnn: knowledge-guided gnn foundation モデル for fertilisation-oriented soil ghg flux prediction",
      "link": "https://arxiv.org/abs/2506.15896",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15896v1 Announce Type: cross \nAbstract: Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning",
      "titleJa": "Pncs:power-norm cosine similarity for diverse client selection in federated learning",
      "link": "https://arxiv.org/abs/2506.15923",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15923v1 Announce Type: cross \nAbstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging diverse datasets from multiple sources while preserving data privacy by avoiding centralized storage. However, many existing approaches fail to account for the intricate gradient correlations between remote clients, a limitation that becomes especially problematic in data heterogeneity scenarios. In this work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity (PNCS) to improve client selection for model aggregation. By capturing higher-order gradient moments, PNCS addresses non-IID data challenges, enhancing convergence speed and accuracy. Additionally, we introduce a simple algorithm ensuring diverse client selection through a selection history queue. Experiments with a VGG16 model across varied data partitions demonstrate consistent improvements over state-of-the-art methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Moir\\'eXNet: Adaptive Multi-Scale Demoir\\'eing with Linear Attention Test-Time Training and Truncated Flow Matching Prior",
      "titleJa": "Moir\\'exnet: adaptive multi-scale demoir\\'eing with linear attention test-time 学習 and truncated flow matching prior",
      "link": "https://arxiv.org/abs/2506.15929",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15929v1 Announce Type: cross \nAbstract: This paper introduces a novel framework for image and video demoir\\'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir\\'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods.\n  Traditional supervised learning approaches either fail to remove moir\\'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir\\'eing and often introduce artifacts.\n  To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\\'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging",
      "titleJa": "Heterogeneous-modal unsupervised domain adaptation via latent space bridging",
      "link": "https://arxiv.org/abs/2506.15971",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15971v1 Announce Type: cross \nAbstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps but become struggled when the source and target domains belong to entirely distinct modalities. To address this limitation, we propose a novel setting called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which enables knowledge transfer between completely different modalities by leveraging a bridge domain containing unlabeled samples from both modalities. To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a specialized framework designed for the semantic segmentation task. Specifically, LSB utilizes a dual-branch architecture, incorporating a feature consistency loss to align representations across modalities and a domain alignment loss to reduce discrepancies between class centroids across domains. Extensive experiments conducted on six benchmark datasets demonstrate that LSB achieves state-of-the-art performance.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension",
      "titleJa": "A vietnamese データセット for text segmentation and multiple choices reading comprehension",
      "link": "https://arxiv.org/abs/2506.15978",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15978v1 Announce Type: cross \nAbstract: Vietnamese, the 20th most spoken language with over 102 million native speakers, lacks robust resources for key natural language processing tasks such as text segmentation and machine reading comprehension (MRC). To address this gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset includes 15,942 documents for text segmentation and 16,347 synthetic multiple-choice question-answer pairs generated with human quality assurance, ensuring a reliable and diverse resource. Experiments show that mBERT consistently outperforms monolingual models on both tasks, achieving an accuracy of 88.01% on MRC test set and an F1 score of 63.15\\% on text segmentation test set. Our analysis reveals that multilingual models excel in NLP tasks for Vietnamese, suggesting potential applications to other under-resourced languages. VSMRC is available at HuggingFace",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization",
      "titleJa": "Advanced sign language video generation with compressed and quantized multi-condition tokenization",
      "link": "https://arxiv.org/abs/2506.15980",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15980v1 Announce Type: cross \nAbstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion",
      "titleJa": "Double entendre: robust audio-based ai-generated lyrics detection via multi-view fusion",
      "link": "https://arxiv.org/abs/2506.15981",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15981v1 Announce Type: cross \nAbstract: The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "DIGMAPPER: A Modular System for Automated Geologic Map Digitization",
      "titleJa": "Digmapper: a modular system for automated geologic map digitization",
      "link": "https://arxiv.org/abs/2506.16006",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16006v1 Announce Type: cross \nAbstract: Historical geologic maps contain rich geospatial information, such as rock units, faults, folds, and bedding planes, that is critical for assessing mineral resources essential to renewable energy, electric vehicles, and national security. However, digitizing maps remains a labor-intensive and time-consuming task. We present DIGMAPPER, a modular, scalable system developed in collaboration with the United States Geological Survey (USGS) to automate the digitization of geologic maps. DIGMAPPER features a fully dockerized, workflow-orchestrated architecture that integrates state-of-the-art deep learning models for map layout analysis, feature extraction, and georeferencing. To overcome challenges such as limited training data and complex visual content, our system employs innovative techniques, including in-context learning with large language models, synthetic data generation, and transformer-based models. Evaluations on over 100 annotated maps from the DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point feature extraction, and reliable georeferencing performance. Deployed at USGS, DIGMAPPER significantly accelerates the creation of analysis-ready geospatial datasets, supporting national-scale critical mineral assessments and broader geoscientific applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "VRAIL: Vectorized Reward-based Attribution for Interpretable Learning",
      "titleJa": "Vrail: vectorized reward-based attribution for interpretable learning",
      "link": "https://arxiv.org/abs/2506.16014",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16014v1 Announce Type: cross \nAbstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable Learning), a bi-level framework for value-based reinforcement learning (RL) that learns interpretable weight representations from state features. VRAIL consists of two stages: a deep learning (DL) stage that fits an estimated value function using state features, and an RL stage that uses this to shape learning via potential-based reward transformations. The estimator is modeled in either linear or quadratic form, allowing attribution of importance to individual features and their interactions. Empirical results on the Taxi-v3 environment demonstrate that VRAIL improves training stability and convergence compared to standard DQN, without requiring environment modifications. Further analysis shows that VRAIL uncovers semantically meaningful subgoals, such as passenger possession, highlighting its ability to produce human-interpretable behavior. Our findings suggest that VRAIL serves as a general, model-agnostic framework for reward shaping that enhances both learning and interpretability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding",
      "titleJa": "Vision-guided chunking is all you need: enhancing rag with multimodal document understanding",
      "link": "https://arxiv.org/abs/2506.16035",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16035v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text",
      "titleJa": "A hybrid deberta and gated broad learning system for cyberbullying detection in english text",
      "link": "https://arxiv.org/abs/2506.16052",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16052v1 Announce Type: cross \nAbstract: The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\\% accuracy on HateXplain, 95.41\\% accuracy on SOSNet, 91.37\\% accuracy on Mendeley-I, and 94.67\\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations",
      "titleJa": "Cria: a cross-view interaction and instance-adapted pre-学習 framework for generalizable eeg representations",
      "link": "https://arxiv.org/abs/2506.16056",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16056v1 Announce Type: cross \nAbstract: The difficulty of extracting deep features from EEG data and effectively integrating information from multiple views presents significant challenges for developing a generalizable pretraining framework for EEG representation learning. However, most existing pre-training methods rely solely on the contextual semantics of a single view, failing to capture the complex and synergistic interactions among different perspectives, limiting the expressiveness and generalization of learned representations. To address these issues, this paper proposes CRIA, an adaptive framework that utilizes variable-length and variable-channel coding to achieve a unified representation of EEG data across different datasets. In this work, we define cross-view information as the integrated representation that emerges from the interaction among temporal, spectral, and spatial views of EEG signals. The model employs a cross-attention mechanism to fuse temporal, spectral, and spatial features effectively, and combines an attention matrix masking strategy based on the information bottleneck principle with a novel viewpoint masking pre-training scheme. Experimental results on the Temple University EEG corpus and the CHB-MIT dataset show that CRIA outperforms existing methods with the same pre-training conditions, achieving a balanced accuracy of 57.02% for multi-class event classification and 80.03% for anomaly detection, highlighting its strong generalization ability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching",
      "titleJa": "Improved intelligibility of dysarthric speech using conditional flow matching",
      "link": "https://arxiv.org/abs/2506.16127",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16127v1 Announce Type: cross \nAbstract: Dysarthria is a neurological disorder that significantly impairs speech intelligibility, often rendering affected individuals unable to communicate effectively. This necessitates the development of robust dysarthric-to-regular speech conversion techniques. In this work, we investigate the utility and limitations of self-supervised learning (SSL) features and their quantized representations as an alternative to mel-spectrograms for speech generation. Additionally, we explore methods to mitigate speaker variability by generating clean speech in a single-speaker voice using features extracted from WavLM. To this end, we propose a fully non-autoregressive approach that leverages Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct mapping from dysarthric to clean speech. Our findings highlight the effectiveness of discrete acoustic units in improving intelligibility while achieving faster convergence compared to traditional mel-spectrogram-based approaches.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Category-based Galaxy Image Generation via Diffusion Models",
      "titleJa": "Category-based galaxy image generation via diffusion models",
      "link": "https://arxiv.org/abs/2506.16255",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16255v1 Announce Type: cross \nAbstract: Conventional galaxy generation methods rely on semi-analytical models and hydrodynamic simulations, which are highly dependent on physical assumptions and parameter tuning. In contrast, data-driven generative models do not have explicit physical parameters pre-determined, and instead learn them efficiently from observational data, making them alternative solutions to galaxy generation. Among these, diffusion models outperform Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in quality and diversity. Leveraging physical prior knowledge to these models can further enhance their capabilities. In this work, we present GalCatDiff, the first framework in astronomy to leverage both galaxy image features and astrophysical properties in the network design of diffusion models. GalCatDiff incorporates an enhanced U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which dynamically combines attention mechanisms with convolution operations to ensure global consistency and local feature fidelity. Moreover, GalCatDiff uses category embeddings for class-specific galaxy generation, avoiding the high computational costs of training separate models for each category. Our experimental results demonstrate that GalCatDiff significantly outperforms existing methods in terms of the consistency of sample color and size distributions, and the generated galaxies are both visually realistic and physically consistent. This framework will enhance the reliability of galaxy simulations and can potentially serve as a data augmentor to support future galaxy classification algorithm development.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning Multi-scale Spatial-frequency Features for Image Denoising",
      "titleJa": "Learning multi-scale spatial-frequency features for image denoising",
      "link": "https://arxiv.org/abs/2506.16307",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16307v1 Announce Type: cross \nAbstract: Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks",
      "titleJa": "Improved exploration in gflownets via enhanced epistemic neural networks",
      "link": "https://arxiv.org/abs/2506.16313",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16313v1 Announce Type: cross \nAbstract: Efficiently identifying the right trajectories for training remains an open problem in GFlowNets. To address this, it is essential to prioritize exploration in regions of the state space where the reward distribution has not been sufficiently learned. This calls for uncertainty-driven exploration, in other words, the agent should be aware of what it does not know. This attribute can be measured by joint predictions, which are particularly important for combinatorial and sequential decision problems. In this research, we integrate epistemic neural networks (ENN) with the conventional architecture of GFlowNets to enable more efficient joint predictions and better uncertainty quantification, thereby improving exploration and the identification of optimal trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the baseline method in GFlownets and evaluated in grid environments and structured sequence generation in various settings, demonstrating both its efficacy and efficiency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation",
      "titleJa": "Segment anything for satellite imagery: a strong baseline and a regional データセット for automatic field delineation",
      "link": "https://arxiv.org/abs/2506.16318",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16318v1 Announce Type: cross \nAbstract: Accurate mapping of agricultural field boundaries is essential for the efficient operation of agriculture. Automatic extraction from high-resolution satellite imagery, supported by computer vision techniques, can avoid costly ground surveys. In this paper, we present a pipeline for field delineation based on the Segment Anything Model (SAM), introducing a fine-tuning strategy to adapt SAM to this task. In addition to using published datasets, we describe a method for acquiring a complementary regional dataset that covers areas beyond current sources. Extensive experiments assess segmentation accuracy and evaluate the generalization capabilities. Our approach provides a robust baseline for automated field delineation. The new regional dataset, known as ERAS, is now publicly available.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Reliable Few-shot Learning under Dual Noises",
      "titleJa": "Reliable few-shot learning under dual noises",
      "link": "https://arxiv.org/abs/2506.16330",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16330v1 Announce Type: cross \nAbstract: Recent advances in model pre-training give rise to task adaptation-based few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic model for capturing task-specific knowledge with a few-labeled support samples of the target task.Nevertheless, existing approaches may still fail in the open world due to the inevitable in-distribution (ID) and out-of-distribution (OOD) noise from both support and query samples of the target task. With limited support samples available, i) the adverse effect of the dual noises can be severely amplified during task adaptation, and ii) the adapted model can produce unreliable predictions on query samples in the presence of the dual noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate image and region weights for support samples, based on which a clean prototype loss and a noise entropy maximization loss are proposed to achieve noise-robust task adaptation. Additionally,DETA++ employs a memory bank to store and refine clean regions for each inner-task class, based on which a Local Nearest Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping (IntraSwap) strategy to rectify ID class prototypes during task adaptation, enhancing the model's robustness to the dual noises. Extensive experiments demonstrate the effectiveness and flexibility of DETA++.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Watermarking Autoregressive Image Generation",
      "titleJa": "Watermarking autoregressive image generation",
      "link": "https://arxiv.org/abs/2506.16349",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16349v1 Announce Type: cross \nAbstract: Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset",
      "titleJa": "Clip-mg: guiding semantic attention with skeletal pose features and rgb data for micro-gesture recognition on the imigue データセット",
      "link": "https://arxiv.org/abs/2506.16385",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16385v1 Announce Type: cross \nAbstract: Micro-gesture recognition is a challenging task in affective computing due to the subtle, involuntary nature of the gestures and their low movement amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG integrates human pose (skeleton) information into the CLIP-based recognition pipeline through pose-guided semantic query generation and a gated multi-modal fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These results demonstrate both the potential of our approach and the remaining difficulty in fully adapting vision-language models like CLIP for micro-gesture recognition.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks",
      "titleJa": "Robustness evaluation of ocr-based visual document understanding under multi-modal adversarial attacks",
      "link": "https://arxiv.org/abs/2506.16407",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16407v1 Announce Type: cross \nAbstract: Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility.\n  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Efficient Transformations in Deep Learning Convolutional Neural Networks",
      "titleJa": "Efficient transformations in ディープラーニング convolutional neural networks",
      "link": "https://arxiv.org/abs/2506.16418",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16418v1 Announce Type: cross \nAbstract: This study investigates the integration of signal processing transformations -- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete Cosine Transform (DCT) -- within the ResNet50 convolutional neural network (CNN) model for image classification. The primary objective is to assess the trade-offs between computational efficiency, energy consumption, and classification accuracy during training and inference. Using the CIFAR-100 dataset (100 classes, 60,000 images), experiments demonstrated that incorporating WHT significantly reduced energy consumption while improving accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified ResNet50 incorporating WHT in the early convolutional layers achieved 74% accuracy, and an enhanced version with WHT applied to both early and late layers achieved 79% accuracy, with an average energy consumption of only 39 kJ per model. These results demonstrate the potential of WHT as a highly efficient and effective approach for energy-constrained CNN applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models",
      "titleJa": "Optimizing moe routers: design, implementation, and evaluation in トランスフォーマー models",
      "link": "https://arxiv.org/abs/2506.16419",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16419v1 Announce Type: cross \nAbstract: Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks",
      "titleJa": "Leveraging influence functions for resampling data in physics-informed neural networks",
      "link": "https://arxiv.org/abs/2506.16443",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16443v1 Announce Type: cross \nAbstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving partial differential equations (PDEs), which are ubiquitous in the quantitative sciences. Applied to both forward and inverse problems across various scientific domains, PINNs have recently emerged as a valuable tool in the field of scientific machine learning. A key aspect of their training is that the data -- spatio-temporal points sampled from the PDE's input domain -- are readily available. Influence functions, a tool from the field of explainable AI (XAI), approximate the effect of individual training points on the model, enhancing interpretability. In the present work, we explore the application of influence function-based sampling approaches for the training data. Our results indicate that such targeted resampling based on data attribution methods has the potential to enhance prediction accuracy in physics-informed neural networks, demonstrating a practical application of an XAI method in PINN training.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach",
      "titleJa": "Consumer-friendly eeg-based emotion recognition system: a multi-scale convolutional ニューラルネットワーク approach",
      "link": "https://arxiv.org/abs/2506.16448",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16448v1 Announce Type: cross \nAbstract: EEG is a non-invasive, safe, and low-risk method to record electrophysiological signals inside the brain. Especially with recent technology developments like dry electrodes, consumer-grade EEG devices, and rapid advances in machine learning, EEG is commonly used as a resource for automatic emotion recognition. With the aim to develop a deep learning model that can perform EEG-based emotion recognition in a real-life context, we propose a novel approach to utilize multi-scale convolutional neural networks to accomplish such tasks. By implementing feature extraction kernels with many ratio coefficients as well as a new type of kernel that learns key information from four separate areas of the brain, our model consistently outperforms the state-of-the-art TSception model in predicting valence, arousal, and dominance scores across many performance evaluation metrics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities",
      "titleJa": "Progressive 推論-time annealing of diffusion models for sampling from boltzmann densities",
      "link": "https://arxiv.org/abs/2506.16471",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16471v1 Announce Type: cross \nAbstract: Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: https://github.com/taraak/pita",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Subspace-Boosted Model Merging",
      "titleJa": "Subspace-boosted モデル merging",
      "link": "https://arxiv.org/abs/2506.16506",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16506v1 Announce Type: cross \nAbstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous existing merging methods) continues for more and more experts, the associated task vector space experiences rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 expert models by large margins of more than 10% when evaluated on vision benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to further quantify task similarity, offering a new interpretable perspective on model merging.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "One Sample is Enough to Make Conformal Prediction Robust",
      "titleJa": "One sample is enough to make conformal prediction robust",
      "link": "https://arxiv.org/abs/2506.16553",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16553v1 Announce Type: cross \nAbstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends this to inputs with worst-case noise. A well-established approach is to use randomized smoothing for RCP since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, current smoothing-based RCP requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a forward pass on a single randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. around 100) passes per input. Our key insight is to certify the conformal prediction procedure itself rather than individual scores. Our approach is agnostic to the setup (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Spatially-Aware Evaluation of Segmentation Uncertainty",
      "titleJa": "Spatially-aware evaluation of segmentation uncertainty",
      "link": "https://arxiv.org/abs/2506.16589",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16589v1 Announce Type: cross \nAbstract: Uncertainty maps highlight unreliable regions in segmentation predictions. However, most uncertainty evaluation metrics treat voxels independently, ignoring spatial context and anatomical structure. As a result, they may assign identical scores to qualitatively distinct patterns (e.g., scattered vs. boundary-aligned uncertainty). We propose three spatially aware metrics that incorporate structural and boundary information and conduct a thorough validation on medical imaging data from the prostate zonal segmentation challenge within the Medical Segmentation Decathlon. Our results demonstrate improved alignment with clinically important factors and better discrimination between meaningful and spurious uncertainty patterns.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE",
      "titleJa": "Flame: towards federated fine-tuning large language models through adaptive smoe",
      "link": "https://arxiv.org/abs/2506.16600",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16600v1 Announce Type: cross \nAbstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients to fine-tune models using compressed versions of global LoRA matrices, in order to accommodate various compute resources across clients. This compression requirement will lead to suboptimal performance due to information loss. To address this, we propose FLAME, a novel federated learning framework based on the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches, FLAME retains full (uncompressed) global LoRA matrices and achieves client-side adaptability by varying the number of activated experts per client. However, incorporating SMoE into federated learning introduces unique challenges, specifically, the mismatch in output magnitude from partial expert activation and the imbalance in expert training quality across clients. FLAME tackles these challenges through a lightweight rescaling mechanism and an activation-aware aggregation scheme. Empirical results across diverse computational settings demonstrate that FLAME consistently outperforms existing methods, providing a robust and effective solution for resource-adaptive federated learning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Modeling Public Perceptions of Science in Media",
      "titleJa": "Modeling public perceptions of science in media",
      "link": "https://arxiv.org/abs/2506.16622",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16622v1 Announce Type: cross \nAbstract: Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View",
      "titleJa": "Geoguess: multimodal reasoning based on hierarchy of visual information in street view",
      "link": "https://arxiv.org/abs/2506.16633",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16633v1 Announce Type: cross \nAbstract: Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Long-Context Generalization with Sparse Attention",
      "titleJa": "Long-context generalization with sparse attention",
      "link": "https://arxiv.org/abs/2506.16640",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16640v1 Announce Type: cross \nAbstract: Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $\\alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $\\alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $\\alpha$-entmax baselines on long-context generalization.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures",
      "titleJa": "Relational ディープラーニング: challenges, foundations and next-generation architectures",
      "link": "https://arxiv.org/abs/2506.16654",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16654v1 Announce Type: cross \nAbstract: Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions",
      "titleJa": "How to train your text-to-image モデル: evaluating design choices for synthetic 学習 captions",
      "link": "https://arxiv.org/abs/2506.16679",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16679v1 Announce Type: cross \nAbstract: Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Fast and Stable Diffusion Planning through Variational Adaptive Weighting",
      "titleJa": "Fast and stable diffusion planning through variational adaptive weighting",
      "link": "https://arxiv.org/abs/2506.16688",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16688v1 Announce Type: cross \nAbstract: Diffusion models have recently shown promise in offline RL. However, these methods often suffer from high training costs and slow convergence, particularly when using transformer-based denoising backbones. While several optimization strategies have been proposed -- such as modified noise schedules, auxiliary prediction targets, and adaptive loss weighting -- challenges remain in achieving stable and efficient training. In particular, existing loss weighting functions typically rely on neural network approximators, which can be ineffective in early training phases due to limited generalization capacity of MLPs when exposed to sparse feedback in the early training stages. In this work, we derive a variationally optimal uncertainty-aware weighting function and introduce a closed-form polynomial approximation method for its online estimation under the flow-based generative modeling framework. We integrate our method into a diffusion planning pipeline and evaluate it on standard offline RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our method achieves competitive performance with up to 10 times fewer training steps, highlighting its practical effectiveness.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation",
      "titleJa": "Generalizable agent modeling for agent コラボレーション-competition adaptation with multi-retrieval and dynamic generation",
      "link": "https://arxiv.org/abs/2506.16718",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16718v1 Announce Type: cross \nAbstract: Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis",
      "titleJa": "On 学習-test (mis)alignment in unsupervised combinatorial optimization: observation, empirical exploration, and analysis",
      "link": "https://arxiv.org/abs/2506.16732",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16732v1 Announce Type: cross \nAbstract: In unsupervised combinatorial optimization (UCO), during training, one aims to have continuous decisions that are promising in a probabilistic sense for each training instance, which enables end-to-end training on initially discrete and non-differentiable problems. At the test time, for each test instance, starting from continuous decisions, derandomization is typically applied to obtain the final deterministic decisions. Researchers have developed more and more powerful test-time derandomization schemes to enhance the empirical performance and the theoretical guarantee of UCO methods. However, we notice a misalignment between training and testing in the existing UCO methods. Consequently, lower training losses do not necessarily entail better post-derandomization performance, even for the training instances without any data distribution shift. Empirically, we indeed observe such undesirable cases. We explore a preliminary idea to better align training and testing in UCO by including a differentiable version of derandomization into training. Our empirical exploration shows that such an idea indeed improves training-test alignment, but also introduces nontrivial challenges into training.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
      "titleJa": "Lm-spt: lm-aligned semantic distillation for speech tokenization",
      "link": "https://arxiv.org/abs/2506.16738",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16738v1 Announce Type: cross \nAbstract: With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching",
      "titleJa": "Rapflow-tts: rapid and high-fidelity text-to-speech with improved consistency flow matching",
      "link": "https://arxiv.org/abs/2506.16741",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16741v1 Announce Type: cross \nAbstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in flow matching (FM) training. Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed. To address this challenge, RapFlow-TTS enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps. Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis. Experimental results show that RapFlow-TTS achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly",
      "titleJa": "Language-informed synthesis of rational agent models for grounded theory-of-mind reasoning on-the-fly",
      "link": "https://arxiv.org/abs/2506.16755",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16755v1 Announce Type: cross \nAbstract: Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model",
      "titleJa": "Pqcad-dm: progressive quantization and calibration-assisted distillation for extremely efficient diffusion モデル",
      "link": "https://arxiv.org/abs/2506.16776",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16776v1 Announce Type: cross \nAbstract: Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity",
      "titleJa": "What is the point of equality in 機械学習 fairness? beyond equality of opportunity",
      "link": "https://arxiv.org/abs/2506.16782",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16782v1 Announce Type: cross \nAbstract: Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML morally wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable goods and benefits, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation. Grounding ML fairness in egalitarianism -- the view that equality is a fundamental moral and social ideal -- requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong -- why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups -- and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a multifaceted egalitarian framework for ML fairness that integrates both distributive and relational equality. Drawing on critical social and political philosophy, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the ML pipeline.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection",
      "titleJa": "Loupe: a generalizable and adaptive framework for image forgery detection",
      "link": "https://arxiv.org/abs/2506.16819",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16819v1 Announce Type: cross \nAbstract: The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at https://github.com/Kamichanw/Loupe.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Bandwidth Selectors on Semiparametric Bayesian Networks",
      "titleJa": "Bandwidth selectors on semiparametric bayesian networks",
      "link": "https://arxiv.org/abs/2506.16844",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16844v1 Announce Type: cross \nAbstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models, offering flexibility in learning complex data distributions from samples. In particular, kernel density estimators (KDEs) are employed for the non-parametric component. Under the assumption of data normality, the normal rule is used to learn the bandwidth matrix for the KDEs in SPBNs. This matrix is the key hyperparameter that controls the trade-off between bias and variance. However, real-world data often deviates from normality, potentially leading to suboptimal density estimation and reduced predictive performance. This paper first establishes the theoretical framework for the application of state-of-the-art bandwidth selectors and subsequently evaluates their impact on SPBN performance. We explore the approaches of cross-validation and plug-in selectors, assessing their effectiveness in enhancing the learning capability and applicability of SPBNs. To support this investigation, we have extended the open-source package PyBNesian for SPBNs with the additional bandwidth selection techniques and conducted extensive experimental analyses. Our results demonstrate that the proposed bandwidth selectors leverage increasing information more effectively than the normal rule, which, despite its robustness, stagnates with more data. In particular, unbiased cross-validation generally outperforms the normal rule, highlighting its advantage in high sample size scenarios.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Importance of Being Lazy: Scaling Limits of Continual Learning",
      "titleJa": "The importance of being lazy: scaling limits of continual learning",
      "link": "https://arxiv.org/abs/2506.16884",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16884v1 Announce Type: cross \nAbstract: Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You",
      "titleJa": "With limited data for multimodal alignment, let the structure guide you",
      "link": "https://arxiv.org/abs/2506.16895",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16895v1 Announce Type: cross \nAbstract: Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\\unicode{x2013}$less than $1\\%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6\\%$ in classification and $91.8\\%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A deep learning and machine learning approach to predict neonatal death in the context of S\\~ao Paulo",
      "titleJa": "A ディープラーニング and 機械学習 approach to predict neonatal death in the context of s\\~ao paulo",
      "link": "https://arxiv.org/abs/2506.16929",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16929v1 Announce Type: cross \nAbstract: Neonatal death is still a concerning reality for underdeveloped and even some developed countries. Worldwide data indicate that 26.693 babies out of 1,000 births die, according to Macro Trades. To reduce this number, early prediction of endangered babies is crucial. Such prediction enables the opportunity to take ample care of the child and mother so that early child death can be avoided. In this context, machine learning was used to determine whether a newborn baby is at risk. To train the predictive model, historical data of 1.4 million newborns was used. Machine learning and deep learning techniques such as logical regression, K-nearest neighbor, random forest classifier, extreme gradient boosting (XGBoost), convolutional neural network, and long short-term memory (LSTM) were implemented using the dataset to identify the most accurate model for predicting neonatal mortality. Among the machine learning algorithms, XGBoost and random forest classifier achieved the best accuracy with 94%, while among the deep learning models, LSTM delivered the highest accuracy with 99%. Therefore, using LSTM appears to be the most suitable approach to predict whether precautionary measures for a child are necessary.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)",
      "titleJa": "Formal control for uncertain systems via contract-based probabilistic surrogates (extended version)",
      "link": "https://arxiv.org/abs/2506.16971",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16971v1 Announce Type: cross \nAbstract: The requirement for identifying accurate system representations has not only been a challenge to fulfill, but it has compromised the scalability of formal methods, as the resulting models are often too complex for effective decision making with formal correctness and performance guarantees. Focusing on probabilistic simulation relations and surrogate models of stochastic systems, we propose an approach that significantly enhances the scalability and practical applicability of such simulation relations by eliminating the need to compute error bounds directly. As a result, we provide an abstraction-based technique that scales effectively to higher dimensions while addressing complex nonlinear agent-environment interactions with infinite-horizon temporal logic guarantees amidst uncertainty. Our approach trades scalability for conservatism favorably, as demonstrated on a complex high-dimensional vehicle intersection case study.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation",
      "titleJa": "Lscd: lomb-scargle conditioned diffusion for time series imputation",
      "link": "https://arxiv.org/abs/2506.17039",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17039v1 Announce Type: cross \nAbstract: Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection",
      "titleJa": "Mawiflow benchmark: realistic flow-based evaluation for network intrusion detection",
      "link": "https://arxiv.org/abs/2506.17041",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17041v1 Announce Type: cross \nAbstract: Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic.\n  To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Flow-Based Non-stationary Temporal Regime Causal Structure Learning",
      "titleJa": "Flow-based non-stationary temporal regime causal structure learning",
      "link": "https://arxiv.org/abs/2506.17065",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17065v1 Announce Type: cross \nAbstract: Understanding causal relationships in multivariate time series is crucial in many scenarios, such as those dealing with financial or neurological data. Many such time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure. Inferring causal dependencies and regime shifts is critical for analyzing the underlying processes. However, causal structure learning in this setting is challenging due to (1) non stationarity, i.e., each regime can have its own causal graph and mixing function, and (2) complex noise distributions, which may be non Gaussian or heteroscedastic. Existing causal discovery approaches cannot address these challenges, since generally assume stationarity or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified framework for causal discovery that handles non stationary processes along with non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the number of regimes and their corresponding indices and learns each regime's Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm that maximizes the evidence lower bound of the data log likelihood. On the theoretical side, we prove, under mild assumptions, that temporal heteroscedastic causal models, introduced in FANTOM's formulation, are identifiable in both stationary and non stationary settings. In addition, extensive experiments on synthetic and real data show that FANTOM outperforms existing methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Identifiability of Deep Polynomial Neural Networks",
      "titleJa": "Identifiability of deep polynomial neural networks",
      "link": "https://arxiv.org/abs/2506.17093",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17093v1 Announce Type: cross \nAbstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. This yields both generic conditions determined by the architecture, and effective conditions that depend on the network's parameters. We also settle an open conjecture on the expected dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach its maximum.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Robust Training with Data Augmentation for Medical Imaging Classification",
      "titleJa": "Robust 学習 with data augmentation for 医療 imaging classification",
      "link": "https://arxiv.org/abs/2506.17133",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17133v1 Announce Type: cross \nAbstract: Deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging. Despite their utility, these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals. In this study, we propose a robust training algorithm with data augmentation (RTDA) to mitigate these vulnerabilities in medical image classification. We benchmark classifier robustness against adversarial perturbations and natural variations of RTDA and six competing baseline techniques, including adversarial training and data augmentation approaches in isolation and combination, using experimental data sets with three different imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that RTDA achieves superior robustness against adversarial attacks and improved generalization performance in the presence of distribution shift in each image classification task while maintaining high clean accuracy.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models",
      "titleJa": "Consistent sampling and simulation: molecular dynamics with energy-based diffusion models",
      "link": "https://arxiv.org/abs/2506.17139",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17139v1 Announce Type: cross \nAbstract: Diffusion models have recently gained significant attention due to their effectiveness in various scientific domains, including biochemistry. When trained on equilibrium molecular distributions, diffusion models provide both: a generative procedure to sample equilibrium conformations and associated forces derived from the model's scores. However, using the forces for coarse-grained molecular dynamics simulations uncovers inconsistencies in the samples generated via classical diffusion inference and simulation, despite both originating from the same model. Particularly at the small diffusion timesteps required for simulations, diffusion models fail to satisfy the Fokker-Planck equation, which governs how the score should evolve over time. We interpret this deviation as an indication of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term enforcing consistency. We demonstrate the effectiveness of our approach on toy systems, alanine dipeptide, and introduce a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and demonstrates enhanced consistency and efficient sampling.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity",
      "titleJa": "Sparse-reg: improving sample complexity in offline 強化学習 using sparsity",
      "link": "https://arxiv.org/abs/2506.17155",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17155v1 Announce Type: cross \nAbstract: In this paper, we investigate the use of small datasets in the context of offline reinforcement learning (RL). While many common offline RL benchmarks employ datasets with over a million data points, many offline RL applications rely on considerably smaller datasets. We show that offline RL algorithms can overfit on small datasets, resulting in poor performance. To address this challenge, we introduce \"Sparse-Reg\": a regularization technique based on sparsity to mitigate overfitting in offline reinforcement learning, enabling effective learning in limited data settings and outperforming state-of-the-art baselines in continuous control.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network",
      "titleJa": "Proportional sensitivity in generative adversarial network (gan)-augmented brain tumor classification using convolutional ニューラルネットワーク",
      "link": "https://arxiv.org/abs/2506.17165",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17165v1 Announce Type: cross \nAbstract: Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Continual Learning with Columnar Spiking Neural Networks",
      "titleJa": "Continual learning with columnar spiking neural networks",
      "link": "https://arxiv.org/abs/2506.17169",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17169v1 Announce Type: cross \nAbstract: This study investigates columnar-organized spiking neural networks (SNNs) for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). Our optimal configuration learns ten sequential MNIST tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Facial Landmark Visualization and Emotion Recognition Through Neural Networks",
      "titleJa": "Facial landmark visualization and emotion recognition through neural networks",
      "link": "https://arxiv.org/abs/2506.17191",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17191v1 Announce Type: cross \nAbstract: Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning",
      "titleJa": "Network sparsity unlocks the scaling potential of deep 強化学習",
      "link": "https://arxiv.org/abs/2506.17204",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17204v1 Announce Type: cross \nAbstract: Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens",
      "titleJa": "Machine mental imagery: empower multimodal reasoning with latent visual tokens",
      "link": "https://arxiv.org/abs/2506.17218",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17218v1 Announce Type: cross \nAbstract: Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Behaviour Planning: A Toolkit for Diverse Planning",
      "titleJa": "Behaviour planning: a toolkit for diverse planning",
      "link": "https://arxiv.org/abs/2405.04300",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.04300v3 Announce Type: replace \nAbstract: Diverse planning approaches are utilised in real-world applications like risk management, automated streamed data analysis, and malware detection. The current diverse planning formulations encode the diversity model as a distance function, which is computational inexpensive when comparing two plans. However, such modelling approach limits what can be encoded as measure of diversity, as well as the ability to explain why two plans are different. This paper introduces a novel approach to the diverse planning problem, allowing for more expressive modelling of diversity using a n-dimensional grid representation, where each dimension corresponds to a user-defined feature. Furthermore, we present a novel toolkit that generates diverse plans based on such customisable diversity models, called \\emph{Behaviour Planning}. We provide an implementation for behaviour planning using planning-as-satisfiability. An empirical evaluation of our implementation shows that behaviour planning significantly outperforms the current diverse planning method in generating diverse plans measured on our new customisable diversity models. Our implementation is the first diverse planning approach to support planning categories beyond classical planning, such as over-subscription and numerical planning.",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "POV Learning: Individual Alignment of Multimodal Models using Human Perception",
      "titleJa": "Pov learning: individual alignment of multimodal models using human perception",
      "link": "https://arxiv.org/abs/2405.04443",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.04443v2 Announce Type: replace \nAbstract: Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback. This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data. However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably. Since perception differs for each person, the same situation is observed differently. Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ. We hypothesize that individual perception patterns can be used for improving the alignment on an individual level. We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments. For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer. Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment. It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values.",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Quantifying artificial intelligence through algorithmic generalization",
      "titleJa": "Quantifying 人工知能 through algorithmic generalization",
      "link": "https://arxiv.org/abs/2411.05943",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.05943v2 Announce Type: replace \nAbstract: The rapid development of artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, AI systems fall short on tests requiring algorithmic reasoning -- a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no theoretical framework exists to quantify algorithmic reasoning in AI systems. Here, we adopt a framework from computational complexity theory to quantify algorithmic generalization using algebraic expressions: algebraic circuit complexity. Algebraic circuit complexity theory -- the study of algebraic expressions as circuit models -- is a natural framework to study the complexity of algorithmic computation. Algebraic circuit complexity enables the study of generalization by defining benchmarks in terms of the computational requirements to solve a problem. Moreover, algebraic circuits are generic mathematical objects; an arbitrarily large number of samples can be generated for a specified circuit, making it an ideal experimental sandbox for the data-hungry models that are used today. In this Perspective, we adopt tools from algebraic circuit complexity, apply them to formalize a science of algorithmic generalization, and address key challenges for its successful application to AI science.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning",
      "titleJa": "Autosculpt: a pattern-based モデル auto-pruning framework using 強化学習 and graph learning",
      "link": "https://arxiv.org/abs/2412.18091",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2412.18091v2 Announce Type: replace \nAbstract: As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at https://anonymous.4open.science/r/AutoSculpt-DDA0",
      "summary": "arXiv:2412.",
      "summaryJa": "Arxiv:2412.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "AGI-Driven Generative Semantic Communications: Principles and Practices",
      "titleJa": "Agi-driven generative semantic communications: principles and practices",
      "link": "https://arxiv.org/abs/2504.14947",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.14947v2 Announce Type: replace \nAbstract: Semantic communications leverage artificial intelligence (AI) technologies to extract semantic information for efficient data delivery, thereby significantly reducing communication cost. With the evolution towards artificial general intelligence (AGI), the increasing demands for AGI services pose new challenges to semantic communications. In this context, an AGI application is typically defined on a general-sense task, covering a broad, even unforeseen, set of objectives, as well as driven by the need for a human-friendly interface in forms (e.g., videos, images, or text) easily understood by human users.In response, we introduce an AGI-driven communication paradigm for supporting AGI applications, called generative semantic communication (GSC). We first describe the basic concept of GSC and its difference from existing semantic communications, and then introduce a general framework of GSC based on advanced AI technologies including foundation models and generative models. Two case studies are presented to verify the advantages of GSC. Finally, open challenges and new research directions are discussed to stimulate this line of research and pave the way for practical applications.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation",
      "titleJa": "Advancing embodied agent セキュリティ: from safety benchmarks to input moderation",
      "link": "https://arxiv.org/abs/2504.15699",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.15699v3 Announce Type: replace \nAbstract: Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation",
      "titleJa": "Preference-driven multi-objective combinatorial optimization with conditional computation",
      "link": "https://arxiv.org/abs/2506.08898",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.08898v2 Announce Type: replace \nAbstract: Recent deep reinforcement learning methods have achieved remarkable success in solving multi-objective combinatorial optimization problems (MOCOPs) by decomposing them into multiple subproblems, each associated with a specific weight vector. However, these methods typically treat all subproblems equally and solve them using a single model, hindering the effective exploration of the solution space and thus leading to suboptimal performance. To overcome the limitation, we propose POCCO, a novel plug-and-play framework that enables adaptive selection of model structures for subproblems, which are subsequently optimized based on preference signals rather than explicit reward values. Specifically, we design a conditional computation block that routes subproblems to specialized neural architectures. Moreover, we propose a preference-driven optimization algorithm that learns pairwise preferences between winning and losing solutions. We evaluate the efficacy and versatility of POCCO by applying it to two state-of-the-art neural methods for MOCOPs. Experimental results across four classic MOCOP benchmarks demonstrate its significant superiority and strong generalization.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Voices of Her: Analyzing Gender Differences in the AI Publication World",
      "titleJa": "Voices of her: analyzing gender differences in the ai publication world",
      "link": "https://arxiv.org/abs/2305.14597",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2305.14597v2 Announce Type: replace-cross \nAbstract: While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.",
      "summary": "arXiv:2305.",
      "summaryJa": "Arxiv:2305.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
      "titleJa": "Open-set graph anomaly detection via normal structure regularisation",
      "link": "https://arxiv.org/abs/2311.06835",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2311.06835v5 Announce Type: replace-cross \nAbstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.",
      "summary": "arXiv:2311.",
      "summaryJa": "Arxiv:2311.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency",
      "titleJa": "Guided absolutegrad: magnitude of gradients matters to explanation's localization and saliency",
      "link": "https://arxiv.org/abs/2404.15564",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2404.15564v2 Announce Type: replace-cross \nAbstract: This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction. We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. We propose two propositions for these two objectives and prove the necessity of evaluating them. We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model. Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.",
      "summary": "arXiv:2404.",
      "summaryJa": "Arxiv:2404.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval",
      "titleJa": "Promptdsi: prompt-based rehearsal-free instance-wise incremental learning for document retrieval",
      "link": "https://arxiv.org/abs/2406.12593",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.12593v3 Announce Type: replace-cross \nAbstract: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach",
      "titleJa": "Understanding and reducing the class-dependent effects of data augmentation with a two-player game approach",
      "link": "https://arxiv.org/abs/2407.03146",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2407.03146v4 Announce Type: replace-cross \nAbstract: Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.",
      "summary": "arXiv:2407.",
      "summaryJa": "Arxiv:2407.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension",
      "titleJa": "Mapper: multimodal prior-guided parameter efficient tuning for referring expression comprehension",
      "link": "https://arxiv.org/abs/2409.13609",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2409.13609v4 Announce Type: replace-cross \nAbstract: Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters. Our code is available at https://github.com/liuting20/MaPPER.",
      "summary": "arXiv:2409.",
      "summaryJa": "Arxiv:2409.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "ALTA: Compiler-Based Analysis of Transformers",
      "titleJa": "Alta: compiler-based analysis of transformers",
      "link": "https://arxiv.org/abs/2410.18077",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.18077v2 Announce Type: replace-cross \nAbstract: We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation",
      "titleJa": "Cyclic vision-language manipulator: towards reliable and fine-grained image interpretation for automated report generation",
      "link": "https://arxiv.org/abs/2411.05261",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.05261v3 Announce Type: replace-cross \nAbstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term \"cyclic manipulation\". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse",
      "titleJa": "On the limits of language generation: trade-offs between hallucination and mode collapse",
      "link": "https://arxiv.org/abs/2411.09642",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.09642v2 Announce Type: replace-cross \nAbstract: Specifying all desirable properties of a language model is challenging, but certain requirements seem essential. Given samples from an unknown language, the trained model should produce valid strings not seen in training and be expressive enough to capture the language's full richness. Otherwise, outputting invalid strings constitutes \"hallucination,\" and failing to capture the full range leads to \"mode collapse.\" We ask if a language model can meet both requirements.\n  We investigate this within a statistical language generation setting building on Gold and Angluin. Here, the model receives random samples from a distribution over an unknown language K, which belongs to a possibly infinite collection of languages. The goal is to generate unseen strings from K. We say the model generates from K with consistency and breadth if, as training size increases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in language generation are possible. We answer this negatively: for a large class of language models, including next-token prediction models, this is impossible for most collections of candidate languages. This contrasts with [KM24]'s result, showing consistent generation without breadth is possible for any countable collection of languages. Our finding highlights that generation with breadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples needed for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is achievable for any countable collection of languages when negative examples (strings outside K) are available alongside positive ones. This suggests that post-training feedback, which encodes negative examples, can be crucial in reducing hallucinations while limiting mode collapse.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning Multi-Branch Cooperation for Enhanced Click-Through Rate Prediction at Taobao",
      "titleJa": "Learning multi-branch cooperation for enhanced click-through rate prediction at taobao",
      "link": "https://arxiv.org/abs/2411.13057",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.13057v2 Announce Type: replace-cross \nAbstract: Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model's capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Extensible Feature Grouping and Crossing (EFGC) branch that promotes the model's memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance explicit and implicit feature crossing for improved generalization. Among these branches, a novel cooperation scheme is proposed based on two principles: Branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations on the same inputs. This cooperation strategy improves learning through mutual knowledge sharing and boosts the discovery of diverse feature interactions across branches. Experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet's superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available online.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control",
      "titleJa": "Song form-aware full-song text-to-lyrics generation with multi-level granularity syllable count control",
      "link": "https://arxiv.org/abs/2411.13100",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.13100v2 Announce Type: replace-cross \nAbstract: Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Representation Learning of Point Cloud Upsampling in Global and Local Inputs",
      "titleJa": "Representation learning of point クラウド upsampling in global and local inputs",
      "link": "https://arxiv.org/abs/2501.07076",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.07076v3 Announce Type: replace-cross \nAbstract: In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Guaranteed prediction sets for functional surrogate models",
      "titleJa": "Guaranteed prediction sets for functional surrogate models",
      "link": "https://arxiv.org/abs/2501.18426",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.18426v2 Announce Type: replace-cross \nAbstract: We propose a method for obtaining statistically guaranteed prediction sets for functional machine learning methods: surrogate models which map between function spaces, motivated by the need to build reliable PDE emulators. The method constructs nested prediction sets on a low-dimensional representation (an SVD) of the surrogate model's error, and then maps these sets to the prediction space using set-propagation techniques. This results in prediction sets for functional surrogate models with conformal prediction coverage guarantees. We use zonotopes as basis of the set construction, which allow an exact linear propagation and are closed under Cartesian products, making them well-suited to this high-dimensional problem. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also introduce a technique to capture the truncation error of the SVD, preserving the guarantees of the method.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue",
      "titleJa": "Selective use of yannakakis' アルゴリズム to improve query performance: 機械学習 to the rescue",
      "link": "https://arxiv.org/abs/2502.20233",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.20233v2 Announce Type: replace-cross \nAbstract: Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not.\n  In this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning",
      "titleJa": "Eau de $q$-network: adaptive distillation of neural networks in deep 強化学習",
      "link": "https://arxiv.org/abs/2503.01437",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.01437v2 Announce Type: replace-cross \nAbstract: Recent works have successfully demonstrated that sparse deep reinforcement learning agents can be competitive against their dense counterparts. This opens up opportunities for reinforcement learning applications in fields where inference time and memory requirements are cost-sensitive or limited by hardware. Until now, dense-to-sparse methods have relied on hand-designed sparsity schedules that are not synchronized with the agent's learning pace. Crucially, the final sparsity level is chosen as a hyperparameter, which requires careful tuning as setting it too high might lead to poor performances. In this work, we address these shortcomings by crafting a dense-to-sparse algorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at the agent's learning pace, we consider multiple online networks with different sparsity levels, where each online network is trained from a shared target network. At each target update, the online network with the smallest loss is chosen as the next target network, while the other networks are replaced by a pruned version of the chosen network. We evaluate the proposed approach on the Atari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN reaches high sparsity levels while keeping performances high.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
      "titleJa": "Qg-sms: enhancing test item analysis via student modeling and simulation",
      "link": "https://arxiv.org/abs/2503.05888",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.05888v2 Announce Type: replace-cross \nAbstract: While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Decentralized Collective World Model for Emergent Communication and Coordination",
      "titleJa": "Decentralized collective world モデル for emergent communication and coordination",
      "link": "https://arxiv.org/abs/2504.03353",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.03353v2 Announce Type: replace-cross \nAbstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations",
      "titleJa": "Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations",
      "link": "https://arxiv.org/abs/2504.08584",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.08584v2 Announce Type: replace-cross \nAbstract: Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "DeepSelective: Interpretable Prognosis Prediction via Feature Selection and Compression in EHR Data",
      "titleJa": "Deepselective: interpretable prognosis prediction via feature selection and compression in ehr data",
      "link": "https://arxiv.org/abs/2504.11264",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.11264v2 Announce Type: replace-cross \nAbstract: The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http://www.healthinformaticslab.org/supp/resources.php .",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation",
      "titleJa": "Spin-ode: stiff physics-informed neural ode for chemical reaction rate estimation",
      "link": "https://arxiv.org/abs/2505.05625",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.05625v2 Announce Type: replace-cross \nAbstract: Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks",
      "titleJa": "Representation learning with mutual influence of modalities for node classification in multi-modal heterogeneous networks",
      "link": "https://arxiv.org/abs/2505.07895",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.07895v3 Announce Type: replace-cross \nAbstract: Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression",
      "titleJa": "Breaking the compression ceiling: data-free pipeline for ultra-efficient delta compression",
      "link": "https://arxiv.org/abs/2505.13563",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.13563v2 Announce Type: replace-cross \nAbstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis",
      "titleJa": "Cryoccd: conditional cycle-consistent diffusion with biophysical modeling for cryo-em synthesis",
      "link": "https://arxiv.org/abs/2505.23444",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.23444v2 Announce Type: replace-cross \nAbstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning",
      "titleJa": "Optimizing sensory neurons: nonlinear attention mechanisms for accelerated convergence in permutation-invariant neural networks for 強化学習",
      "link": "https://arxiv.org/abs/2506.00691",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.00691v3 Announce Type: replace-cross \nAbstract: Training reinforcement learning (RL) agents often requires significant computational resources and prolonged training durations. To address this challenge, we build upon prior work that introduced a neural architecture with permutation-invariant sensory processing. We propose a modified attention mechanism that applies a non-linear transformation to the key vectors (K), producing enriched representations (K') through a custom mapping function. This Nonlinear Attention (NLA) mechanism enhances the representational capacity of the attention layer, enabling the agent to learn more expressive feature interactions. As a result, our model achieves significantly faster convergence and improved training efficiency, while maintaining performance on par with the baseline. These results highlight the potential of nonlinear attention mechanisms to accelerate reinforcement learning without sacrificing effectiveness.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution",
      "titleJa": "Towards efficient few-shot graph neural architecture search via partitioning gradient contribution",
      "link": "https://arxiv.org/abs/2506.01231",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.01231v2 Announce Type: replace-cross \nAbstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution",
      "titleJa": "Info-coevolution: an efficient framework for data モデル coevolution",
      "link": "https://arxiv.org/abs/2506.08070",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.08070v2 Announce Type: replace-cross \nAbstract: Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Convergent Linear Representations of Emergent Misalignment",
      "titleJa": "Convergent linear representations of emergent misalignment",
      "link": "https://arxiv.org/abs/2506.11618",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11618v2 Announce Type: replace-cross \nAbstract: Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Human-like Forgetting Curves in Deep Neural Networks",
      "titleJa": "Human-like forgetting curves in deep neural networks",
      "link": "https://arxiv.org/abs/2506.12034",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12034v2 Announce Type: replace-cross \nAbstract: This study bridges cognitive science and neural network design by examining whether artificial models exhibit human-like forgetting curves. Drawing upon Ebbinghaus' seminal work on memory decay and principles of spaced repetition, we propose a quantitative framework to measure information retention in neural networks. Our approach computes the recall probability by evaluating the similarity between a network's current hidden state and previously stored prototype representations. This retention metric facilitates the scheduling of review sessions, thereby mitigating catastrophic forgetting during deployment and enhancing training efficiency by prompting targeted reviews. Our experiments with Multi-Layer Perceptrons reveal human-like forgetting curves, with knowledge becoming increasingly robust through scheduled reviews. This alignment between neural network forgetting curves and established human memory models identifies neural networks as an architecture that naturally emulates human memory decay and can inform state-of-the-art continual learning algorithms.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
      "titleJa": "A minimalist method for fine-tuning text-to-image diffusion models",
      "link": "https://arxiv.org/abs/2506.12036",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12036v2 Announce Type: replace-cross \nAbstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the \"golden noise\" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Two Heads Are Better than One: Simulating Large Transformers with Small Ones",
      "titleJa": "Two heads are better than one: simulating large transformers with small ones",
      "link": "https://arxiv.org/abs/2506.12220",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12220v2 Announce Type: replace-cross \nAbstract: The quadratic complexity of self-attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences?\n  In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Machine Learning Methods for Small Data and Upstream Bioprocessing Applications: A Comprehensive Review",
      "titleJa": "機械学習 methods for small data and upstream bioprocessing applications: a comprehensive review",
      "link": "https://arxiv.org/abs/2506.12322",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.12322v2 Announce Type: replace-cross \nAbstract: Data is crucial for machine learning (ML) applications, yet acquiring large datasets can be costly and time-consuming, especially in complex, resource-intensive fields like biopharmaceuticals. A key process in this industry is upstream bioprocessing, where living cells are cultivated and optimised to produce therapeutic proteins and biologics. The intricate nature of these processes, combined with high resource demands, often limits data collection, resulting in smaller datasets. This comprehensive review explores ML methods designed to address the challenges posed by small data and classifies them into a taxonomy to guide practical applications. Furthermore, each method in the taxonomy was thoroughly analysed, with a detailed discussion of its core concepts and an evaluation of its effectiveness in tackling small data challenges, as demonstrated by application results in the upstream bioprocessing and other related domains. By analysing how these methods tackle small data challenges from different perspectives, this review provides actionable insights, identifies current research gaps, and offers guidance for leveraging ML in data-constrained environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models",
      "titleJa": "Decoupled classifier-free guidance for counterfactual diffusion models",
      "link": "https://arxiv.org/abs/2506.14399",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14399v2 Announce Type: replace-cross \nAbstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Refining music sample identification with a self-supervised graph neural network",
      "titleJa": "Refining music sample identification with a self-supervised graph ニューラルネットワーク",
      "link": "https://arxiv.org/abs/2506.14684",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14684v2 Announce Type: replace-cross \nAbstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.\n  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\n  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "WebXAII: an open-source web framework to study human-XAI interaction",
      "titleJa": "Webxaii: an open-source web framework to study human-xai interaction",
      "link": "https://arxiv.org/abs/2506.14777",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14777v2 Announce Type: replace-cross \nAbstract: This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis",
      "titleJa": "Efficient retail video annotation: a robust key frame generation approach for product and customer interaction analysis",
      "link": "https://arxiv.org/abs/2506.14854",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14854v2 Announce Type: replace-cross \nAbstract: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution",
      "titleJa": "One-step diffusion for detail-rich and temporally consistent video super-resolution",
      "link": "https://arxiv.org/abs/2506.15591",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15591v2 Announce Type: replace-cross \nAbstract: It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at https://github.com/yjsunnn/DLoRAL.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction",
      "titleJa": "Federated learning for mri-based brainage: a multicenter study on post-stroke functional outcome prediction",
      "link": "https://arxiv.org/abs/2506.15626",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15626v2 Announce Type: replace-cross \nAbstract: $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Sekai: A Video Dataset towards World Exploration",
      "titleJa": "Sekai: a video データセット towards world exploration",
      "link": "https://arxiv.org/abs/2506.15675",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15675v2 Announce Type: replace-cross \nAbstract: Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications. The project page is https://lixsp11.github.io/sekai-project/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv AI",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks",
      "titleJa": "Verifiable safety q-filters via hamilton-jacobi reachability and multiplicative q-networks",
      "link": "https://arxiv.org/abs/2506.15693",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15693v1 Announce Type: new \nAbstract: Recent learning-based safety filters have outperformed conventional methods, such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting to complex constraints. However, these learning-based approaches lack formal safety guarantees. In this work, we introduce a verifiable model-free safety filter based on Hamilton-Jacobi reachability analysis. Our primary contributions include: 1) extending verifiable self-consistency properties for Q value functions, 2) proposing a multiplicative Q-network structure to mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification pipeline capable of soundly verifying these self-consistency properties. Our proposed approach successfully synthesizes formally verified, model-free safety certificates across four standard safe-control benchmarks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction",
      "titleJa": "Development of a multiprocessing interface genetic アルゴリズム for optimising a multilayer perceptron for disease prediction",
      "link": "https://arxiv.org/abs/2506.15694",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15694v1 Announce Type: new \nAbstract: This study introduces a framework that integrates nonlinear feature extraction, classification, and efficient optimization. First, kernel principal component analysis with a radial basis function kernel reduces dimensionality while preserving 95% of the variance. Second, a multilayer perceptron (MLP) learns to predict disease status. Finally, a modified multiprocessing genetic algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten generations. We evaluated this approach on three datasets: the Wisconsin Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100% for chronic kidney disease. These results outperform those of other methods, such as grid search, random search, and Bayesian optimization. Compared with a standard genetic algorithm, kernel PCA revealed nonlinear relationships that improved classification, and the MIGA's parallel fitness evaluations reduced the tuning time by approximately 60%. The genetic algorithm incurs high computational cost from sequential fitness evaluations, but our multiprocessing interface GA (MIGA) parallelizes this step, slashing the tuning time and steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for breast cancer, Parkinson's disease, and CKD, respectively.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Global Context-aware Representation Learning for Spatially Resolved Transcriptomics",
      "titleJa": "Global context-aware representation learning for spatially resolved transcriptomics",
      "link": "https://arxiv.org/abs/2506.15698",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15698v1 Announce Type: new \nAbstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that captures the spatial context of cells within tissues, enabling the study of complex biological networks. Recent graph-based methods leverage both gene expression and spatial information to identify relevant spatial domains. However, these approaches fall short in obtaining meaningful spot representations, especially for spots near spatial domain boundaries, as they heavily emphasize adjacent spots that have minimal feature differences from an anchor node. To address this, we propose Spotscape, a novel framework that introduces the Similarity Telescope module to capture global relationships between multiple spots. Additionally, we propose a similarity scaling strategy to regulate the distances between intra- and inter-slice spots, facilitating effective multi-slice integration. Extensive experiments demonstrate the superiority of Spotscape in various downstream tasks, including single-slice and multi-slice scenarios. Our code is available at the following link: https: //github.com/yunhak0/Spotscape.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "An application of machine learning to the motion response prediction of floating assets",
      "titleJa": "An application of 機械学習 to the motion response prediction of floating assets",
      "link": "https://arxiv.org/abs/2506.15713",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15713v1 Announce Type: new \nAbstract: The real-time prediction of floating offshore asset behavior under stochastic metocean conditions remains a significant challenge in offshore engineering. While traditional empirical and frequency-domain methods work well in benign conditions, they struggle with both extreme sea states and nonlinear responses. This study presents a supervised machine learning approach using multivariate regression to predict the nonlinear motion response of a turret-moored vessel in 400 m water depth. We developed a machine learning workflow combining a gradient-boosted ensemble method with a custom passive weathervaning solver, trained on approximately $10^6$ samples spanning 100 features. The model achieved mean prediction errors of less than 5% for critical mooring parameters and vessel heading accuracy to within 2.5 degrees across diverse metocean conditions, significantly outperforming traditional frequency-domain methods. The framework has been successfully deployed on an operational facility, demonstrating its efficacy for real-time vessel monitoring and operational decision-making in offshore environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems",
      "titleJa": "Data-driven heat pump management: combining 機械学習 with anomaly detection for residential hot water systems",
      "link": "https://arxiv.org/abs/2506.15719",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15719v1 Announce Type: new \nAbstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for sustainable energy systems, but their efficiency in producing hot water remains restricted by conventional threshold-based control methods. Although machine learning (ML) has been successfully implemented for various HP applications, optimization of household hot water demand forecasting remains understudied. This paper addresses this problem by introducing a novel approach that combines predictive ML with anomaly detection to create adaptive hot water production strategies based on household-specific consumption patterns. Our key contributions include: (1) a composite approach combining ML and isolation forest (iForest) to forecast household demand for hot water and steer responsive HP operations; (2) multi-step feature selection with advanced time-series analysis to capture complex usage patterns; (3) application and tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention mechanism on data from different types of real HP installations; and (4) experimental validation on six real household installations. Our experiments show that the best-performing model LightGBM achieves superior performance, with RMSE improvements of up to 9.37\\% compared to LSTM variants with $R^2$ values between 0.748-0.983. For anomaly detection, our iForest implementation achieved an F1-score of 0.87 with a false alarm rate of only 5.2\\%, demonstrating strong generalization capabilities across different household types and consumption patterns, making it suitable for real-world HP deployments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning",
      "titleJa": "Tripartite weight-space ensemble for few-shot class-incremental learning",
      "link": "https://arxiv.org/abs/2506.15720",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15720v1 Announce Type: new \nAbstract: Few-shot class incremental learning (FSCIL) enables the continual learning of new concepts with only a few training examples. In FSCIL, the model undergoes substantial updates, making it prone to forgetting previous concepts and overfitting to the limited new examples. Most recent trend is typically to disentangle the learning of the representation from the classification head of the model. A well-generalized feature extractor on the base classes (many examples and many classes) is learned, and then fixed during incremental learning. Arguing that the fixed feature extractor restricts the model's adaptability to new classes, we introduce a novel FSCIL method to effectively address catastrophic forgetting and overfitting issues. Our method enables to seamlessly update the entire model with a few examples. We mainly propose a tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base, immediately previous, and current models in weight-space, especially for the classification heads of the models. Then, it collaboratively maintains knowledge from the base and previous models. In addition, we recognize the challenges of distilling generalized representations from the previous model from scarce data. Hence, we suggest a regularization loss term using amplified data knowledge distillation. Simply intermixing the few-shot data, we can produce richer data enabling the distillation of critical knowledge from the previous model. Consequently, we attain state-of-the-art results on the miniImageNet, CUB200, and CIFAR100 datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling",
      "titleJa": "Deepj: graph convolutional transformers with differentiable pooling for patient trajectory modeling",
      "link": "https://arxiv.org/abs/2506.15809",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15809v1 Announce Type: new \nAbstract: In recent years, graph learning has gained significant interest for modeling complex interactions among medical events in structured Electronic Health Record (EHR) data. However, existing graph-based approaches often work in a static manner, either restricting interactions within individual encounters or collapsing all historical encounters into a single snapshot. As a result, when it is necessary to identify meaningful groups of medical events spanning longitudinal encounters, existing methods are inadequate in modeling interactions cross encounters while accounting for temporal dependencies. To address this limitation, we introduce Deep Patient Journey (DeepJ), a novel graph convolutional transformer model with differentiable graph pooling to effectively capture intra-encounter and inter-encounter medical event interactions. DeepJ can identify groups of temporally and functionally related medical events, offering valuable insights into key event clusters pertinent to patient outcome prediction. DeepJ significantly outperformed five state-of-the-art baseline models while enhancing interpretability, demonstrating its potential for improved patient risk stratification.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions",
      "titleJa": "Optimizing bidding strategies in first-price auctions in binary feedback setting with predictions",
      "link": "https://arxiv.org/abs/2506.15817",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15817v1 Announce Type: new \nAbstract: This paper studies Vickrey first-price auctions under binary feedback. Leveraging the enhanced performance of machine learning algorithms, the new algorithm uses past information to improve the regret bounds of the BROAD-OMD algorithm. Motivated by the growing relevance of first-price auctions and the predictive capabilities of machine learning models, this paper proposes a new algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages predictions of the highest competing bid. This paper's main contribution is an algorithm that achieves zero regret under accurate predictions. Additionally, a bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain normality conditions.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "AI-based modular warning machine for risk identification in proximity healthcare",
      "titleJa": "Ai-based modular warning machine for risk identification in proximity ヘルスケア",
      "link": "https://arxiv.org/abs/2506.15823",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15823v1 Announce Type: new \nAbstract: \"DHEAL-COM - Digital Health Solutions in Community Medicine\" is a research and technology project funded by the Italian Department of Health for the development of digital solutions of interest in proximity healthcare. The activity within the DHEAL-COM framework allows scientists to gather a notable amount of multi-modal data whose interpretation can be performed by means of machine learning algorithms. The present study illustrates a general automated pipeline made of numerous unsupervised and supervised methods that can ingest such data, provide predictive results, and facilitate model interpretations via feature identification.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters",
      "titleJa": "Heterogeneous federated 強化学習 using wasserstein barycenters",
      "link": "https://arxiv.org/abs/2506.15825",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15825v1 Announce Type: new \nAbstract: In this paper, we first propose a novel algorithm for model fusion that leverages Wasserstein barycenters in training a global Deep Neural Network (DNN) in a distributed architecture. To this end, we divide the dataset into equal parts that are fed to \"agents\" who have identical deep neural networks and train only over the dataset fed to them (known as the local dataset). After some training iterations, we perform an aggregation step where we combine the weight parameters of all neural networks using Wasserstein barycenters. These steps form the proposed algorithm referred to as FedWB. Moreover, we leverage the processes created in the first part of the paper to develop an algorithm to tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test experiment is the CartPole toy problem, where we vary the lengths of the poles to create heterogeneous environments. We train a deep Q-Network (DQN) in each environment to learn to control each cart, while occasionally performing a global aggregation step to generalize the local models; the end outcome is a global DQN that functions across all environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "In-field Calibration of Low-Cost Sensors through XGBoost $\\&$ Aggregate Sensor Data",
      "titleJa": "In-field calibration of low-cost sensors through xgboost $\\&$ aggregate sensor data",
      "link": "https://arxiv.org/abs/2506.15840",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15840v1 Announce Type: new \nAbstract: Effective large-scale air quality monitoring necessitates distributed sensing due to the pervasive and harmful nature of particulate matter (PM), particularly in urban environments. However, precision comes at a cost: highly accurate sensors are expensive, limiting the spatial deployments and thus their coverage. As a result, low-cost sensors have become popular, though they are prone to drift caused by environmental sensitivity and manufacturing variability. This paper presents a model for in-field sensor calibration using XGBoost ensemble learning to consolidate data from neighboring sensors. This approach reduces dependence on the presumed accuracy of individual sensors and improves generalization across different locations.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Improving Rectified Flow with Boundary Conditions",
      "titleJa": "Improving rectified flow with boundary conditions",
      "link": "https://arxiv.org/abs/2506.15864",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15864v1 Announce Type: new \nAbstract: Rectified Flow offers a simple and effective approach to high-quality generative modeling by learning a velocity field. However, we identify a limitation in directly modeling the velocity with an unconstrained neural network: the learned velocity often fails to satisfy certain boundary conditions, leading to inaccurate velocity field estimations that deviate from the desired ODE. This issue is particularly critical during stochastic sampling at inference, as the score function's errors are amplified near the boundary. To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary RF Model), in which we enforce boundary conditions with a minimal code modification. Boundary RF Model improves performance over vanilla RF model, demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and 8.98% improvement using SDE sampling.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings",
      "titleJa": "Job market cheat codes: prototyping salary prediction and job grouping with synthetic job listings",
      "link": "https://arxiv.org/abs/2506.15879",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15879v1 Announce Type: new \nAbstract: This paper presents a machine learning methodology prototype using a large synthetic dataset of job listings to identify trends, predict salaries, and group similar job roles. Employing techniques such as regression, classification, clustering, and natural language processing (NLP) for text-based feature extraction and representation, this study aims to uncover the key features influencing job market dynamics and provide valuable insights for job seekers, employers, and researchers. Exploratory data analysis was conducted to understand the dataset's characteristics. Subsequently, regression models were developed to predict salaries, classification models to predict job titles, and clustering techniques were applied to group similar jobs. The analyses revealed significant factors influencing salary and job roles, and identified distinct job clusters based on the provided data. While the results are based on synthetic data and not intended for real-world deployment, the methodology demonstrates a transferable framework for job market analysis.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders",
      "titleJa": "T-shred: symbolic regression for regularization and モデル discovery with トランスフォーマー shallow recurrent decoders",
      "link": "https://arxiv.org/abs/2506.15881",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15881v1 Announce Type: new \nAbstract: SHallow REcurrent Decoders (SHRED) are effective for system identification and forecasting from sparse sensor measurements. Such models are light-weight and computationally efficient, allowing them to be trained on consumer laptops. SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding respectively. Despite the relatively simple structure of SHRED, they are able to predict chaotic dynamical systems on different physical, spatial, and temporal scales directly from a sparse set of sensor measurements. In this work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal encoding which improves performance on next-step state prediction on large datasets. We also introduce a sparse identification of nonlinear dynamics (SINDy) attention mechanism into T-SHRED to perform symbolic regression directly on the latent space as part of the model regularization architecture. Symbolic regression improves model interpretability by learning and regularizing the dynamics of the latent space during training. We analyze the performance of T-SHRED on three different dynamical systems ranging from low-data to high-data regimes. We observe that SINDy attention T-SHRED accurately predicts future frames based on an interpretable symbolic model across all tested datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Formal Models of Active Learning from Contrastive Examples",
      "titleJa": "Formal models of active learning from contrastive examples",
      "link": "https://arxiv.org/abs/2506.15893",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15893v1 Announce Type: new \nAbstract: Machine learning can greatly benefit from providing learning algorithms with pairs of contrastive training examples -- typically pairs of instances that differ only slightly, yet have different class labels. Intuitively, the difference in the instances helps explain the difference in the class labels. This paper proposes a theoretical framework in which the effect of various types of contrastive examples on active learners is studied formally. The focus is on the sample complexity of learning concept classes and how it is influenced by the choice of contrastive examples. We illustrate our results with geometric concept classes and classes of Boolean functions. Interestingly, we reveal a connection between learning from contrastive examples and the classical model of self-directed learning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation",
      "titleJa": "Trajdiff: diffusion bridge network with semantic alignment for trajectory similarity computation",
      "link": "https://arxiv.org/abs/2506.15898",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15898v1 Announce Type: new \nAbstract: With the proliferation of location-tracking technologies, massive volumes of trajectory data are continuously being collected. As a fundamental task in trajectory data mining, trajectory similarity computation plays a critical role in a wide range of real-world applications. However, existing learning-based methods face three challenges: First, they ignore the semantic gap between GPS and grid features in trajectories, making it difficult to obtain meaningful trajectory embeddings. Second, the noise inherent in the trajectories, as well as the noise introduced during grid discretization, obscures the true motion patterns of the trajectories. Third, existing methods focus solely on point-wise and pair-wise losses, without utilizing the global ranking information obtained by sorting all trajectories according to their similarity to a given trajectory. To address the aforementioned challenges, we propose a novel trajectory similarity computation framework, named TrajDiff. Specifically, the semantic alignment module relies on cross-attention and an attention score mask mechanism with adaptive fusion, effectively eliminating semantic discrepancies between data at two scales and generating a unified representation. Additionally, the DDBM-based Noise-robust Pre-Training introduces the transfer patterns between any two trajectories into the model training process, enhancing the model's noise robustness. Finally, the overall ranking-aware regularization shifts the model's focus from a local to a global perspective, enabling it to capture the holistic ordering information among trajectories. Extensive experiments on three publicly available datasets show that TrajDiff consistently outperforms state-of-the-art baselines. In particular, it achieves an average HR@1 gain of 33.38% across all three evaluation metrics and datasets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach",
      "titleJa": "Clinically interpretable mortality prediction for icu patients with diabetes and atrial fibrillation: a 機械学習 approach",
      "link": "https://arxiv.org/abs/2506.15901",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15901v1 Announce Type: new \nAbstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation (AF) face elevated mortality in intensive care units (ICUs), yet models targeting this high-risk group remain limited.\n  Objective: To develop an interpretable machine learning (ML) model predicting 28-day mortality in ICU patients with concurrent DM and AF using early-phase clinical data.\n  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF was extracted from the MIMIC-IV database. Data preprocessing involved median/mode imputation, z-score normalization, and early temporal feature engineering. A two-step feature selection pipeline-univariate filtering (ANOVA F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable features. Seven ML models were trained with stratified 5-fold cross-validation and SMOTE oversampling. Interpretability was assessed via ablation and Accumulated Local Effects (ALE) analysis.\n  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95% CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS, age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects such as age-related risk acceleration and bilirubin thresholds.\n  Conclusion: This interpretable ML model offers accurate risk prediction and clinical insights for early ICU triage in patients with DM and AF.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics",
      "titleJa": "Vectoredits: a データセット and benchmark for instruction-based editing of vector graphics",
      "link": "https://arxiv.org/abs/2506.15903",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15903v1 Announce Type: new \nAbstract: We introduce a large-scale dataset for instruction-guided vector image editing, consisting of over 270,000 pairs of SVG images paired with natural language edit instructions. Our dataset enables training and evaluation of models that modify vector graphics based on textual commands. We describe the data collection process, including image pairing via CLIP similarity and instruction generation with vision-language models. Initial experiments with state-of-the-art large language models reveal that current methods struggle to produce accurate and valid edits, underscoring the challenge of this task. To foster research in natural language-driven vector graphic generation and editing, we make our resources created within this work publicly available.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI",
      "titleJa": "Pieceformer: similarity-driven knowledge transfer via scalable graph トランスフォーマー in vlsi",
      "link": "https://arxiv.org/abs/2506.15907",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15907v1 Announce Type: new \nAbstract: Accurate graph similarity is critical for knowledge transfer in VLSI design, enabling the reuse of prior solutions to reduce engineering effort and turnaround time. We propose Pieceformer, a scalable, self-supervised similarity assessment framework, equipped with a hybrid message-passing and graph transformer encoder. To address transformer scalability, we incorporate a linear transformer backbone and introduce a partitioned training pipeline for efficient memory and parallelism management. Evaluations on synthetic and real-world CircuitNet datasets show that Pieceformer reduces mean absolute error (MAE) by 24.9% over the baseline and is the only method to correctly cluster all real-world design groups. We further demonstrate the practical usage of our model through a case study on a partitioning task, achieving up to 89% runtime reduction. These results validate the framework's effectiveness for scalable, unbiased design reuse in modern VLSI systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Early Attentive Sparsification Accelerates Neural Speech Transcription",
      "titleJa": "Early attentive sparsification accelerates neural speech transcription",
      "link": "https://arxiv.org/abs/2506.15912",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15912v1 Announce Type: new \nAbstract: Transformer-based neural speech processing has achieved state-of-the-art performance. Since speech audio signals are known to be highly compressible, here we seek to accelerate neural speech transcription by time-domain signal sparsification early in the neural encoding stage, taking advantage of the interpretability of the self-attention mechanism in transformer audio encoders. With the Whisper family of models, we perform a systematic architecture search over the joint space of sparsification stage (a certain encoder layer) and compression ratio (sparsity). We found that the best resulting solutions under 1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration in English speech transcription tasks on Nvidia GPUs without any fine-tuning.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Competing Bandits in Matching Markets via Super Stability",
      "titleJa": "Competing bandits in matching markets via super stability",
      "link": "https://arxiv.org/abs/2506.15926",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15926v1 Announce Type: new \nAbstract: We study bandit learning in matching markets with two-sided reward uncertainty, extending prior research primarily focused on single-sided uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the standard GS algorithm in achieving true stable matchings under incomplete information. By employing the Extended GS algorithm, our centralized algorithm attains a logarithmic pessimal stable regret dependent on an instance-dependent admissible gap parameter. This algorithm is further adapted to a decentralized setting with a constant regret increase. Finally, we establish a novel centralized instance-dependent lower bound for binary stable regret, elucidating the roles of the admissible gap and super-stable matching in characterizing the complexity of stable matching with bandit feedback.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CORAL: Disentangling Latent Representations in Long-Tailed Diffusion",
      "titleJa": "Coral: disentangling latent representations in long-tailed diffusion",
      "link": "https://arxiv.org/abs/2506.15933",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15933v1 Announce Type: new \nAbstract: Diffusion models have achieved impressive performance in generating high-quality and diverse synthetic data. However, their success typically assumes a class-balanced training distribution. In real-world settings, multi-class data often follow a long-tailed distribution, where standard diffusion models struggle -- producing low-diversity and lower-quality samples for tail classes. While this degradation is well-documented, its underlying cause remains poorly understood. In this work, we investigate the behavior of diffusion models trained on long-tailed datasets and identify a key issue: the latent representations (from the bottleneck layer of the U-Net) for tail class subspaces exhibit significant overlap with those of head classes, leading to feature borrowing and poor generation quality. Importantly, we show that this is not merely due to limited data per class, but that the relative class imbalance significantly contributes to this phenomenon. To address this, we propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive latent alignment framework that leverages supervised contrastive losses to encourage well-separated latent class representations. Experiments demonstrate that CORAL significantly improves both the diversity and visual quality of samples generated for tail classes relative to state-of-the-art methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On the optimal regret of collaborative personalized linear bandits",
      "titleJa": "On the optimal regret of collaborative personalized linear bandits",
      "link": "https://arxiv.org/abs/2506.15943",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15943v1 Announce Type: new \nAbstract: Stochastic linear bandits are a fundamental model for sequential decision making, where an agent selects a vector-valued action and receives a noisy reward with expected value given by an unknown linear function. Although well studied in the single-agent setting, many real-world scenarios involve multiple agents solving heterogeneous bandit problems, each with a different unknown parameter. Applying single agent algorithms independently ignores cross-agent similarity and learning opportunities. This paper investigates the optimal regret achievable in collaborative personalized linear bandits. We provide an information-theoretic lower bound that characterizes how the number of agents, the interaction rounds, and the degree of heterogeneity jointly affect regret. We then propose a new two-stage collaborative algorithm that achieves the optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian framework and introduces a novel information-theoretic technique for bounding regret. Our results offer a complete characterization of when and how collaboration helps with a optimal regret bound $\\tilde{O}(d\\sqrt{mn})$, $\\tilde{O}(dm^{1-\\gamma}\\sqrt{n})$, $\\tilde{O}(dm\\sqrt{n})$ for the number of rounds $n$ in the range of $(0, \\frac{d}{m \\sigma^2})$, $[\\frac{d}{m^{2\\gamma} \\sigma^2}, \\frac{d}{\\sigma^2}]$ and $(\\frac{d}{\\sigma^2}, \\infty)$ respectively, where $\\sigma$ measures the level of heterogeneity, $m$ is the number of agents, and $\\gamma\\in[0, 1/2]$ is an absolute constant. In contrast, agents without collaboration achieve a regret bound $O(dm\\sqrt{n})$ at best.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks",
      "titleJa": "One period to rule them all: identifying critical learning periods in deep networks",
      "link": "https://arxiv.org/abs/2506.15954",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15954v1 Announce Type: new \nAbstract: Critical Learning Periods comprehend an important phenomenon involving deep learning, where early epochs play a decisive role in the success of many training recipes, such as data augmentation. Existing works confirm the existence of this phenomenon and provide useful insights. However, the literature lacks efforts to precisely identify when critical periods occur. In this work, we fill this gap by introducing a systematic approach for identifying critical periods during the training of deep neural networks, focusing on eliminating computationally intensive regularization techniques and effectively applying mechanisms for reducing computational costs, such as data pruning. Our method leverages generalization prediction mechanisms to pinpoint critical phases where training recipes yield maximum benefits to the predictive ability of models. By halting resource-intensive recipes beyond these periods, we significantly accelerate the learning phase and achieve reductions in training time, energy consumption, and CO$_2$ emissions. Experiments on standard architectures and benchmarks confirm the effectiveness of our method. Specifically, we achieve significant milestones by reducing the training time of popular architectures by up to 59.67%, leading to a 59.47% decrease in CO$_2$ emissions and a 60% reduction in financial costs, without compromising performance. Our work enhances understanding of training dynamics and paves the way for more sustainable and efficient deep learning practices, particularly in resource-constrained environments. In the era of the race for foundation models, we believe our method emerges as a valuable framework. The repository is available at https://github.com/baunilhamarga/critical-periods",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Bridging Brain with Foundation Models through Self-Supervised Learning",
      "titleJa": "Bridging brain with foundation models through self-supervised learning",
      "link": "https://arxiv.org/abs/2506.16009",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16009v1 Announce Type: new \nAbstract: Foundation models (FMs), powered by self-supervised learning (SSL), have redefined the capabilities of artificial intelligence, demonstrating exceptional performance in domains like natural language processing and computer vision. These advances present a transformative opportunity for brain signal analysis. Unlike traditional supervised learning, which is limited by the scarcity of labeled neural data, SSL offers a promising solution by enabling models to learn meaningful representations from unlabeled data. This is particularly valuable in addressing the unique challenges of brain signals, including high noise levels, inter-subject variability, and low signal-to-noise ratios. This survey systematically reviews the emerging field of bridging brain signals with foundation models through the innovative application of SSL. It explores key SSL techniques, the development of brain-specific foundation models, their adaptation to downstream tasks, and the integration of brain signals with other modalities in multimodal SSL frameworks. The review also covers commonly used evaluation metrics and benchmark datasets that support comparative analysis. Finally, it highlights key challenges and outlines future research directions. This work aims to provide researchers with a structured understanding of this rapidly evolving field and a roadmap for developing generalizable brain foundation models powered by self-supervision.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience",
      "titleJa": "From data to decision: data-centric infrastructure for reproducible ml in collaborative escience",
      "link": "https://arxiv.org/abs/2506.16051",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16051v1 Announce Type: new \nAbstract: Reproducibility remains a central challenge in machine learning (ML), especially in collaborative eScience projects where teams iterate over data, features, and models. Current ML workflows are often dynamic yet fragmented, relying on informal data sharing, ad hoc scripts, and loosely connected tools. This fragmentation impedes transparency, reproducibility, and the adaptability of experiments over time. This paper introduces a data-centric framework for lifecycle-aware reproducibility, centered around six structured artifacts: Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These artifacts formalize the relationships between data, code, and decisions, enabling ML experiments to be versioned, interpretable, and traceable over time. The approach is demonstrated through a clinical ML use case of glaucoma detection, illustrating how the system supports iterative exploration, improves reproducibility, and preserves the provenance of collaborative decisions across the ML lifecycle.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators",
      "titleJa": "Floating-point neural networks are provably robust universal approximators",
      "link": "https://arxiv.org/abs/2506.16065",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16065v1 Announce Type: new \nAbstract: The classical universal approximation (UA) theorem for neural networks establishes mild conditions under which a feedforward neural network can approximate a continuous function $f$ with arbitrary accuracy. A recent result shows that neural networks also enjoy a more general interval universal approximation (IUA) theorem, in the sense that the abstract interpretation semantics of the network using the interval domain can approximate the direct image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with arbitrary accuracy. These theorems, however, rest on the unrealistic assumption that the neural network computes over infinitely precise real numbers, whereas their software implementations in practice compute over finite-precision floating-point numbers. An open question is whether the IUA theorem still holds in the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural networks that proves their remarkable ability to perfectly capture the direct image map of any rounded target function $f$, showing no limits exist on their expressiveness. Our IUA theorem in the floating-point setting exhibits material differences from the real-valued setting, which reflects the fundamental distinctions between these two computational models. This theorem also implies surprising corollaries, which include (i) the existence of provably robust floating-point neural networks; and (ii) the computational completeness of the class of straight-line programs that use only floating-point additions and multiplications for the class of all floating-point programs that halt.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems",
      "titleJa": "A lightweight rl-driven deep unfolding network for robust wmmse precoding in massive mu-mimo-ofdm systems",
      "link": "https://arxiv.org/abs/2506.16072",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16072v1 Announce Type: new \nAbstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for its near-optimal weighted sum rate performance. However, its practical deployment in massive multi-user (MU) multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) systems is hindered by the assumption of perfect channel state information (CSI) and high computational complexity. To address these issues, we first develop a wideband stochastic WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net), where each SWMMSE iteration is mapped to a network layer. Specifically, its DU module integrates approximation techniques and leverages beam-domain sparsity as well as frequency-domain subcarrier correlation, significantly accelerating convergence and reducing computational overhead. Furthermore, the RL module adaptively adjusts the network depth and generates compensation matrices to mitigate approximation errors. Simulation results under imperfect CSI demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance while offering superior computational and convergence efficiency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification",
      "titleJa": "Mitigating over-squashing in graph neural networks by spectrum-preserving sparsification",
      "link": "https://arxiv.org/abs/2506.16110",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16110v1 Announce Type: new \nAbstract: The message-passing paradigm of Graph Neural Networks often struggles with exchanging information across distant nodes typically due to structural bottlenecks in certain graph regions, a limitation known as \\textit{over-squashing}. To reduce such bottlenecks, \\textit{graph rewiring}, which modifies graph topology, has been widely used. However, existing graph rewiring techniques often overlook the need to preserve critical properties of the original graph, e.g., \\textit{spectral properties}. Moreover, many approaches rely on increasing edge count to improve connectivity, which introduces significant computational overhead and exacerbates the risk of over-smoothing. In this paper, we propose a novel graph rewiring method that leverages \\textit{spectrum-preserving} graph \\textit{sparsification}, for mitigating over-squashing. Our method generates graphs with enhanced connectivity while maintaining sparsity and largely preserving the original graph spectrum, effectively balancing structural bottleneck reduction and graph property preservation. Experimental results validate the effectiveness of our approach, demonstrating its superiority over strong baseline methods in classification accuracy and retention of the Laplacian spectrum.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song",
      "titleJa": "Hallucination level of 人工知能 whisperer: case speech recognizing pantterinousut rap song",
      "link": "https://arxiv.org/abs/2506.16174",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16174v1 Announce Type: new \nAbstract: All languages are peculiar. Some of them are considered more challenging to understand than others. The Finnish Language is known to be a complex language. Also, when languages are used by artists, the pronunciation and meaning might be more tricky to understand. Therefore, we are putting AI to a fun, yet challenging trial: translating a Finnish rap song to text. We will compare the Faster Whisperer algorithm and YouTube's internal speech-to-text functionality. The reference truth will be Finnish rap lyrics, which the main author's little brother, Mc Timo, has written. Transcribing the lyrics will be challenging because the artist raps over synth music player by Syntikka Janne. The hallucination level and mishearing of AI speech-to-text extractions will be measured by comparing errors made against the original Finnish lyrics. The error function is informal but still works for our case.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data",
      "titleJa": "Think global, act local: bayesian causal discovery with language models in sequential data",
      "link": "https://arxiv.org/abs/2506.16234",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16234v1 Announce Type: new \nAbstract: Causal discovery from observational data typically assumes full access to data and availability of domain experts. In practice, data often arrive in batches, and expert knowledge is scarce. Language Models (LMs) offer a surrogate but come with their own issues-hallucinations, inconsistencies, and bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid Bayesian framework that bridges these gaps by adaptively integrating sequential batch data with LM-derived noisy, expert knowledge while accounting for both data-induced and LM-induced biases. Our proposed representation shift from Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates ambiguities within a coherent Bayesian framework, allowing grounding the global LM knowledge in local observational data. To guide LM interaction, we use a sequential optimization scheme that adaptively queries the most informative edges. Across varied datasets, BLANCE outperforms prior work in structural accuracy and extends to Bayesian parameter estimation, showing robustness to LM noise.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimizing Multilingual Text-To-Speech with Accents & Emotions",
      "titleJa": "Optimizing multilingual text-to-speech with accents & emotions",
      "link": "https://arxiv.org/abs/2506.16310",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16310v1 Announce Type: new \nAbstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in monolingual environments, synthesizing speech with correct multilingual accents (especially for Indic languages) and context-relevant emotions still poses difficulty owing to cultural nuance discrepancies in current frameworks. This paper introduces a new TTS architecture integrating accent along with preserving transliteration with multi-scale emotion modelling, in particularly tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS model by integrating A language-specific phoneme alignment hybrid encoder-decoder architecture, and culture-sensitive emotion embedding layers trained on native speaker corpora, as well as incorporating a dynamic accent code switching with residual vector quantization. Quantitative tests demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system is that it can mix code in real time - generating statements such as \"Namaste, let's talk about \" with uninterrupted accent shifts while preserving emotional consistency. Subjective evaluation with 200 users reported a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than existing multilingual systems (p<0.01). This research makes cross-lingual synthesis more feasible by showcasing scalable accent-emotion disentanglement, with direct application in South Asian EdTech and accessibility software.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Signatures to help interpretability of anomalies",
      "titleJa": "Signatures to help interpretability of anomalies",
      "link": "https://arxiv.org/abs/2506.16314",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16314v1 Announce Type: new \nAbstract: Machine learning is often viewed as a black box when it comes to understanding its output, be it a decision or a score. Automatic anomaly detection is no exception to this rule, and quite often the astronomer is left to independently analyze the data in order to understand why a given event is tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is to help the interpretability of anomalies by highlighting which features contributed to the decision.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Bayesian Optimization over Bounded Domains with the Beta Product Kernel",
      "titleJa": "Bayesian optimization over bounded domains with the beta product kernel",
      "link": "https://arxiv.org/abs/2506.16316",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16316v1 Announce Type: new \nAbstract: Bayesian optimization with Gaussian processes (GP) is commonly used to optimize black-box functions. The Mat\\'ern and the Radial Basis Function (RBF) covariance functions are used frequently, but they do not make any assumptions about the domain of the function, which may limit their applicability in bounded domains. To address the limitation, we introduce the Beta kernel, a non-stationary kernel induced by a product of Beta distribution density functions. Such a formulation allows our kernel to naturally model functions on bounded domains. We present statistical evidence supporting the hypothesis that the kernel exhibits an exponential eigendecay rate, based on empirical analyses of its spectral properties across different settings. Our experimental results demonstrate the robustness of the Beta kernel in modeling functions with optima located near the faces or vertices of the unit hypercube. The experiments show that our kernel consistently outperforms a wide range of kernels, including the well-known Mat\\'ern and RBF, in different problems, including synthetic function optimization and the compression of vision and language models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data",
      "titleJa": "Classification of cattle behavior and detection of heat (estrus) using sensor data",
      "link": "https://arxiv.org/abs/2506.16380",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16380v1 Announce Type: new \nAbstract: This paper presents a novel system for monitoring cattle behavior and detecting estrus (heat) periods using sensor data and machine learning. We designed and deployed a low-cost Bluetooth-based neck collar equipped with accelerometer and gyroscope sensors to capture real-time behavioral data from real cows, which was synced to the cloud. A labeled dataset was created using synchronized CCTV footage to annotate behaviors such as feeding, rumination, lying, and others. We evaluated multiple machine learning models -- Support Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks (CNN) -- for behavior classification. Additionally, we implemented a Long Short-Term Memory (LSTM) model for estrus detection using behavioral patterns and anomaly detection. Our system achieved over 93% behavior classification accuracy and 96% estrus detection accuracy on a limited test set. The approach offers a scalable and accessible solution for precision livestock monitoring, especially in resource-constrained environments.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification",
      "titleJa": "State-space kolmogorov arnold networks for interpretable nonlinear system identification",
      "link": "https://arxiv.org/abs/2506.16392",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16392v1 Announce Type: new \nAbstract: While accurate, black-box system identification models lack interpretability of the underlying system dynamics. This paper proposes State-Space Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating Kolmogorov-Arnold Networks within a state-space framework. The proposed model is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein benchmarks. Results show that SS-KAN provides enhanced interpretability due to sparsity-promoting regularization and the direct visualization of its learned univariate functions, which reveal system nonlinearities at the cost of accuracy when compared to state-of-the-art black-box models, highlighting SS-KAN as a promising approach for interpretable nonlinear system identification, balancing accuracy and interpretability of nonlinear system dynamics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Generating Directed Graphs with Dual Attention and Asymmetric Encoding",
      "titleJa": "Generating directed graphs with dual attention and asymmetric encoding",
      "link": "https://arxiv.org/abs/2506.16404",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16404v1 Announce Type: new \nAbstract: Directed graphs naturally model systems with asymmetric, ordered relationships, essential to applications in biology, transportation, social networks, and visual understanding. Generating such graphs enables tasks such as simulation, data augmentation and novel instance discovery; however, directed graph generation remains underexplored. We identify two key factors limiting progress in this direction: first, modeling edge directionality introduces a substantially larger dependency space, making the underlying distribution harder to learn; second, the absence of standardized benchmarks hinders rigorous evaluation. Addressing the former requires more expressive models that are sensitive to directional topologies. We propose Directo, the first generative model for directed graphs built upon the discrete flow matching framework. Our approach combines: (i) principled positional encodings tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism capturing both incoming and outgoing dependencies, and (iii) a robust, discrete generative framework. To support evaluation, we introduce a benchmark suite covering synthetic and real-world datasets. It shows that our method performs strongly across diverse settings and even competes with specialized models for particular classes, such as directed acyclic graphs. Our results highlight the effectiveness and generality of our approach, establishing a solid foundation for future research in directed graph generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems",
      "titleJa": "Eformer: an effective edge-based トランスフォーマー for vehicle routing problems",
      "link": "https://arxiv.org/abs/2506.16428",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16428v1 Announce Type: new \nAbstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely on node coordinates as input, which may be less effective in practical scenarios where real cost metrics-such as edge-based distances-are more relevant. To address this limitation, we introduce EFormer, an Edge-based Transformer model that uses edge as the sole input for VRPs. Our approach employs a precoder module with a mixed-score attention mechanism to convert edge information into temporary node embeddings. We also present a parallel encoding strategy characterized by a graph encoder and a node encoder, each responsible for processing graph and node embeddings in distinct feature spaces, respectively. This design yields a more comprehensive representation of the global relationships among edges. In the decoding phase, parallel context embedding and multi-query integration are used to compute separate attention mechanisms over the two encoded embeddings, facilitating efficient path construction. We train EFormer using reinforcement learning in an autoregressive manner. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer outperforms established baselines on synthetic datasets, including large-scale and diverse distributions. Moreover, EFormer demonstrates strong generalization on real-world instances from TSPLib and CVRPLib. These findings confirm the effectiveness of EFormer's core design in solving VRPs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning",
      "titleJa": "Black-box プライバシー attacks on shared representations in multitask learning",
      "link": "https://arxiv.org/abs/2506.16460",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16460v1 Announce Type: new \nAbstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages similarities among multiple learning tasks, each with insufficient samples to train a standalone model, to solve them simultaneously while minimizing data sharing across users and organizations. MTL typically accomplishes this goal by learning a shared representation that captures common structure among the tasks by embedding data from all tasks into a common feature space. Despite being designed to be the smallest unit of shared information necessary to effectively learn patterns across multiple tasks, these shared representations can inadvertently leak sensitive information about the particular tasks they were trained on.\n  In this work, we investigate what information is revealed by the shared representations through the lens of inference attacks. Towards this, we propose a novel, black-box task-inference threat model where the adversary, given the embedding vectors produced by querying the shared representation on samples from a particular task, aims to determine whether that task was present when training the shared representation. We develop efficient, purely black-box attacks on machine learning models that exploit the dependencies between embeddings from the same task without requiring shadow models or labeled reference data. We evaluate our attacks across vision and language domains for multiple use cases of MTL and demonstrate that even with access only to fresh task samples rather than training data, a black-box adversary can successfully infer a task's inclusion in training. To complement our experiments, we provide theoretical analysis of a simplified learning setting and show a strict separation between adversaries with training samples and fresh samples from the target task's distribution.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias",
      "titleJa": "Manifold learning for personalized and label-free detection of cardiac arrhythmias",
      "link": "https://arxiv.org/abs/2506.16494",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16494v1 Announce Type: new \nAbstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart activity and are well-established tools for detecting and monitoring cardiovascular disease. However, manual ECG analysis can be time-consuming and prone to errors. Machine learning has emerged as a promising approach for automated heartbeat recognition and classification, but substantial variations in ECG signals make it challenging to develop generalizable models. ECG signals can vary widely across individuals and leads, while datasets often follow different labeling standards and may be biased, all of which greatly hinder supervised methods. Conventional unsupervised methods, e.g. principal component analysis, prioritize large (and often obvious) variances in the data and typically overlook subtle yet clinically relevant patterns. If labels are missing and/or variations are significant but small, both approaches fail. Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate these issues and identify medically relevant features in ECG signals, with no need for training or prior information. Using the MLII and V1 leads of the MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor embedding and uniform manifold approximation and projection can discriminate individual recordings in mixed populations with >= 90% accuracy and distinguish different arrhythmias in individual patients with a median accuracy of 98.96% and a median F1-score of 91.02%. The results show that NLDR holds much promise for cardiac monitoring, including the limiting cases of single-lead ECG and the current 12-lead standard of care, and for personalized health care beyond cardiology.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Free Probabilistic Framework for Analyzing the Transformer-based Language Models",
      "titleJa": "A free probabilistic framework for analyzing the トランスフォーマー-based language models",
      "link": "https://arxiv.org/abs/2506.16550",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16550v1 Announce Type: new \nAbstract: We outline an operator-theoretic framework for analyzing transformer-based language models using the tools of free probability theory. By representing token embeddings and attention mechanisms as self-adjoint operators in a racial probability space, we reinterpret attention as a non-commutative convolution and view the layer-wise propagation of representations as an evolution governed by free additive convolution. This formalism reveals a spectral dynamical system underpinning deep transformer stacks and offers insight into their inductive biases, generalization behavior, and entropy dynamics. We derive a generalization bound based on free entropy and demonstrate that the spectral trace of transformer layers evolves predictably with depth. Our approach bridges neural architecture with non-commutative harmonic analysis, enabling principled analysis of information flow and structural complexity in large language models",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics",
      "titleJa": "Slepnet: spectral subgraph representation learning for neural dynamics",
      "link": "https://arxiv.org/abs/2506.16602",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16602v1 Announce Type: new \nAbstract: Graph neural networks have been useful in machine learning on graph-structured data, particularly for node classification and some types of graph classification tasks. However, they have had limited use in representing patterning of signals over graphs. Patterning of signals over graphs and in subgraphs carries important information in many domains including neuroscience. Neural signals are spatiotemporally patterned, high dimensional and difficult to decode. Graph signal processing and associated GCN models utilize the graph Fourier transform and are unable to efficiently represent spatially or spectrally localized signal patterning on graphs. Wavelet transforms have shown promise here, but offer non-canonical representations and cannot be tightly confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian harmonics optimally concentrate signal energy on specifically relevant subgraphs that are automatically learned with a mask. Thus, they can produce canonical and highly resolved representations of neural activity, focusing energy of harmonics on areas of the brain which are activated. We evaluated SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and two traffic dynamics datasets, comparing its performance against conventional GNNs and graph signal processing constructs. SlepNet outperforms the baselines in all datasets. Moreover, the extracted representations of signal patterns from SlepNet offers more resolution in distinguishing between similar patterns, and thus represent brain signaling transients as informative trajectories. Here we have shown that these extracted trajectory representations can be used for other downstream untrained tasks. Thus we establish that SlepNet is useful both for prediction and representation learning in spatiotemporal data.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data",
      "titleJa": "Learning causally predictable outcomes from psychiatric longitudinal data",
      "link": "https://arxiv.org/abs/2506.16629",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16629v1 Announce Type: new \nAbstract: Causal inference in longitudinal biomedical data remains a central challenge, especially in psychiatry, where symptom heterogeneity and latent confounding frequently undermine classical estimators. Most existing methods for treatment effect estimation presuppose a fixed outcome variable and address confounding through observed covariate adjustment. However, the assumption of unconfoundedness may not hold for a fixed outcome in practice. To address this foundational limitation, we directly optimize the outcome definition to maximize causal identifiability. Our DEBIAS (Durable Effects with Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative, clinically interpretable weights for outcome aggregation, maximizing durable treatment effects and empirically minimizing both observed and latent confounding by leveraging the time-limited direct effects of prior treatments in psychiatric longitudinal data. The algorithm also furnishes an empirically verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms state-of-the-art methods in recovering causal effects for clinically interpretable composite outcomes across comprehensive experiments in depression and schizophrenia.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Distributional-Lifting Theorem for PAC Learning",
      "titleJa": "A distributional-lifting theorem for pac learning",
      "link": "https://arxiv.org/abs/2506.16651",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16651v1 Announce Type: new \nAbstract: The apparent difficulty of efficient distribution-free PAC learning has led to a large body of work on distribution-specific learning. Distributional assumptions facilitate the design of efficient algorithms but also limit their reach and relevance. Towards addressing this, we prove a distributional-lifting theorem: This upgrades a learner that succeeds with respect to a limited distribution family $\\mathcal{D}$ to one that succeeds with respect to any distribution $D^\\star$, with an efficiency overhead that scales with the complexity of expressing $D^\\star$ as a mixture of distributions in $\\mathcal{D}$.\n  Recent work of Blanc, Lange, Malik, and Tan considered the special case of lifting uniform-distribution learners and designed a lifter that uses a conditional sample oracle for $D^\\star$, a strong form of access not afforded by the standard PAC model. Their approach, which draws on ideas from semi-supervised learning, first learns $D^\\star$ and then uses this information to lift.\n  We show that their approach is information-theoretically intractable with access only to random examples, thereby giving formal justification for their use of the conditional sample oracle. We then take a different approach that sidesteps the need to learn $D^\\star$, yielding a lifter that works in the standard PAC model and enjoys additional advantages: it works for all base distribution families, preserves the noise tolerance of learners, has better sample complexity, and is simpler.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Mesh-Informed Neural Operator : A Transformer Generative Approach",
      "titleJa": "Mesh-informed neural operator : a トランスフォーマー generative approach",
      "link": "https://arxiv.org/abs/2506.16656",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16656v1 Announce Type: new \nAbstract: Generative models in function spaces, situated at the intersection of generative modeling and operator learning, are attracting increasing attention due to their immense potential in diverse scientific and engineering applications. While functional generative models are theoretically domain- and discretization-agnostic, current implementations heavily rely on the Fourier Neural Operator (FNO), limiting their applicability to regular grids and rectangular domains. To overcome these critical limitations, we introduce the Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and cross-attention mechanisms, MINO offers a principled, domain- and discretization-agnostic backbone for generative modeling in function spaces. This advancement significantly expands the scope of such models to more diverse applications in generative, inverse, and regression tasks. Furthermore, MINO provides a unified perspective on integrating neural operators with general advanced deep learning architectures. Finally, we introduce a suite of standardized evaluation metrics that enable objective comparison of functional generative models, addressing another critical gap in the field.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Private Training & Data Generation by Clustering Embeddings",
      "titleJa": "Private 学習 & data generation by clustering embeddings",
      "link": "https://arxiv.org/abs/2506.16661",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16661v1 Announce Type: new \nAbstract: Deep neural networks often use large, high-quality datasets to achieve high performance on many machine learning tasks. When training involves potentially sensitive data, this process can raise privacy concerns, as large models have been shown to unintentionally memorize and reveal sensitive information, including reconstructing entire training samples. Differential privacy (DP) provides a robust framework for protecting individual data and in particular, a new approach to privately training deep neural networks is to approximate the input dataset with a privately generated synthetic dataset, before any subsequent training algorithm. We introduce a novel principled method for DP synthetic image embedding generation, based on fitting a Gaussian Mixture Model (GMM) in an appropriate embedding space using DP clustering. Our method provably learns a GMM under separation conditions. Empirically, a simple two-layer neural network trained on synthetically generated embeddings achieves state-of-the-art (SOTA) classification accuracy on standard benchmark datasets. Additionally, we demonstrate that our method can generate realistic synthetic images that achieve downstream classification accuracy comparable to SOTA methods. Our method is quite general, as the encoder and decoder modules can be freely substituted to suit different tasks. It is also highly scalable, consisting only of subroutines that scale linearly with the number of samples and/or can be implemented efficiently in distributed systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "SIDE: Semantic ID Embedding for effective learning from sequences",
      "titleJa": "Side: semantic id embedding for effective learning from sequences",
      "link": "https://arxiv.org/abs/2506.16698",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16698v1 Announce Type: new \nAbstract: Sequence-based recommendations models are driving the state-of-the-art for industrial ad-recommendation systems. Such systems typically deal with user histories or sequence lengths ranging in the order of O(10^3) to O(10^4) events. While adding embeddings at this scale is manageable in pre-trained models, incorporating them into real-time prediction models is challenging due to both storage and inference costs. To address this scaling challenge, we propose a novel approach that leverages vector quantization (VQ) to inject a compact Semantic ID (SID) as input to the recommendation models instead of a collection of embeddings. Our method builds on recent works of SIDs by introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ fusion that fuses multiple content embeddings and categorical predictions into a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding conversion technique, called SIDE, that is validated with two content embedding collections, thereby eliminating the need for a large parameterized lookup table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which generalizes and enhances residual quantization techniques. The proposed enhancements when applied to a large-scale industrial ads-recommendation system achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in data footprint compared to traditional SID methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension",
      "titleJa": "How many domains suffice for domain generalization? a tight characterization via the domain shattering dimension",
      "link": "https://arxiv.org/abs/2506.16704",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16704v1 Announce Type: new \nAbstract: We study a fundamental question of domain generalization: given a family of domains (i.e., data distributions), how many randomly sampled domains do we need to collect data from in order to learn a model that performs reasonably well on every seen and unseen domain in the family? We model this problem in the PAC framework and introduce a new combinatorial measure, which we call the domain shattering dimension. We show that this dimension characterizes the domain sample complexity. Furthermore, we establish a tight quantitative relationship between the domain shattering dimension and the classic VC dimension, demonstrating that every hypothesis class that is learnable in the standard PAC setting is also learnable in our setting.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimism Without Regularization: Constant Regret in Zero-Sum Games",
      "titleJa": "Optimism without regularization: constant regret in zero-sum games",
      "link": "https://arxiv.org/abs/2506.16736",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16736v1 Announce Type: new \nAbstract: This paper studies the optimistic variant of Fictitious Play for learning in two-player zero-sum games. While it is known that Optimistic FTRL -- a regularized algorithm with a bounded stepsize parameter -- obtains constant regret in this setting, we show for the first time that similar, optimal rates are also achievable without regularization: we prove for two-strategy games that Optimistic Fictitious Play (using any tiebreaking rule) obtains only constant regret, providing surprising new evidence on the ability of non-no-regret algorithms for fast learning in games. Our proof technique leverages a geometric view of Optimistic Fictitious Play in the dual space of payoff vectors, where we show a certain energy function of the iterates remains bounded over time. Additionally, we also prove a regret lower bound of $\\Omega(\\sqrt{T})$ for Alternating Fictitious Play. In the unregularized regime, this separates the ability of optimism and alternation in achieving $o(\\sqrt{T})$ regret.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps",
      "titleJa": "Revisiting lora through the lens of parameter redundancy: spectral encoding helps",
      "link": "https://arxiv.org/abs/2506.16787",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16787v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models. Despite its successes, the substantial parameter redundancy, which limits the capacity and efficiency of LoRA, has been recognized as a bottleneck. In this work, we systematically investigate the impact of redundancy in fine-tuning LoRA and reveal that reducing density redundancy does not degrade expressiveness. Based on this insight, we introduce \\underline{S}pectral-\\underline{e}ncoding \\underline{L}ow-\\underline{R}ank \\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of spectral bases to re-parameterize LoRA from a sparse spectral subspace. Designed with simplicity, SeLoRA enables seamless integration with various LoRA variants for performance boosting, serving as a scalable plug-and-play framework. Extensive experiments substantiate that SeLoRA achieves greater efficiency with fewer parameters, delivering superior performance enhancements over strong baselines on various downstream tasks, including commonsense reasoning, math reasoning, and code generation.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective",
      "titleJa": "Exploring and improving initialization for deep graph neural networks: a signal propagation perspective",
      "link": "https://arxiv.org/abs/2506.16790",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16790v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the network depth increases. This paper addresses this issue by introducing initialization methods that enhance signal propagation (SP) within GNNs. We propose three key metrics for effective SP in GNNs: forward propagation, backward propagation, and graph embedding variation (GEV). While the first two metrics derive from classical SP theory, the third is specifically designed for GNNs. We theoretically demonstrate that a broad range of commonly used initialization methods for GNNs, which exhibit performance degradation with increasing depth, fail to control these three metrics simultaneously. To deal with this limitation, a direct exploitation of the SP analysis--searching for weight initialization variances that optimize the three metrics--is shown to significantly enhance the SP in deep GCNs. This approach is called Signal Propagation on Graph-guided Initialization (SPoGInit). Our experiments demonstrate that SPoGInit outperforms commonly used initialization methods on various tasks and architectures. Notably, SPoGInit enables performance improvements as GNNs deepen, which represents a significant advancement in addressing depth-related challenges and highlights the validity and effectiveness of the SP analysis framework.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Robust Group Anomaly Detection for Quasi-Periodic Network Time Series",
      "titleJa": "Robust group anomaly detection for quasi-periodic network time series",
      "link": "https://arxiv.org/abs/2506.16815",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16815v1 Announce Type: new \nAbstract: Many real-world multivariate time series are collected from a network of physical objects embedded with software, electronics, and sensors. The quasi-periodic signals generated by these objects often follow a similar repetitive and periodic pattern, but have variations in the period, and come in different lengths caused by timing (synchronization) errors. Given a multitude of such quasi-periodic time series, can we build machine learning models to identify those time series that behave differently from the majority of the observations? In addition, can the models help human experts to understand how the decision was made? We propose a sequence to Gaussian Mixture Model (seq2GMM) framework. The overarching goal of this framework is to identify unusual and interesting time series within a network time series database. We further develop a surrogate-based optimization algorithm that can efficiently train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a plurality of public benchmark datasets, outperforming state-of-the-art anomaly detection techniques by a significant margin. We also theoretically analyze the convergence property of the proposed training algorithm and provide numerical results to substantiate our theoretical claims.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "FedFitTech: A Baseline in Federated Learning for Fitness Tracking",
      "titleJa": "Fedfittech: a baseline in federated learning for fitness tracking",
      "link": "https://arxiv.org/abs/2506.16840",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16840v1 Announce Type: new \nAbstract: Rapid evolution of sensors and resource-efficient machine learning models have spurred the widespread adoption of wearable fitness tracking devices. Equipped with inertial sensors, such devices can continuously capture physical movements for fitness technology (FitTech), enabling applications from sports optimization to preventive healthcare. Traditional centralized learning approaches to detect fitness activities struggle with privacy concerns, regulatory constraints, and communication inefficiencies. In contrast, Federated Learning (FL) enables a decentralized model training by communicating model updates rather than private wearable sensor data. Applying FL to FitTech presents unique challenges, such as data imbalance, lack of labelled data, heterogeneous user activity patterns, and trade-offs between personalization and generalization. To simplify research on FitTech in FL, we present the FedFitTech baseline, under the Flower framework, which is publicly available and widely used by both industry and academic researchers. Additionally, to illustrate its usage, this paper presents a case study that implements a system based on the FedFitTech baseline, incorporating a client-side early stopping strategy and comparing the results. For instance, this system allows wearable devices to optimize the trade-off between capturing common fitness activity patterns and preserving individuals' nuances, thereby enhancing both the scalability and efficiency of privacy-aware fitness tracking applications. Results show that this reduces overall redundant communications by 13 percent, while maintaining the overall recognition performance at a negligible recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation for a wide range of new research and development opportunities in FitTech, and it is available as open-source at: https://github.com/adap/flower/tree/main/baselines/fedfittech",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Soft decision trees for survival analysis",
      "titleJa": "Soft decision trees for survival analysis",
      "link": "https://arxiv.org/abs/2506.16846",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16846v1 Announce Type: new \nAbstract: Decision trees are popular in survival analysis for their interpretability and ability to model complex relationships. Survival trees, which predict the timing of singular events using censored historical data, are typically built through heuristic approaches. Recently, there has been growing interest in globally optimized trees, where the overall tree is trained by minimizing the error function over all its parameters. We propose a new soft survival tree model (SST), with a soft splitting rule at each branch node, trained via a nonlinear optimization formulation amenable to decomposition. Since SSTs provide for every input vector a specific survival function associated to a single leaf node, they satisfy the conditional computation property and inherit the related benefits. SST and the training formulation combine flexibility with interpretability: any smooth survival function (parametric, semiparametric, or nonparametric) estimated through maximum likelihood can be used, and each leaf node of an SST yields a cluster of distinct survival functions which are associated to the data points routed to it. Numerical experiments on 15 well-known datasets show that SSTs, with parametric and spline-based semiparametric survival functions, trained using an adaptation of the node-based decomposition algorithm proposed by Consolo et al. (2024) for soft regression trees, outperform three benchmark survival trees in terms of four widely-used discrimination and calibration measures. SSTs can also be extended to consider group fairness.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning",
      "titleJa": "Anomaly detection in event-triggered traffic time series via similarity learning",
      "link": "https://arxiv.org/abs/2506.16855",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16855v1 Announce Type: new \nAbstract: Time series analysis has achieved great success in cyber security such as intrusion detection and device identification. Learning similarities among multiple time series is a crucial problem since it serves as the foundation for downstream analysis. Due to the complex temporal dynamics of the event-triggered time series, it often remains unclear which similarity metric is appropriate for security-related tasks, such as anomaly detection and clustering. The overarching goal of this paper is to develop an unsupervised learning framework that is capable of learning similarities among a set of event-triggered time series. From the machine learning vantage point, the proposed framework harnesses the power of both hierarchical multi-resolution sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively learn the low-dimensional representations from the time series. Finally, the obtained similarity measure can be easily visualized for the explanation. The proposed framework aspires to offer a stepping stone that gives rise to a systematic approach to model and learn similarities among a multitude of event-triggered time series. Through extensive qualitative and quantitative experiments, it is revealed that the proposed method outperforms state-of-the-art methods considerably.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimal Depth of Neural Networks",
      "titleJa": "Optimal depth of neural networks",
      "link": "https://arxiv.org/abs/2506.16862",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16862v1 Announce Type: new \nAbstract: Determining the optimal depth of a neural network is a fundamental yet challenging problem, typically resolved through resource-intensive experimentation. This paper introduces a formal theoretical framework to address this question by recasting the forward pass of a deep network, specifically a Residual Network (ResNet), as an optimal stopping problem. We model the layer-by-layer evolution of hidden representations as a sequential decision process where, at each layer, a choice is made between halting computation to make a prediction or continuing to a deeper layer for a potentially more refined representation. This formulation captures the intrinsic trade-off between accuracy and computational cost. Our primary theoretical contribution is a proof that, under a plausible condition of diminishing returns on the residual functions, the expected optimal stopping depth is provably finite, even in an infinite-horizon setting. We leverage this insight to propose a novel and practical regularization term, $\\mathcal{L}_{\\rm depth}$, that encourages the network to learn representations amenable to efficient, early exiting. We demonstrate the generality of our framework by extending it to the Transformer architecture and exploring its connection to continuous-depth models via free-boundary problems. Empirical validation on ImageNet confirms that our regularizer successfully induces the theoretically predicted behavior, leading to significant gains in computational efficiency without compromising, and in some cases improving, final model accuracy.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators",
      "titleJa": "Robust 強化学習 for discrete compositional generation via general soft operators",
      "link": "https://arxiv.org/abs/2506.17007",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17007v1 Announce Type: new \nAbstract: A major bottleneck in scientific discovery involves narrowing a large combinatorial set of objects, such as proteins or molecules, to a small set of promising candidates. While this process largely relies on expert knowledge, recent methods leverage reinforcement learning (RL) to enhance this filtering. They achieve this by estimating proxy reward functions from available datasets and using regularization to generate more diverse candidates. These reward functions are inherently uncertain, raising a particularly salient challenge for scientific discovery. In this work, we show that existing methods, often framed as sampling proportional to a reward function, are inadequate and yield suboptimal candidates, especially in large search spaces. To remedy this issue, we take a robust RL approach and introduce a unified operator that seeks robustness to the uncertainty of the proxy reward function. This general operator targets peakier sampling distributions while encompassing known soft RL operators. It also leads us to a novel algorithm that identifies higher-quality, diverse candidates in both synthetic and real-world tasks. Ultimately, our work offers a new, flexible perspective on discrete compositional generation tasks. Code: https://github.com/marcojira/tgm.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation",
      "titleJa": "The hidden cost of an image: quantifying the energy consumption of ai image generation",
      "link": "https://arxiv.org/abs/2506.17016",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17016v1 Announce Type: new \nAbstract: With the growing adoption of AI image generation, in conjunction with the ever-increasing environmental resources demanded by AI, we are urged to answer a fundamental question: What is the environmental impact hidden behind each image we generate? In this research, we present a comprehensive empirical experiment designed to assess the energy consumption of AI image generation. Our experiment compares 17 state-of-the-art image generation models by considering multiple factors that could affect their energy consumption, such as model quantization, image resolution, and prompt length. Additionally, we consider established image quality metrics to study potential trade-offs between energy consumption and generated image quality. Results show that image generation models vary drastically in terms of the energy they consume, with up to a 46x difference. Image resolution affects energy consumption inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution. U-Net-based models tend to consume less than Transformer-based one. Model quantization instead results to deteriorate the energy efficiency of most models, while prompt length and content have no statistically significant impact. Improving image quality does not always come at the cost of a higher energy consumption, with some of the models producing the highest quality images also being among the most energy efficient ones.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment",
      "titleJa": "Scalable and reliable multi-agent 強化学習 for traffic assignment",
      "link": "https://arxiv.org/abs/2506.17029",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17029v1 Announce Type: new \nAbstract: The evolution of metropolitan cities and the increase in travel demands impose stringent requirements on traffic assignment methods. Multi-agent reinforcement learning (MARL) approaches outperform traditional methods in modeling adaptive routing behavior without requiring explicit system dynamics, which is beneficial for real-world deployment. However, MARL frameworks face challenges in scalability and reliability when managing extensive networks with substantial travel demand, which limiting their practical applicability in solving large-scale traffic assignment problems. To address these challenges, this study introduces MARL-OD-DA, a new MARL framework for the traffic assignment problem, which redefines agents as origin-destination (OD) pair routers rather than individual travelers, significantly enhancing scalability. Additionally, a Dirichlet-based action space with action pruning and a reward function based on the local relative gap are designed to enhance solution reliability and improve convergence efficiency. Experiments demonstrate that the proposed MARL framework effectively handles medium-sized networks with extensive and varied city-level OD demand, surpassing existing MARL methods. When implemented in the SiouxFalls network, MARL-OD-DA achieves better assignment solutions in 10 steps, with a relative gap that is 94.99% lower than that of conventional methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Critical Appraisal of Fairness Metrics in Clinical Predictive AI",
      "titleJa": "Critical appraisal of fairness metrics in clinical predictive ai",
      "link": "https://arxiv.org/abs/2506.17035",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17035v1 Announce Type: new \nAbstract: Predictive artificial intelligence (AI) offers an opportunity to improve clinical practice and patient outcomes, but risks perpetuating biases if fairness is inadequately addressed. However, the definition of \"fairness\" remains unclear. We conducted a scoping review to identify and critically appraise fairness metrics for clinical predictive AI. We defined a \"fairness metric\" as a measure quantifying whether a model discriminates (societally) against individuals or groups defined by sensitive attributes. We searched five databases (2014-2024), screening 820 records, to include 41 studies, and extracted 62 fairness metrics. Metrics were classified by performance-dependency, model output level, and base performance metric, revealing a fragmented landscape with limited clinical validation and overreliance on threshold-dependent measures. Eighteen metrics were explicitly developed for healthcare, including only one clinical utility metric. Our findings highlight conceptual challenges in defining and quantifying fairness and identify gaps in uncertainty quantification, intersectionality, and real-world applicability. Future work should prioritise clinically meaningful metrics.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks",
      "titleJa": "Navigating the deep: signature extraction on deep neural networks",
      "link": "https://arxiv.org/abs/2506.17047",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17047v1 Announce Type: new \nAbstract: Neural network model extraction has emerged in recent years as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. A key step in this process is signature extraction, which aims to recover the absolute values of the network's weights layer by layer. Prior work, notably by Carlini et al. (2020), introduced a technique inspired by differential cryptanalysis to extract neural network parameters. However, their method suffers from several limitations that restrict its applicability to networks with a few layers only. Later works focused on improving sign extraction, but largely relied on the assumption that signature extraction itself was feasible.\n  In this work, we revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues, greatly improving the efficiency of signature extraction. Our approach permits the extraction of much deeper networks than was previously possible. We validate our method through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth and accuracy. For instance, our extracted network matches the target network on at least 95% of the input space for each of the eight layers of a neural network trained on the CIFAR-10 dataset, while previous works could barely extract the first three layers. Our results represent a crucial step toward practical attacks on larger and more complex neural network architectures.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Deep generative models as the probability transformation functions",
      "titleJa": "Deep generative models as the probability transformation functions",
      "link": "https://arxiv.org/abs/2506.17171",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17171v1 Announce Type: new \nAbstract: This paper introduces a unified theoretical perspective that views deep generative models as probability transformation functions. Despite the apparent differences in architecture and training methodologies among various types of generative models - autoencoders, autoregressive models, generative adversarial networks, normalizing flows, diffusion models, and flow matching - we demonstrate that they all fundamentally operate by transforming simple predefined distributions into complex target data distributions. This unifying perspective facilitates the transfer of methodological improvements between model architectures and provides a foundation for developing universal theoretical approaches, potentially leading to more efficient and effective generative modeling techniques.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Variational Learning of Disentangled Representations",
      "titleJa": "Variational learning of disentangled representations",
      "link": "https://arxiv.org/abs/2506.17182",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17182v1 Announce Type: new \nAbstract: Disentangled representations enable models to separate factors of variation that are shared across experimental conditions from those that are condition-specific. This separation is essential in domains such as biomedical data analysis, where generalization to new treatments, patients, or species depends on isolating stable biological signals from context-dependent effects. While extensions of the variational autoencoder (VAE) framework have been proposed to address this problem, they frequently suffer from leakage between latent representations, limiting their ability to generalize to unseen conditions. Here, we introduce DISCoVeR, a new variational framework that explicitly separates condition-invariant and condition-specific factors. DISCoVeR integrates three key components: (i) a dual-latent architecture that models shared and specific factors separately; (ii) two parallel reconstructions that ensure both representations remain informative; and (iii) a novel max-min objective that encourages clean separation without relying on handcrafted priors, while making only minimal assumptions. Theoretically, we show that this objective maximizes data likelihood while promoting disentanglement, and that it admits a unique equilibrium. Empirically, we demonstrate that DISCoVeR achieves improved disentanglement on synthetic datasets, natural images, and single-cell RNA-seq data. Together, these results establish DISCoVeR as a principled approach for learning disentangled representations in multi-condition settings.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimal Implicit Bias in Linear Regression",
      "titleJa": "Optimal implicit バイアス in linear regression",
      "link": "https://arxiv.org/abs/2506.17187",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17187v1 Announce Type: new \nAbstract: Most modern learning problems are over-parameterized, where the number of learnable parameters is much greater than the number of training data points. In this over-parameterized regime, the training loss typically has infinitely many global optima that completely interpolate the data with varying generalization performance. The particular global optimum we converge to depends on the implicit bias of the optimization algorithm. The question we address in this paper is, ``What is the implicit bias that leads to the best generalization performance?\". To find the optimal implicit bias, we provide a precise asymptotic analysis of the generalization performance of interpolators obtained from the minimization of convex functions/potentials for over-parameterized linear regression with non-isotropic Gaussian data. In particular, we obtain a tight lower bound on the best generalization error possible among this class of interpolators in terms of the over-parameterization ratio, the variance of the noise in the labels, the eigenspectrum of the data covariance, and the underlying distribution of the parameter to be estimated. Finally, we find the optimal convex implicit bias that achieves this lower bound under certain sufficient conditions involving the log-concavity of the distribution of a Gaussian convolved with the prior of the true underlying parameter.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Modern approaches to building effective interpretable models of the property market using machine learning",
      "titleJa": "Modern approaches to building effective interpretable models of the property market using 機械学習",
      "link": "https://arxiv.org/abs/2506.15723",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15723v1 Announce Type: cross \nAbstract: In this article, we review modern approaches to building interpretable models of property markets using machine learning on the base of mass valuation of property in the Primorye region, Russia. The researcher, lacking expertise in this topic, encounters numerous difficulties in the effort to build a good model. The main source of this is the huge difference between noisy real market data and ideal data which is very common in all types of tutorials on machine learning. This paper covers all stages of modeling: the collection of initial data, identification of outliers, the search and analysis of patterns in data, the formation and final choice of price factors, the building of the model, and the evaluation of its efficiency. For each stage, we highlight potential issues and describe sound methods for overcoming emerging difficulties on actual examples. We show that the combination of classical linear regression with interpolation methods of geostatistics allows to build an effective model for land parcels. For flats, when many objects are attributed to one spatial point the application of geostatistical methods is difficult. Therefore we suggest linear regression with automatic generation and selection of additional rules on the base of decision trees, so called the RuleFit method. Thus we show, that despite the strong restriction as the requirement of interpretability which is important in practical aspects, for example, legal matters, it is still possible to build effective models of real property markets.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Sampling conditioned diffusions via Pathspace Projected Monte Carlo",
      "titleJa": "Sampling conditioned diffusions via pathspace projected monte carlo",
      "link": "https://arxiv.org/abs/2506.15743",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15743v1 Announce Type: cross \nAbstract: We present an algorithm to sample stochastic differential equations conditioned on rather general constraints, including integral constraints, endpoint constraints, and stochastic integral constraints. The algorithm is a pathspace Metropolis-adjusted manifold sampling scheme, which samples stochastic paths on the submanifold of realizations that adhere to the conditioning constraint. We demonstrate the effectiveness of the algorithm by sampling a dynamical condensation phase transition, conditioning a random walk on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave equation on high amplitude waves, and sampling a stochastic partial differential equation model of turbulent pipe flow conditioned on relaminarization events.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Pixel-wise Modulated Dice Loss for Medical Image Segmentation",
      "titleJa": "Pixel-wise modulated dice loss for 医療 image segmentation",
      "link": "https://arxiv.org/abs/2506.15744",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15744v1 Announce Type: cross \nAbstract: Class imbalance and the difficulty imbalance are the two types of data imbalance that affect the performance of neural networks in medical segmentation tasks. In class imbalance the loss is dominated by the majority classes and in difficulty imbalance the loss is dominated by easy to classify pixels. This leads to an ineffective training. Dice loss, which is based on a geometrical metric, is very effective in addressing the class imbalance compared to the cross entropy (CE) loss, which is adopted directly from classification tasks. To address the difficulty imbalance, the common approach is employing a re-weighted CE loss or a modified Dice loss to focus the training on difficult to classify areas. The existing modification methods are computationally costly and with limited success. In this study we propose a simple modification to the Dice loss with minimal computational cost. With a pixel level modulating term, we take advantage of the effectiveness of Dice loss in handling the class imbalance to also handle the difficulty imbalance. Results on three commonly used medical segmentation tasks show that the proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other methods, which are designed to tackle the difficulty imbalance problem.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Approximate Ricci-flat Metrics for Calabi-Yau Manifolds",
      "titleJa": "Approximate ricci-flat metrics for calabi-yau manifolds",
      "link": "https://arxiv.org/abs/2506.15766",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15766v1 Announce Type: cross \nAbstract: We outline a method to determine analytic K\\\"ahler potentials with associated approximately Ricci-flat K\\\"ahler metrics on Calabi-Yau manifolds. Key ingredients are numerically calculating Ricci-flat K\\\"ahler potentials via machine learning techniques and fitting the numerical results to Donaldson's Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in $\\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY hypersurfaces in $\\mathbb{P}^2\\times\\mathbb{P}^2$. In each case, a relatively simple analytic expression is obtained for the approximately Ricci-flat K\\\"ahler potentials, including the explicit dependence on the complex structure parameter. We find that these K\\\"ahler potentials only depend on the modulus of the complex structure parameter.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces",
      "titleJa": "Convergent methods for koopman operators on reproducing kernel hilbert spaces",
      "link": "https://arxiv.org/abs/2506.15782",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15782v1 Announce Type: cross \nAbstract: Data-driven spectral analysis of Koopman operators is a powerful tool for understanding numerous real-world dynamical systems, from neuronal activity to variations in sea surface temperature. The Koopman operator acts on a function space and is most commonly studied on the space of square-integrable functions. However, defining it on a suitable reproducing kernel Hilbert space (RKHS) offers numerous practical advantages, including pointwise predictions with error bounds, improved spectral properties that facilitate computations, and more efficient algorithms, particularly in high dimensions. We introduce the first general, provably convergent, data-driven algorithms for computing spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These methods efficiently compute spectra and pseudospectra with error control and spectral measures while exploiting the RKHS structure to avoid the large-data limits required in the $L^2$ settings. The function space is determined by a user-specified kernel, eliminating the need for quadrature-based sampling as in $L^2$ and enabling greater flexibility with finite, externally provided datasets. Using the Solvability Complexity Index hierarchy, we construct adversarial dynamical systems for these problems to show that no algorithm can succeed in fewer limits, thereby proving the optimality of our algorithms. Notably, this impossibility extends to randomized algorithms and datasets. We demonstrate the effectiveness of our algorithms on challenging, high-dimensional datasets arising from real-world measurements and high-fidelity numerical simulations, including turbulent channel flow, molecular dynamics of a binding protein, Antarctic sea ice concentration, and Northern Hemisphere sea surface height. The algorithms are publicly available in the software package $\\texttt{SpecRKHS}$.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Code Rate Optimization via Neural Polar Decoders",
      "titleJa": "Code rate optimization via neural polar decoders",
      "link": "https://arxiv.org/abs/2506.15836",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15836v1 Announce Type: cross \nAbstract: This paper proposes a method to optimize communication code rates via the application of neural polar decoders (NPDs). Employing this approach enables simultaneous optimization of code rates over input distributions while providing a practical coding scheme within the framework of polar codes. The proposed approach is designed for scenarios where the channel model is unknown, treating the channel as a black box that produces output samples from input samples. We employ polar codes to achieve our objectives, using NPDs to estimate mutual information (MI) between the channel inputs and outputs, and optimize a parametric model of the input distribution. The methodology involves a two-phase process: a training phase and an inference phase. In the training phase, two steps are repeated interchangeably. First, the estimation step estimates the MI of the channel inputs and outputs via NPDs. Second, the improvement step optimizes the input distribution parameters to maximize the MI estimate obtained by the NPDs. In the inference phase, the optimized model is used to construct polar codes. This involves incorporating the Honda-Yamamoto (HY) scheme to accommodate the optimized input distributions and list decoding to enhance decoding performance. Experimental results on memoryless and finite-state channels (FSCs) demonstrate the effectiveness of our approach, particularly in cases where the channel's capacity-achieving input distribution is non-uniform. For these cases, we show significant improvements in MI and bit error rates (BERs) over those achieved by uniform and independent and identically distributed (i.i.d.) input distributions, validating our method for block lengths up to 1024. This scalable approach has potential applications in real-world communication systems, bridging theoretical capacity estimation and practical coding performance.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Fair Contracts in Principal-Agent Games with Heterogeneous Types",
      "titleJa": "Fair contracts in principal-agent games with heterogeneous types",
      "link": "https://arxiv.org/abs/2506.15887",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15887v1 Announce Type: cross \nAbstract: Fairness is desirable yet challenging to achieve within multi-agent systems, especially when agents differ in latent traits that affect their abilities. This hidden heterogeneity often leads to unequal distributions of wealth, even when agents operate under the same rules. Motivated by real-world examples, we propose a framework based on repeated principal-agent games, where a principal, who also can be seen as a player of the game, learns to offer adaptive contracts to agents. By leveraging a simple yet powerful contract structure, we show that a fairness-aware principal can learn homogeneous linear contracts that equalize outcomes across agents in a sequential social dilemma. Importantly, this fairness does not come at the cost of efficiency: our results demonstrate that it is possible to promote equity and stability in the system while preserving overall performance.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems",
      "titleJa": "From local interactions to global operators: scalable gaussian process operator for physical systems",
      "link": "https://arxiv.org/abs/2506.15906",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15906v1 Announce Type: cross \nAbstract: Operator learning offers a powerful paradigm for solving parametric partial differential equations (PDEs), but scaling probabilistic neural operators such as the recently proposed Gaussian Processes Operators (GPOs) to high-dimensional, data-intensive regimes remains a significant challenge. In this work, we introduce a novel, scalable GPO, which capitalizes on sparsity, locality, and structural information through judicious kernel design. Addressing the fundamental limitation of cubic computational complexity, our method leverages nearest-neighbor-based local kernel approximations in the spatial domain, sparse kernel approximation in the parameter space, and structured Kronecker factorizations to enable tractable inference on large-scale datasets and high-dimensional input. While local approximations often introduce accuracy trade-offs due to limited kernel interactions, we overcome this by embedding operator-aware kernel structures and employing expressive, task-informed mean functions derived from neural operator architectures. Through extensive evaluations on a broad class of nonlinear PDEs - including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations - we demonstrate that our framework consistently achieves high accuracy across varying discretization scales. These results underscore the potential of our approach to bridge the gap between scalability and fidelity in GPO, offering a compelling foundation for uncertainty-aware modeling in complex physical systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Pediatric Pancreas Segmentation from MRI Scans with Deep Learning",
      "titleJa": "Pediatric pancreas segmentation from mri scans with ディープラーニング",
      "link": "https://arxiv.org/abs/2506.15908",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15908v1 Announce Type: cross \nAbstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep learning (DL) algorithm for pediatric pancreas segmentation on MRI in children with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls. Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T Siemens Aera/Verio) from children aged 2-19 years at Gazi University (2015-2024). The dataset includes healthy children as well as patients diagnosed with AP or CP based on clinical criteria. Pediatric and general radiologists manually segmented the pancreas, then confirmed by a senior pediatric radiologist. PanSegNet-generated segmentations were assessed using Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance (HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years) and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98 mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86 (controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and 0.81. Strong agreement was observed between automated and manual volumes (R^2 = 0.85 in controls, 0.77 in diseased), demonstrating clinical reliability. Conclusion: PanSegNet represents the first validated deep learning solution for pancreatic MRI segmentation, achieving expert-level performance across healthy and diseased states. This tool, algorithm, along with our annotated dataset, are freely available on GitHub and OSF, advancing accessible, radiation-free pediatric pancreatic imaging and fostering collaborative research in this underserved domain.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Data-Agnostic Cardinality Learning from Imperfect Workloads",
      "titleJa": "Data-agnostic cardinality learning from imperfect workloads",
      "link": "https://arxiv.org/abs/2506.16007",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16007v1 Announce Type: cross \nAbstract: Cardinality estimation (CardEst) is a critical aspect of query optimization. Traditionally, it leverages statistics built directly over the data. However, organizational policies (e.g., regulatory compliance) may restrict global data access. Fortunately, query-driven cardinality estimation can learn CardEst models using query workloads. However, existing query-driven models often require access to data or summaries for best performance, and they assume perfect training workloads with complete and balanced join templates (or join graphs). Such assumptions rarely hold in real-world scenarios, in which join templates are incomplete and imbalanced. We present GRASP, a data-agnostic cardinality learning system designed to work under these real-world constraints. GRASP's compositional design generalizes to unseen join templates and is robust to join template imbalance. It also introduces a new per-table CardEst model that handles value distribution shifts for range predicates, and a novel learned count sketch model that captures join correlations across base relations. Across three database instances, we demonstrate that GRASP consistently outperforms existing query-driven models on imperfect workloads, both in terms of estimation accuracy and query latency. Remarkably, GRASP achieves performance comparable to, or even surpassing, traditional approaches built over the underlying data on the complex CEB-IMDb-full benchmark -- despite operating without any data access and using only 10% of all possible join templates.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Diffusion-Based Hypothesis Testing and Change-Point Detection",
      "titleJa": "Diffusion-based hypothesis testing and change-point detection",
      "link": "https://arxiv.org/abs/2506.16089",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16089v1 Announce Type: cross \nAbstract: Score-based methods have recently seen increasing popularity in modeling and generation. Methods have been constructed to perform hypothesis testing and change-point detection with score functions, but these methods are in general not as powerful as their likelihood-based peers. Recent works consider generalizing the score-based Fisher divergence into a diffusion-divergence by transforming score functions via multiplication with a matrix-valued function or a weight matrix. In this paper, we extend the score-based hypothesis test and change-point detection stopping rule into their diffusion-based analogs. Additionally, we theoretically quantify the performance of these diffusion-based algorithms and study scenarios where optimal performance is achievable. We propose a method of numerically optimizing the weight matrix and present numerical simulations to illustrate the advantages of diffusion-based algorithms.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "VideoGAN-based Trajectory Proposal for Automated Vehicles",
      "titleJa": "Videogan-based trajectory proposal for automated vehicles",
      "link": "https://arxiv.org/abs/2506.16209",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16209v1 Announce Type: cross \nAbstract: Being able to generate realistic trajectory options is at the core of increasing the degree of automation of road vehicles. While model-driven, rule-based, and classical learning-based methods are widely used to tackle these tasks at present, they can struggle to effectively capture the complex, multimodal distributions of future trajectories. In this paper we investigate whether a generative adversarial network (GAN) trained on videos of bird's-eye view (BEV) traffic scenarios can generate statistically accurate trajectories that correctly capture spatial relationships between the agents. To this end, we propose a pipeline that uses low-resolution BEV occupancy grid videos as training data for a video generative model. From the generated videos of traffic scenarios we extract abstract trajectory data using single-frame object detection and frame-to-frame object matching. We particularly choose a GAN architecture for the fast training and inference times with respect to diffusion models. We obtain our best results within 100 GPU hours of training, with inference times under 20\\,ms. We demonstrate the physical realism of the proposed trajectories in terms of distribution alignment of spatial and dynamic parameters with respect to the ground truth videos from the Waymo Open Motion Dataset.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy",
      "titleJa": "Malware classification leveraging nlp & 機械学習 for enhanced accuracy",
      "link": "https://arxiv.org/abs/2506.16224",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16224v1 Announce Type: cross \nAbstract: This paper investigates the application of natural language processing (NLP)-based n-gram analysis and machine learning techniques to enhance malware classification. We explore how NLP can be used to extract and analyze textual features from malware samples through n-grams, contiguous string or API call sequences. This approach effectively captures distinctive linguistic patterns among malware and benign families, enabling finer-grained classification. We delve into n-gram size selection, feature representation, and classification algorithms. While evaluating our proposed method on real-world malware samples, we observe significantly improved accuracy compared to the traditional methods. By implementing our n-gram approach, we achieved an accuracy of 99.02% across various machine learning algorithms by using hybrid feature selection technique to address high dimensionality. Hybrid feature selection technique reduces the feature set to only 1.6% of the original features.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation",
      "titleJa": "Can ai dream of unseen galaxies? conditional diffusion モデル for galaxy morphology augmentation",
      "link": "https://arxiv.org/abs/2506.16233",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16233v1 Announce Type: cross \nAbstract: Observational astronomy relies on visual feature identification to detect critical astrophysical phenomena. While machine learning (ML) increasingly automates this process, models often struggle with generalization in large-scale surveys due to the limited representativeness of labeled datasets -- whether from simulations or human annotation -- a challenge pronounced for rare yet scientifically valuable objects. To address this, we propose a conditional diffusion model to synthesize realistic galaxy images for augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains visual feature -- galaxy image pairs from volunteer annotation, we demonstrate that our model generates diverse, high-fidelity galaxy images closely adhere to the specified morphological feature conditions. Moreover, this model enables generative extrapolation to project well-annotated data into unseen domains and advancing rare object detection. Integrating synthesized images into ML pipelines improves performance in standard morphology classification, boosting completeness and purity by up to 30\\% across key metrics. For rare object detection, using early-type galaxies with prominent dust lane features ( $\\sim$0.1\\% in GZ2 dataset) as a test case, our approach doubled the number of detected instances from 352 to 872, compared to previous studies based on visual inspection. This study highlights the power of generative models to bridge gaps between scarce labeled data and the vast, uncharted parameter space of observational astronomy and sheds insight for future astrophysical foundation model developments. Our project homepage is available at https://galaxysd-webpage.streamlit.app/.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Random feature approximation for general spectral methods",
      "titleJa": "Random feature approximation for general spectral methods",
      "link": "https://arxiv.org/abs/2506.16283",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16283v1 Announce Type: cross \nAbstract: Random feature approximation is arguably one of the most widely used techniques for kernel methods in large-scale learning algorithms. In this work, we analyze the generalization properties of random feature methods, extending previous results for Tikhonov regularization to a broad class of spectral regularization techniques. This includes not only explicit methods but also implicit schemes such as gradient descent and accelerated algorithms like the Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical analysis of neural networks and neural operators through the lens of the Neural Tangent Kernel (NTK) approach trained via gradient descent. For our estimators we obtain optimal learning rates over regularity classes (even for classes that are not included in the reproducing kernel Hilbert space), which are defined through appropriate source conditions. This improves or completes previous results obtained in related settings for specific kernel algorithms.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units",
      "titleJa": "The condition number as a scale-invariant proxy for information encoding in neural units",
      "link": "https://arxiv.org/abs/2506.16289",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16289v1 Announce Type: cross \nAbstract: This paper explores the relationship between the condition number of a neural network's weight tensor and the extent of information encoded by the associated processing unit, viewed through the lens of information theory. We argue that a high condition number, though not sufficient for effective knowledge encoding, may indicate that the unit has learned to selectively amplify and compress information. We formalize this intuition, particularly for linear units with Gaussian inputs, linking the condition number and the transformation's log-volume scaling factor to the characteristics of the output entropy and the geometric properties of the learned transformation. Our analysis demonstrates that for a fixed weight norm, a concentrated distribution of singular values (high condition number) corresponds to reduced overall information transfer, indicating a specialized and efficient encoding strategy. Furthermore, we present a practical case study where these principles are applied to guide selective fine-tuning of a multimodal Large Language Model, aiming to mitigate catastrophic forgetting during cross-modal adaptation. Unlike many existing catastrophic forgetting mitigation methods that rely on access to pre-training statistics, which are often unavailable, our selective fine-tuning approach offers a way to bypass this common requirement.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Identifying Heterogeneity in Distributed Learning",
      "titleJa": "Identifying heterogeneity in distributed learning",
      "link": "https://arxiv.org/abs/2506.16394",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16394v1 Announce Type: cross \nAbstract: We study methods for identifying heterogeneous parameter components in distributed M-estimation with minimal data transmission. One is based on a re-normalized Wald test, which is shown to be consistent as long as the number of distributed data blocks $K$ is of a smaller order of the minimum block sample size {and the level of heterogeneity is dense}. The second one is an extreme contrast test (ECT) based on the difference between the largest and smallest component-wise estimated parameters among data blocks. By introducing a sample splitting procedure, the ECT can avoid the bias accumulation arising from the M-estimation procedures, and exhibits consistency for $K$ being much larger than the sample size while the heterogeneity is sparse. The ECT procedure is easy to operate and communication-efficient. A combination of the Wald and the extreme contrast tests is formulated to attain more robust power under varying levels of sparsity of the heterogeneity. We also conduct intensive numerical experiments to compare the family-wise error rate (FWER) and the power of the proposed methods. Additionally, we conduct a case study to present the implementation and validity of the proposed methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On Continuous Monitoring of Risk Violations under Unknown Shift",
      "titleJa": "On continuous monitoring of risk violations under unknown shift",
      "link": "https://arxiv.org/abs/2506.16416",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16416v1 Announce Type: cross \nAbstract: Machine learning systems deployed in the real world must operate under dynamic and often unpredictable distribution shifts. This challenges the validity of statistical safety assurances on the system's risk established beforehand. Common risk control frameworks rely on fixed assumptions and lack mechanisms to continuously monitor deployment reliability. In this work, we propose a general framework for the real-time monitoring of risk violations in evolving data streams. Leveraging the 'testing by betting' paradigm, we propose a sequential hypothesis testing procedure to detect violations of bounded risks associated with the model's decision-making mechanism, while ensuring control on the false alarm rate. Our method operates under minimal assumptions on the nature of encountered shifts, rendering it broadly applicable. We illustrate the effectiveness of our approach by monitoring risks in outlier detection and set prediction under a variety of shifts.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models",
      "titleJa": "Flatcad: fast curvature regularization of neural sdfs for cad models",
      "link": "https://arxiv.org/abs/2506.16627",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16627v1 Announce Type: cross \nAbstract: Neural signed-distance fields (SDFs) have become a versatile backbone for geometric learning, yet enforcing developable, CAD-style behavior still hinges on Gaussian curvature penalties that require full Hessian evaluation and second-order automatic differentiation, both of which are costly in memory and runtime. We present a curvature proxy that regularizes only the mixed second-order term (Weingarten term), allowing the two principal curvatures to adapt freely to data while suppressing unwanted warp. Two complementary instantiations realize this idea: (i) a finite-difference proxy that replaces each Hessian entry with four forward SDF evaluations and a single first-order gradient, and (ii) an autodiff proxy that computes the same mixed derivative via one Hessian-vector product, sidestepping explicit full Hessian assembly and remaining faster in practice. Both variants converge to the exact mixed second derivative, thus preserving the intended geometric bias without incurring full second-order graphs. On the ABC benchmarks, the proxies match or exceed the reconstruction fidelity of Hessian-based baselines while reducing GPU memory use and wall-clock time by a factor of two. Because the method is drop-in and framework-agnostic, it opens a practical path toward scalable, curvature-aware SDF learning for engineering-grade shape reconstruction.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards",
      "titleJa": "Multi-armed bandits with 機械学習-generated surrogate rewards",
      "link": "https://arxiv.org/abs/2506.16658",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16658v1 Announce Type: cross \nAbstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential decision-making under uncertainty. Traditional bandit algorithms rely solely on online data, which tends to be scarce as it must be gathered during the online phase when the arms are actively pulled. However, in many practical settings, rich auxiliary data, such as covariates of past users, is available prior to deploying any arms. We introduce a new setting for MAB where pre-trained machine learning (ML) models are applied to convert side information and historical data into \\emph{surrogate rewards}. A prominent feature of this setting is that the surrogate rewards may exhibit substantial bias, as true reward data is typically unavailable in the offline phase, forcing ML predictions to heavily rely on extrapolation. To address the issue, we propose the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which can be applied to any reward prediction model and any form of auxiliary data. When the predicted and true rewards are jointly Gaussian, it provably improves the cumulative regret, provided that the correlation is non-zero -- even in cases where the mean surrogate reward completely misaligns with the true mean rewards. Notably, our method requires no prior knowledge of the covariance matrix between true and surrogate rewards. We compare MLA-UCB with the standard UCB on a range of numerical studies and show a sizable efficiency gain even when the size of the offline data and the correlation between predicted and true rewards are moderate.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing",
      "titleJa": "The hitchhiker's guide to efficient, end-to-end, and tight dp auditing",
      "link": "https://arxiv.org/abs/2506.16666",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16666v1 Announce Type: cross \nAbstract: This paper systematizes research on auditing Differential Privacy (DP) techniques, aiming to identify key insights into the current state of the art and open challenges. First, we introduce a comprehensive framework for reviewing work in the field and establish three cross-contextual desiderata that DP audits should target--namely, efficiency, end-to-end-ness, and tightness. Then, we systematize the modes of operation of state-of-the-art DP auditing techniques, including threat models, attacks, and evaluation functions. This allows us to highlight key details overlooked by prior work, analyze the limiting factors to achieving the three desiderata, and identify open research problems. Overall, our work provides a reusable and systematic methodology geared to assess progress in the field and identify friction points and future directions for our community to focus on.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Beyond Blur: A Fluid Perspective on Generative Diffusion Models",
      "titleJa": "Beyond blur: a fluid perspective on generative diffusion models",
      "link": "https://arxiv.org/abs/2506.16827",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16827v1 Announce Type: cross \nAbstract: We propose a novel PDE-driven corruption process for generative image synthesis based on advection-diffusion processes which generalizes existing PDE-based approaches. Our forward pass formulates image corruption via a physically motivated PDE that couples directional advection with isotropic diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet, Fourier). We implement this PDE numerically through a GPU-accelerated custom Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence, we generate stochastic velocity fields that introduce coherent motion and capture multi-scale mixing. In the generative process, a neural network learns to reverse the advection-diffusion operator thus constituting a novel generative model. We discuss how previous methods emerge as specific cases of our operator, demonstrating that our framework generalizes prior PDE-based corruption techniques. We illustrate how advection improves the diversity and quality of the generated images while keeping the overall color palette unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and deep generative modeling, offering a fresh perspective on physically informed image corruption processes for diffusion-based synthesis.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "RCNet: $\\Delta\\Sigma$ IADCs as Recurrent AutoEncoders",
      "titleJa": "Rcnet: $\\delta\\sigma$ iadcs as recurrent autoencoders",
      "link": "https://arxiv.org/abs/2506.16903",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16903v1 Announce Type: cross \nAbstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma ($\\Delta\\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both modulators and filters. This analogy is applied to Incremental ADCs (IADC). High-end optimizers combined with full-custom losses are used to define additional hardware design constraints: quantized weights, signal saturation, temporal noise injection, devices area. Focusing on DC conversion, our early results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB) can be optimized under a certain hardware mapping complexity. The proposed RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples). Interestingly, it appears that the best RCNet architectures do not necessarily rely on high-order modulators, leveraging additional topology exploration degrees of freedom.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts",
      "titleJa": "From data to knowledge: evaluating how efficiently language models learn facts",
      "link": "https://arxiv.org/abs/2506.16912",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16912v1 Announce Type: cross \nAbstract: Sample efficiency is a crucial property of language models with practical implications for training efficiency. In real-world text, information follows a long-tailed distribution. Yet, we expect models to learn and recall frequent and infrequent facts. Sample-efficient models are better equipped to handle this challenge of learning and retaining rare information without requiring excessive exposure. This study analyzes multiple models of varying architectures and sizes, all trained on the same pre-training data. By annotating relational facts with their frequencies in the training corpus, we examine how model performance varies with fact frequency. Our findings show that most models perform similarly on high-frequency facts but differ notably on low-frequency facts. This analysis provides new insights into the relationship between model architecture, size, and factual learning efficiency.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials",
      "titleJa": "A neural operator based hybrid microscale モデル for multiscale simulation of rate-dependent materials",
      "link": "https://arxiv.org/abs/2506.16918",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.16918v1 Announce Type: cross \nAbstract: The behavior of materials is influenced by a wide range of phenomena occurring across various time and length scales. To better understand the impact of microstructure on macroscopic response, multiscale modeling strategies are essential. Numerical methods, such as the $\\text{FE}^2$ approach, account for micro-macro interactions to predict the global response in a concurrent manner. However, these methods are computationally intensive due to the repeated evaluations of the microscale. This challenge has led to the integration of deep learning techniques into computational homogenization frameworks to accelerate multiscale simulations. In this work, we employ neural operators to predict the microscale physics, resulting in a hybrid model that combines data-driven and physics-based approaches. This allows for physics-guided learning and provides flexibility for different materials and spatial discretizations. We apply this method to time-dependent solid mechanics problems involving viscoelastic material behavior, where the state is represented by internal variables only at the microscale. The constitutive relations of the microscale are incorporated into the model architecture and the internal variables are computed based on established physical principles. The results for homogenized stresses ($<6\\%$ error) show that the approach is computationally efficient ($\\sim 100 \\times$ faster).",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows",
      "titleJa": "Simulating correlated electrons with symmetry-enforced normalizing flows",
      "link": "https://arxiv.org/abs/2506.17015",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17015v1 Announce Type: cross \nAbstract: We present the first proof of principle that normalizing flows can accurately learn the Boltzmann distribution of the fermionic Hubbard model - a key framework for describing the electronic structure of graphene and related materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from ergodicity issues near the time-continuum limit, leading to biased estimates. Leveraging symmetry-aware architectures as well as independent and identically distributed sampling, our approach resolves these issues and achieves significant speed-ups over traditional methods.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction",
      "titleJa": "Bayesian joint モデル of multi-sensor and failure event data for multi-mode failure prediction",
      "link": "https://arxiv.org/abs/2506.17036",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17036v1 Announce Type: cross \nAbstract: Modern industrial systems are often subject to multiple failure modes, and their conditions are monitored by multiple sensors, generating multiple time-series signals. Additionally, time-to-failure data are commonly available. Accurately predicting a system's remaining useful life (RUL) requires effectively leveraging multi-sensor time-series data alongside multi-mode failure event data. In most existing models, failure modes and RUL prediction are performed independently, ignoring the inherent relationship between these two tasks. Some models integrate multiple failure modes and event prediction using black-box machine learning approaches, which lack statistical rigor and cannot characterize the inherent uncertainty in the model and data. This paper introduces a unified approach to jointly model the multi-sensor time-series data and failure time concerning multiple failure modes. This proposed model integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian Process, and multinomial failure mode distributions in a hierarchical Bayesian framework with corresponding priors, enabling accurate prediction with robust uncertainty quantification. Posterior distributions are effectively obtained by Variational Bayes, and prediction is performed with Monte Carlo sampling. The advantages of the proposed model is validated through extensive numerical and case studies with jet-engine dataset.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Universal Music Representations? Evaluating Foundation Models on World Music Corpora",
      "titleJa": "Universal music representations? evaluating foundation models on world music corpora",
      "link": "https://arxiv.org/abs/2506.17055",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17055v1 Announce Type: cross \nAbstract: Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these models' cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios. Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks",
      "titleJa": "Client selection strategies for federated semantic communications in heterogeneous iot networks",
      "link": "https://arxiv.org/abs/2506.17063",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17063v1 Announce Type: cross \nAbstract: The exponential growth of IoT devices presents critical challenges in bandwidth-constrained wireless networks, particularly regarding efficient data transmission and privacy preservation. This paper presents a novel federated semantic communication (SC) framework that enables collaborative training of bandwidth-efficient models for image reconstruction across heterogeneous IoT devices. By leveraging SC principles to transmit only semantic features, our approach dramatically reduces communication overhead while preserving reconstruction quality. We address the fundamental challenge of client selection in federated learning environments where devices exhibit significant disparities in dataset sizes and data distributions. Our framework implements three distinct client selection strategies that explore different trade-offs between system performance and fairness in resource allocation. The system employs an end-to-end SC architecture with semantic bottlenecks, coupled with a loss-based aggregation mechanism that naturally adapts to client heterogeneity. Experimental evaluation on image data demonstrates that while Utilitarian selection achieves the highest reconstruction quality, Proportional Fairness maintains competitive performance while significantly reducing participation inequality and improving computational efficiency. These results establish that federated SC can successfully balance reconstruction quality, resource efficiency, and fairness in heterogeneous IoT deployments, paving the way for sustainable and privacy-preserving edge intelligence applications.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings",
      "titleJa": "Generative modeling of full-atom protein conformations using latent diffusion on graph embeddings",
      "link": "https://arxiv.org/abs/2506.17064",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17064v1 Announce Type: cross \nAbstract: Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Neural Polar Decoders for DNA Data Storage",
      "titleJa": "Neural polar decoders for dna data storage",
      "link": "https://arxiv.org/abs/2506.17076",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17076v1 Announce Type: cross \nAbstract: Synchronization errors, such as insertions and deletions, present a fundamental challenge in DNA-based data storage systems, arising from both synthesis and sequencing noise. These channels are often modeled as insertion-deletion-substitution (IDS) channels, for which designing maximum-likelihood decoders is computationally expensive. In this work, we propose a data-driven approach based on neural polar decoders (NPDs) to design low-complexity decoders for channels with synchronization errors. The proposed architecture enables decoding over IDS channels with reduced complexity $O(AN log N )$, where $A$ is a tunable parameter independent of the channel. NPDs require only sample access to the channel and can be trained without an explicit channel model. Additionally, NPDs provide mutual information (MI) estimates that can be used to optimize input distributions and code design. We demonstrate the effectiveness of NPDs on both synthetic deletion and IDS channels. For deletion channels, we show that NPDs achieve near-optimal decoding performance and accurate MI estimation, with significantly lower complexity than trellis-based decoders. We also provide numerical estimates of the channel capacity for the deletion channel. We extend our evaluation to realistic DNA storage settings, including channels with multiple noisy reads and real-world Nanopore sequencing data. Our results show that NPDs match or surpass the performance of existing methods while using significantly fewer parameters than the state-of-the-art. These findings highlight the promise of NPDs for robust and efficient decoding in DNA data storage systems.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Schr\\\"odinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres",
      "titleJa": "Schr\\\"odinger bridge matching for tree-structured costs and entropic wasserstein barycentres",
      "link": "https://arxiv.org/abs/2506.17197",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.17197v1 Announce Type: cross \nAbstract: Recent advances in flow-based generative modelling have provided scalable methods for computing the Schr\\\"odinger Bridge (SB) between distributions, a dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB problem via sequential bridge-matching steps, presenting an elegant and practical approach with many favourable properties over the more traditional Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting, optimal transport can be generalised to the multi-marginal case in which the objective is to minimise a cost defined over several marginal distributions. Of particular importance are costs defined over a tree structure, from which Wasserstein barycentres can be recovered as a special case. In this work, we extend the IMF procedure to solve for the tree-structured SB problem. Our resulting algorithm inherits the many advantages of IMF over IPF approaches in the tree-based setting. In the specific case of Wasserstein barycentres, our approach can be viewed as extending fixed-point approaches for barycentre computation to the case of flow-based entropic OT solvers.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On the generalization of Tanimoto-type kernels to real valued functions",
      "titleJa": "On the generalization of tanimoto-type kernels to real valued functions",
      "link": "https://arxiv.org/abs/2007.05943",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2007.05943v3 Announce Type: replace \nAbstract: The Tanimoto kernel (Jaccard index) is a well known tool to describe the similarity between sets of binary attributes. It has been extended to the case when the attributes are nonnegative real values. This paper introduces a more general Tanimoto kernel formulation which allows to measure the similarity of arbitrary real-valued functions. This extension is constructed by unifying the representation of the attributes via properly chosen sets. After deriving the general form of the kernel, explicit feature representation is extracted from the kernel function, and a simply way of including general kernels into the Tanimoto kernel is shown. Finally, the kernel is also expressed as a quotient of piecewise linear functions, and a smooth approximation is provided.",
      "summary": "arXiv:2007.",
      "summaryJa": "Arxiv:2007.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Sheaf Hypergraph Networks",
      "titleJa": "Sheaf hypergraph networks",
      "link": "https://arxiv.org/abs/2309.17116",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2309.17116v2 Announce Type: replace \nAbstract: Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.",
      "summary": "arXiv:2309.",
      "summaryJa": "Arxiv:2309.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "On the Robustness of Decision-Focused Learning",
      "titleJa": "On the robustness of decision-focused learning",
      "link": "https://arxiv.org/abs/2311.16487",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2311.16487v4 Announce Type: replace \nAbstract: Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles the task of training a machine learning (ML) model to predict missing parameters of an incomplete optimization problem, where the missing parameters are predicted. DFL trains an ML model in an end-to-end system, by integrating the prediction and optimization tasks, providing better alignment of the training and testing objectives. DFL has shown a lot of promise and holds the capacity to revolutionize decision-making in many real-world applications. However, very little is known about the performance of these models under adversarial attacks. We adopt ten unique DFL methods and benchmark their performance under two distinctly focused attacks adapted towards the Predict-then-Optimize problem setting. Our study proposes the hypothesis that the robustness of a model is highly correlated with its ability to find predictions that lead to optimal decisions without deviating from the ground-truth label. Furthermore, we provide insight into how to target the models that violate this condition and show how these models respond differently depending on the achieved optimality at the end of their training cycles.",
      "summary": "arXiv:2311.",
      "summaryJa": "Arxiv:2311.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation",
      "titleJa": "Icc: quantifying image caption concreteness for multimodal データセット curation",
      "link": "https://arxiv.org/abs/2403.01306",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2403.01306v4 Announce Type: replace \nAbstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.",
      "summary": "arXiv:2403.",
      "summaryJa": "Arxiv:2403.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Client-Centered Federated Learning for Heterogeneous EHRs: Use Fewer Participants to Achieve the Same Performance",
      "titleJa": "Client-centered federated learning for heterogeneous ehrs: use fewer participants to achieve the same performance",
      "link": "https://arxiv.org/abs/2404.13318",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2404.13318v4 Announce Type: replace \nAbstract: The increasing volume of electronic health records (EHRs) presents the opportunity to improve the accuracy and robustness of models in clinical prediction tasks. Unlike traditional centralized approaches, federated learning enables training on data from multiple institutions while preserving patient privacy and complying with regulatory constraints. In practice, healthcare institutions (i.e., hosts) often need to build predictive models tailored to their specific needs using federated learning. In this scenario, two key challenges arise: (1) ensuring compatibility across heterogeneous EHR systems, and (2) managing federated learning costs within budget constraints. To address these challenges, we propose EHRFL, a federated learning framework designed for building a cost-effective, host-specific predictive model using patient EHR data. EHRFL consists of two components: (1) text-based EHR modeling, which facilitates cross-institution compatibility without costly data standardization, and (2) a participant selection strategy based on averaged patient embedding similarity to reduce the number of participants without degrading performance. Experiments on multiple open-source EHR datasets demonstrate the effectiveness of both components. We believe our framework offers a practical solution for enabling healthcare institutions to build institution-specific predictive models under budgetary constraints.",
      "summary": "arXiv:2404.",
      "summaryJa": "Arxiv:2404.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Graph is all you need? Lightweight data-agnostic neural architecture search without training",
      "titleJa": "Graph is all you need? lightweight data-agnostic neural architecture search without 学習",
      "link": "https://arxiv.org/abs/2405.01306",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.01306v2 Announce Type: replace \nAbstract: Neural architecture search (NAS) enables the automatic design of neural network models. However, training the candidates generated by the search algorithm for performance evaluation incurs considerable computational overhead. Our method, dubbed nasgraph, remarkably reduces the computational costs by converting neural architectures to graphs and using the average degree, a graph measure, as the proxy in lieu of the evaluation metric. Our training-free NAS method is data-agnostic and light-weight. It can find the best architecture among 200 randomly sampled architectures from NAS-Bench201 in 217 CPU seconds. Besides, our method is able to achieve competitive performance on various datasets including NASBench-101, NASBench-201, and NDS search spaces. We also demonstrate that nasgraph generalizes to more challenging tasks on Micro TransNAS-Bench-101.",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
      "titleJa": "Disentangling and integrating relational and sensory information in トランスフォーマー architectures",
      "link": "https://arxiv.org/abs/2405.16727",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.16727v3 Announce Type: replace \nAbstract: Relational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Recent empirical evidence shows that many existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: sensory information about the properties of individual objects, and relational information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate DAT on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency.",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Distributional Adversarial Loss",
      "titleJa": "Distributional adversarial loss",
      "link": "https://arxiv.org/abs/2406.03458",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.03458v2 Announce Type: replace \nAbstract: We initiate the study of a new notion of adversarial loss which we call distributional adversarial loss. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions, and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss. We show sample complexity bounds in the PAC-learning setting for our notion of adversarial loss. Our notion of adversarial loss contrasts the prior work on robust learning that considers a set of points, not distributions, as the perturbation set of each clean example. As an application of our approach, we show how to unify the two lines of work on randomized smoothing and robust learning in the PAC-learning setting and derive sample complexity bounds for randomized smoothing methods.\n  Furthermore, we investigate the role of randomness in achieving robustness against adversarial attacks. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks and show its effectiveness empirically.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Semantic-Aware Spectrum Sharing in Internet of Vehicles Based on Deep Reinforcement Learning",
      "titleJa": "Semantic-aware spectrum sharing in internet of vehicles based on deep 強化学習",
      "link": "https://arxiv.org/abs/2406.07213",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2406.07213v4 Announce Type: replace \nAbstract: This work aims to investigate semantic communication in high-speed mobile Internet of vehicles (IoV) environments, with a focus on the spectrum sharing between vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications. We specifically address spectrum scarcity and network traffic and then propose a semantic-aware spectrum sharing algorithm (SSS) based on the deep reinforcement learning (DRL) soft actor-critic (SAC) approach. Firstly, we delve into the extraction of semantic information. Secondly, we redefine metrics for semantic information in V2V and V2I spectrum sharing in IoV environments, introducing high-speed semantic spectrum efficiency (HSSE) and semantic transmission rate (HSR). Finally, we employ the SAC algorithm for decision optimization in V2V and V2I spectrum sharing based on semantic information. This optimization encompasses the optimal link of V2V and V2I sharing strategies, the transmission power for vehicles sending semantic information and the length of transmitted semantic symbols, aiming at maximizing HSSE of V2I and enhancing success rate of effective semantic information transmission (SRS) of V2V. Experimental results demonstrate that the SSS algorithm outperforms other baseline algorithms, including other traditional-communication-based spectrum sharing algorithms and spectrum sharing algorithm using other reinforcement learning approaches. The SSS algorithm exhibits a 15% increase in HSSE and approximately a 7% increase in SRS.",
      "summary": "arXiv:2406.",
      "summaryJa": "Arxiv:2406.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Resource Allocation for Twin Maintenance and Computing Task Processing in Digital Twin Vehicular Edge Computing Network",
      "titleJa": "Resource allocation for twin maintenance and computing task processing in digital twin vehicular エッジコンピューティング network",
      "link": "https://arxiv.org/abs/2407.07575",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2407.07575v2 Announce Type: replace \nAbstract: As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.",
      "summary": "arXiv:2407.",
      "summaryJa": "Arxiv:2407.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Mobility-Aware Federated Self-supervised Learning in Vehicular Network",
      "titleJa": "Mobility-aware federated self-supervised learning in vehicular network",
      "link": "https://arxiv.org/abs/2408.00256",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.00256v2 Announce Type: replace \nAbstract: Federated Learning (FL) is an advanced distributed machine learning approach, that protects the privacy of each vehicle by allowing the model to be trained on multiple devices simultaneously without the need to upload all data to a road side unit (RSU). This enables FL to handle scenarios with sensitive or widely distributed data. However, in these fields, it is well known that the labeling costs can be a significant expense, and models relying on labels are not suitable for these rapidly evolving fields especially in vehicular networks, or mobile internet of things (MIoT), where new data emerges constantly. To handle this issue, the self-supervised learning paves the way for training without labels. Additionally, for vehicles with high velocity, owing to blurred images, simple aggregation not only impacts the accuracy of the aggregated model but also reduces the convergence speed of FL. This paper proposes a FL algorithm based on image blur level to aggregation, called FLSimCo, which does not require labels and serves as a pre-training stage for self-supervised learning in the vehicular environment. Simulation results demonstrate that the proposed algorithm exhibits fast and stable convergence.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "DRL-Based Federated Self-Supervised Learning for Task Offloading and Resource Allocation in ISAC-Enabled Vehicle Edge Computing",
      "titleJa": "Drl-based federated self-supervised learning for task offloading and resource allocation in isac-enabled vehicle エッジコンピューティング",
      "link": "https://arxiv.org/abs/2408.14831",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.14831v2 Announce Type: replace \nAbstract: Intelligent Transportation Systems (ITS) leverage Integrated Sensing and Communications (ISAC) to enhance data exchange between vehicles and infrastructure in the Internet of Vehicles (IoV). This integration inevitably increases computing demands, risking real-time system stability. Vehicle Edge Computing (VEC) addresses this by offloading tasks to Road Side Unit (RSU), ensuring timely services. Our previous work FLSimCo algorithm, which uses local resources for Federated Self-Supervised Learning (SSL), though vehicles often can't complete all iterations task. Our improved algorithm offloads partial task to RSU and optimizes energy consumption by adjusting transmission power, CPU frequency, and task assignment ratios, balancing local and RSU-based training. Meanwhile, setting an offloading threshold further prevents inefficiencies. Simulation results show that the enhanced algorithm reduces energy consumption, improves offloading efficiency and the accuracy of Federated SSL.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Out-of-Distribution Detection: A Task-Oriented Survey of Recent Advances",
      "titleJa": "Out-of-distribution detection: a task-oriented survey of recent advances",
      "link": "https://arxiv.org/abs/2409.11884",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2409.11884v3 Announce Type: replace \nAbstract: Out-of-distribution (OOD) detection aims to detect test samples outside the training category space, which is an essential component in building reliable machine learning systems. Existing reviews on OOD detection primarily focus on method taxonomy, surveying the field by categorizing various approaches. However, many recent works concentrate on non-traditional OOD detection scenarios, such as test-time adaptation, multi-modal data sources and other novel contexts. In this survey, we uniquely review recent advances in OOD detection from the task-oriented perspective for the first time. According to the user's access to the model, that is, whether the OOD detection method is allowed to modify or retrain the model, we classify the methods as training-driven or training-agnostic. Besides, considering the rapid development of pre-trained models, large pre-trained model-based OOD detection is also regarded as an important category and discussed separately. Furthermore, we provide a discussion of the evaluation scenarios, a variety of applications, and several future research directions. We believe this survey with new taxonomy will benefit the proposal of new methods and the expansion of more practical scenarios. A curated list of related papers is provided in the Github repository: https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection.",
      "summary": "arXiv:2409.",
      "summaryJa": "Arxiv:2409.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Diffusion & Adversarial Schr\\\"odinger Bridges via Iterative Proportional Markovian Fitting",
      "titleJa": "Diffusion & adversarial schr\\\"odinger bridges via iterative proportional markovian fitting",
      "link": "https://arxiv.org/abs/2410.02601",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.02601v3 Announce Type: replace \nAbstract: The Iterative Markovian Fitting (IMF) procedure, which iteratively projects onto the space of Markov processes and the reciprocal class, successfully solves the Schr\\\"odinger Bridge (SB) problem. However, an efficient practical implementation requires a heuristic modification - alternating between fitting forward and backward time diffusion at each iteration. This modification is crucial for stabilizing training and achieving reliable results in applications such as unpaired domain translation. Our work reveals a close connection between the modified version of IMF and the Iterative Proportional Fitting (IPF) procedure - a foundational method for the SB problem, also known as Sinkhorn's algorithm. Specifically, we demonstrate that the heuristic modification of the IMF effectively integrates both IMF and IPF procedures. We refer to this combined approach as the Iterative Proportional Markovian Fitting (IPMF) procedure. Through theoretical and empirical analysis, we establish the convergence of IPMF procedure under various settings, contributing to developing a unified framework for solving SB problems. Moreover, from a practical standpoint, the IPMF procedure enables a flexible trade-off between image similarity and generation quality, offering a new mechanism for tailoring models to specific tasks.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Zero-shot Class Unlearning via Layer-wise Relevance Analysis and Neuronal Path Perturbation",
      "titleJa": "Zero-shot class unlearning via layer-wise relevance analysis and neuronal path perturbation",
      "link": "https://arxiv.org/abs/2410.23693",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.23693v2 Announce Type: replace \nAbstract: In the rapid advancement of artificial intelligence, privacy protection has become crucial, giving rise to machine unlearning. Machine unlearning is a technique that removes specific data influences from trained models without the need for extensive retraining. However, it faces several key challenges, including accurately implementing unlearning, ensuring privacy protection during the unlearning process, and achieving effective unlearning without significantly compromising model performance. This paper presents a novel approach to machine unlearning by employing Layer-wise Relevance Analysis and Neuronal Path Perturbation. We address three primary challenges: the lack of detailed unlearning principles, privacy guarantees in zero-shot unlearning scenario, and the balance between unlearning effectiveness and model utility. Our method balances machine unlearning performance and model utility by identifying and perturbing highly relevant neurons, thereby achieving effective unlearning. By using data not present in the original training set during the unlearning process, we satisfy the zero-shot unlearning scenario and ensure robust privacy protection. Experimental results demonstrate that our approach effectively removes targeted data from the target unlearning model while maintaining the model's utility, offering a practical solution for privacy-preserving machine learning.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Hopfield-Fenchel-Young Networks: A Unified Framework for Associative Memory Retrieval",
      "titleJa": "Hopfield-fenchel-young networks: a unified framework for associative memory retrieval",
      "link": "https://arxiv.org/abs/2411.08590",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.08590v2 Announce Type: replace \nAbstract: Associative memory models, such as Hopfield networks and their modern variants, have garnered renewed interest due to advancements in memory capacity and connections with self-attention in transformers. In this work, we introduce a unified framework-Hopfield-Fenchel-Young networks-which generalizes these models to a broader family of energy functions. Our energies are formulated as the difference between two Fenchel-Young losses: one, parameterized by a generalized entropy, defines the Hopfield scoring mechanism, while the other applies a post-transformation to the Hopfield output. By utilizing Tsallis and norm entropies, we derive end-to-end differentiable update rules that enable sparse transformations, uncovering new connections between loss margins, sparsity, and exact retrieval of single memory patterns. We further extend this framework to structured Hopfield networks using the SparseMAP transformation, allowing the retrieval of pattern associations rather than a single pattern. Our framework unifies and extends traditional and modern Hopfield networks and provides an energy minimization perspective for widely used post-transformations like $\\ell_2$-normalization and layer normalization-all through suitable choices of Fenchel-Young losses and by using convex analysis as a building block. Finally, we validate our Hopfield-Fenchel-Young networks on diverse memory recall tasks, including free and sequential recall. Experiments on simulated data, image retrieval, multiple instance learning, and text rationalization demonstrate the effectiveness of our approach.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Competing Bandits in Decentralized Contextual Matching Markets",
      "titleJa": "Competing bandits in decentralized contextual matching markets",
      "link": "https://arxiv.org/abs/2411.11794",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.11794v2 Announce Type: replace \nAbstract: Sequential learning in a multi-agent resource constrained matching market has received significant interest in the past few years. We study decentralized learning in two-sided matching markets where the demand side (aka players or agents) competes for the supply side (aka arms) with potentially time-varying preferences to obtain a stable match. Motivated by the linear contextual bandit framework, we assume that for each agent, an arm-mean may be represented by a linear function of a known feature vector and an unknown (agent-specific) parameter. Moreover, the preferences over arms depend on a latent environment in each round, where the latent environment varies across rounds in a non-stationary manner. We propose learning algorithms to identify the latent environment and obtain stable matchings simultaneously. Our proposed algorithms achieve instance-dependent logarithmic regret, scaling independently of the number of arms, and hence applicable for a large market.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Bridging Text and Crystal Structures: Literature-driven Contrastive Learning for Materials Science",
      "titleJa": "Bridging text and crystal structures: literature-driven contrastive learning for materials science",
      "link": "https://arxiv.org/abs/2501.12919",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.12919v2 Announce Type: replace \nAbstract: Understanding structure-property relationships is an essential yet challenging aspect of materials discovery and development. To facilitate this process, recent studies in materials informatics have sought latent embedding spaces of crystal structures to capture their similarities based on properties and functionalities. However, abstract feature-based embedding spaces are human-unfriendly and prevent intuitive and efficient exploration of the vast materials space. Here we introduce Contrastive Language--Structure Pre-training (CLaSP), a learning paradigm for constructing crossmodal embedding spaces between crystal structures and texts. CLaSP aims to achieve material embeddings that 1) capture property- and functionality-related similarities between crystal structures and 2) allow intuitive retrieval of materials via user-provided description texts as queries. To compensate for the lack of sufficient datasets linking crystal structures with textual descriptions, CLaSP leverages a dataset of over 400,000 published crystal structures and corresponding publication records, including paper titles and abstracts, for training. We demonstrate the effectiveness of CLaSP through text-based crystal structure screening and embedding space visualization.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The Exploration of Error Bounds in Classification with Noisy Labels",
      "titleJa": "The exploration of error bounds in classification with noisy labels",
      "link": "https://arxiv.org/abs/2501.15163",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.15163v2 Announce Type: replace \nAbstract: Numerous studies have shown that label noise can lead to poor generalization performance, negatively affecting classification accuracy. Therefore, understanding the effectiveness of classifiers trained using deep neural networks in the presence of noisy labels is of considerable practical significance. In this paper, we focus on the error bounds of excess risks for classification problems with noisy labels within deep learning frameworks. We derive error bounds for the excess risk, decomposing it into statistical error and approximation error. To handle statistical dependencies (e.g., mixing sequences), we employ an independent block construction to bound the error, leveraging techniques for dependent processes. For the approximation error, we establish these theoretical results to the vector-valued setting, where the output space consists of $K$-dimensional unit vectors. Finally, under the low-dimensional manifold hypothesis, we further refine the approximation error to mitigate the impact of high-dimensional input spaces.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans",
      "titleJa": "Flow matching: markov kernels, stochastic processes and transport plans",
      "link": "https://arxiv.org/abs/2501.16839",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.16839v4 Announce Type: replace \nAbstract: Among generative neural models, flow matching techniques stand out for their simple applicability and good scaling properties. Here, velocity fields of curves connecting a simple latent and a target distribution are learned. Then the corresponding ordinary differential equation can be used to sample from a target distribution, starting in samples from the latent one. This paper reviews from a mathematical point of view different techniques to learn the velocity fields of absolutely continuous curves in the Wasserstein geometry. We show how the velocity fields can be characterized and learned via i) transport plans (couplings) between latent and target distributions, ii) Markov kernels and iii) stochastic processes, where the latter two include the coupling approach, but are in general broader. Besides this main goal, we show how flow matching can be used for solving Bayesian inverse problems, where the definition of conditional Wasserstein distances plays a central role. Finally, we briefly address continuous normalizing flows and score matching techniques, which approach the learning of velocity fields of curves from other directions.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "DAL: A Practical Prior-Free Black-Box Framework for Non-Stationary Bandit Environments",
      "titleJa": "Dal: a practical prior-free black-box framework for non-stationary bandit environments",
      "link": "https://arxiv.org/abs/2501.19401",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2501.19401v3 Announce Type: replace \nAbstract: We introduce a practical, black-box framework termed Detection Augmenting Learning (DAL) for the problem of non-stationary bandits without prior knowledge of the underlying non-stationarity. DAL is modular, accepting any stationary bandit algorithm as input and augmenting it with a change detector. Our approach is applicable to all common parametric and non-parametric bandit variants. Extensive experimentation demonstrates that DAL consistently surpasses current state-of-the-art methods across diverse non-stationary scenarios, including synthetic benchmarks and real-world datasets, underscoring its versatility and scalability. We provide theoretical insights into DAL's strong empirical performance on piecewise stationary and drift settings, complemented by thorough experimental validation.",
      "summary": "arXiv:2501.",
      "summaryJa": "Arxiv:2501.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning Model Successors",
      "titleJa": "Learning モデル successors",
      "link": "https://arxiv.org/abs/2502.00197",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.00197v2 Announce Type: replace \nAbstract: The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). There has been a growing focus on generalization from easy to hard, where a progression of difficulty implicitly governs the direction of domain shifts. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. We argue that the unifying theme is induction -- based on finite samples observed in training, a learner should infer an inductive principle that applies in an unbounded manner. This work formalizes the notion of inductive generalization along a difficulty progression and argues that our path ahead lies in transforming the learning paradigm. We attempt to make inroads by proposing a novel learning paradigm, Inductive Learning, which involves a central concept called model successors. We outline practical steps to adapt well-established techniques towards learning model successors. This work calls for restructuring of the research discussion around induction and generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Harnessing omnipresent oscillator networks as computational resource",
      "titleJa": "Harnessing omnipresent oscillator networks as computational resource",
      "link": "https://arxiv.org/abs/2502.04818",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.04818v3 Announce Type: replace \nAbstract: Nature is pervaded with oscillatory dynamics. In networks of coupled oscillators patterns can arise when the system synchronizes to an external input. Hence, these networks provide processing and memory of input. We present a universal framework for harnessing oscillator networks as computational resource. This computing framework is introduced by the ubiquitous model for phase-locking, the Kuramoto model. We force the Kuramoto model by a nonlinear target-system, then after substituting the target-system with a trained feedback-loop it emulates the target-system. Our results are two-fold. Firstly, the trained network inherits performance properties of the Kuramoto model, where all-to-all coupling is performed in linear time with respect to the number of nodes and parameters for synchronization are abundant. The latter implies that the network is generically successful since the system learns via sychronization. Secondly, the learning capabilities of the oscillator network, which describe a type of collective intelligence, can be explained using Kuramoto model's order parameter. In summary, this work provides the foundation for utilizing nature's oscillator networks as a new class of information processing systems.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension",
      "titleJa": "Discrepancies are virtue: weak-to-strong generalization through lens of intrinsic dimension",
      "link": "https://arxiv.org/abs/2502.05075",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.05075v5 Announce Type: replace \nAbstract: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student-weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\\mathcal{V}_s, \\mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\\mathcal{V}_s \\cap \\mathcal{V}_w$, while reduced by a factor of $\\mathrm{dim}(\\mathcal{V}_s)/N$ in the subspace of discrepancy $\\mathcal{V}_w \\setminus \\mathcal{V}_s$ with $N$ pseudo-labels for W2S. Our analysis further casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported by experiments on synthetic regression problems, as well as real vision and NLP tasks.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "When and How Does CLIP Enable Domain and Compositional Generalization?",
      "titleJa": "When and how does clip enable domain and compositional generalization?",
      "link": "https://arxiv.org/abs/2502.09507",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.09507v2 Announce Type: replace \nAbstract: The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Optimistically Optimistic Exploration for Provably Efficient Infinite-Horizon Reinforcement and Imitation Learning",
      "titleJa": "Optimistically optimistic exploration for provably efficient infinite-horizon reinforcement and imitation learning",
      "link": "https://arxiv.org/abs/2502.13900",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.13900v2 Announce Type: replace \nAbstract: We study the problem of reinforcement learning in infinite-horizon discounted linear Markov decision processes (MDPs), and propose the first computationally efficient algorithm achieving rate-optimal regret guarantees in this setting. Our main idea is to combine two classic techniques for optimistic exploration: additive exploration bonuses applied to the reward function, and artificial transitions made to an absorbing state with maximal return. We show that, combined with a regularized approximate dynamic-programming scheme, the resulting algorithm achieves a regret of order $\\tilde{\\mathcal{O}} (\\sqrt{d^3 (1 - \\gamma)^{- 7 / 2} T})$, where $T$ is the total number of sample transitions, $\\gamma \\in (0,1)$ is the discount factor, and $d$ is the feature dimensionality. The results continue to hold against adversarial reward sequences, enabling application of our method to the problem of imitation learning in linear MDPs, where we achieve state-of-the-art results.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records",
      "titleJa": "Labtop: a unified モデル for lab test outcome prediction on electronic health records",
      "link": "https://arxiv.org/abs/2502.14259",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.14259v4 Announce Type: replace \nAbstract: Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Near Optimal Decision Trees in a SPLIT Second",
      "titleJa": "Near optimal decision trees in a split second",
      "link": "https://arxiv.org/abs/2502.15988",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.15988v2 Announce Type: replace \nAbstract: Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Sustainable Greenhouse Microclimate Modeling: A Comparative Analysis of Recurrent and Graph Neural Networks",
      "titleJa": "Sustainable greenhouse microclimate modeling: a comparative analysis of recurrent and graph neural networks",
      "link": "https://arxiv.org/abs/2502.17371",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.17371v4 Announce Type: replace \nAbstract: The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both environmental dependencies and their directionality. We benchmark RNNs against directed STGNNs on two 15-min-resolution datasets from Volos (Greece): a six-variable 2020 installation and a more complex eight-variable greenhouse monitored in autumn 2024. In the simpler 2020 case the RNN attains near-perfect accuracy, outperforming the STGNN. When additional drivers are available in 2024, the STGNN overtakes the RNN ($R^{2}=0.905$ vs $0.740$), demonstrating that explicitly modelling directional dependencies becomes critical as interaction complexity grows. These findings indicate when graph-based models are warranted and provide a stepping-stone toward digital twins that jointly optimise crop yield and PV power in agrivoltaic greenhouses.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Learning Topology Actions for Power Grid Control: A Graph-Based Soft-Label Imitation Learning Approach",
      "titleJa": "Learning topology actions for power grid control: a graph-based soft-label imitation learning approach",
      "link": "https://arxiv.org/abs/2503.15190",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.15190v2 Announce Type: replace \nAbstract: The rising proportion of renewable energy in the electricity mix introduces significant operational challenges for power grid operators. Effective power grid management demands adaptive decision-making strategies capable of handling dynamic conditions. With the increase in complexity, more and more Deep Learning (DL) approaches have been proposed to find suitable grid topologies for congestion management. In this work, we contribute to this research by introducing a novel Imitation Learning (IL) approach that leverages soft labels derived from simulated topological action outcomes, thereby capturing multiple viable actions per state. Unlike traditional IL methods that rely on hard labels to enforce a single optimal action, our method constructs soft labels that capture the effectiveness of actions that prove suitable in resolving grid congestion. To further enhance decision-making, we integrate Graph Neural Networks (GNNs) to encode the structural properties of power grids, ensuring that the topology-aware representations contribute to better agent performance. Our approach significantly outperforms its hard-label counterparts as well as state-of-the-art Deep Reinforcement Learning (DRL) baseline agents. Most notably, it achieves a 17% better performance compared to the greedy expert agent from which the imitation targets were derived.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Noise Resilient Over-The-Air Federated Learning In Heterogeneous Wireless Networks",
      "titleJa": "Noise resilient over-the-air federated learning in heterogeneous wireless networks",
      "link": "https://arxiv.org/abs/2503.19549",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.19549v2 Announce Type: replace \nAbstract: In 6G wireless networks, Artificial Intelligence (AI)-driven applications demand the adoption of Federated Learning (FL) to enable efficient and privacy-preserving model training across distributed devices. Over-The-Air Federated Learning (OTA-FL) exploits the superposition property of multiple access channels, allowing edge users in 6G networks to efficiently share spectral resources and perform low-latency global model aggregation. However, these advantages come with challenges, as traditional OTA-FL techniques suffer due to the joint effects of Additive White Gaussian Noise (AWGN) at the server, fading, and both data and system heterogeneity at the participating edge devices. In this work, we propose the novel Noise Resilient Over-the-Air Federated Learning (NoROTA-FL) framework to jointly tackle these challenges in federated wireless networks. In NoROTA-FL, the local optimization problems find controlled inexact solutions, which manifests as an additional proximal constraint at the clients. This approach provides robustness against straggler-induced partial work, heterogeneity, noise, and fading. From a theoretical perspective, we leverage the zeroth- and first-order inexactness and establish convergence guarantees for non-convex optimization problems in the presence of heterogeneous data and varying system capabilities. Experimentally, we validate NoROTA-FL on real-world datasets, including FEMNIST, CIFAR10, and CIFAR100, demonstrating its robustness in noisy and heterogeneous environments. Compared to state-of-the-art baselines such as COTAF and FedProx, NoROTA-FL achieves significantly more stable convergence and higher accuracy, particularly in the presence of stragglers.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion",
      "titleJa": "Scalegnn: towards scalable graph neural networks via adaptive high-order neighboring feature fusion",
      "link": "https://arxiv.org/abs/2504.15920",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.15920v3 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have demonstrated impressive performance across diverse graph-based tasks by leveraging message passing to capture complex node relationships. However, when applied to large-scale real-world graphs, GNNs face two major challenges: First, it becomes increasingly difficult to ensure both scalability and efficiency, as the repeated aggregation of large neighborhoods leads to significant computational overhead; Second, the over-smoothing problem arises, where excessive or deep propagation makes node representations indistinguishable, severely hindering model expressiveness. To tackle these issues, we propose ScaleGNN, a novel framework that adaptively fuses multi-level graph features for both scalable and effective graph learning. ScaleGNN first constructs per-order neighbor matrices that capture only the exclusive structural information at each hop, avoiding the redundancy of conventional aggregation. A learnable fusion module then selectively integrates these features, emphasizing the most informative high-order neighbors. To further reduce redundancy and over-smoothing, we introduce a Local Contribution Score (LCS)-based masking mechanism to filter out less relevant high-order neighbors, ensuring that only the most meaningful information is aggregated. In addition, a task-aware feature fusion strategy dynamically balances low- and high-order information, preserving both local detail and global context without incurring excessive complexity. Extensive experiments on real-world datasets demonstrate that ScaleGNN consistently outperforms state-of-the-art GNNs in both predictive accuracy and computational efficiency, highlighting its practical value for large-scale graph learning.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence",
      "titleJa": "Conditional front-door adjustment for heterogeneous treatment assignment effect estimation under non-adherence",
      "link": "https://arxiv.org/abs/2505.05677",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.05677v3 Announce Type: replace \nAbstract: Estimates of heterogeneous treatment assignment effects can inform treatment decisions. Under the presence of non-adherence (e.g., patients do not adhere to their assigned treatment), both the standard backdoor adjustment (SBD) and the conditional front-door adjustment (CFD) can recover unbiased estimates of the treatment assignment effects. However, the estimation variance of these approaches may vary widely across settings, which remains underexplored in the literature. In this work, we demonstrate theoretically and empirically that CFD yields lower-variance estimates than SBD when the true effect of treatment assignment is small (i.e., assigning an intervention leads to small changes in patients' future outcome). Additionally, since CFD requires estimating multiple nuisance parameters, we introduce LobsterNet, a multi-task neural network that implements CFD with joint modeling of the nuisance parameters. Empirically, LobsterNet reduces estimation error across several semi-synthetic and real-world datasets compared to baselines. Our findings suggest CFD with shared nuisance parameter modeling can improve treatment assignment effect estimation under non-adherence.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Variance-Based Defense Against Blended Backdoor Attacks",
      "titleJa": "Variance-based defense against blended backdoor attacks",
      "link": "https://arxiv.org/abs/2506.01444",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.01444v2 Announce Type: replace \nAbstract: Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Temporal horizons in forecasting: a performance-learnability trade-off",
      "titleJa": "Temporal horizons in forecasting: a performance-learnability trade-off",
      "link": "https://arxiv.org/abs/2506.03889",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.03889v2 Announce Type: replace \nAbstract: When training autoregressive models to forecast dynamical systems, a critical question arises: how far into the future should the model be trained to predict? Too short a horizon may miss long-term trends, while too long a horizon can impede convergence due to accumulating prediction errors. In this work, we formalize this trade-off by analyzing how the geometry of the loss landscape depends on the training horizon. We prove that for chaotic systems, the loss landscape's roughness grows exponentially with the training horizon, while for limit cycles, it grows linearly, making long-horizon training inherently challenging. However, we also show that models trained on long horizons generalize well to short-term forecasts, whereas those trained on short horizons suffer exponentially (resp. linearly) worse long-term predictions in chaotic (resp. periodic) systems. We validate our theory through numerical experiments and discuss practical implications for selecting training horizons. Our results provide a principled foundation for hyperparameter optimization in autoregressive forecasting models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Effective Data Pruning through Score Extrapolation",
      "titleJa": "Effective data pruning through score extrapolation",
      "link": "https://arxiv.org/abs/2506.09010",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.09010v2 Announce Type: replace \nAbstract: Training advanced machine learning models demands massive datasets, resulting in prohibitive computational costs. To address this challenge, data pruning techniques identify and remove redundant training samples while preserving model performance. Yet, existing pruning techniques predominantly require a full initial training pass to identify removable samples, negating any efficiency benefits for single training runs. To overcome this limitation, we introduce a novel importance score extrapolation framework that requires training on only a small subset of data. We present two initial approaches in this framework - k-nearest neighbors and graph neural networks - to accurately predict sample importance for the entire dataset using patterns learned from this minimal subset. We demonstrate the effectiveness of our approach for 2 state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training paradigms (supervised, unsupervised, and adversarial). Our results indicate that score extrapolation is a promising direction to scale expensive score calculation methods, such as pruning, data attribution, or other tasks.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity",
      "titleJa": "Kces: 学習-free defense for robust graph neural networks via kernel complexity",
      "link": "https://arxiv.org/abs/2506.11611",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.11611v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have achieved impressive success across a wide range of graph-based tasks, yet they remain highly vulnerable to small, imperceptible perturbations and adversarial attacks. Although numerous defense methods have been proposed to address these vulnerabilities, many rely on heuristic metrics, overfit to specific attack patterns, and suffer from high computational complexity. In this paper, we propose Kernel Complexity-Based Edge Sanitization (KCES), a training-free, model-agnostic defense framework. KCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the graph's Gram matrix that characterizes GNN generalization via its test error bound. Building on GKC, we define a KC score for each edge, measuring the change in GKC when the edge is removed. Edges with high KC scores, typically introduced by adversarial perturbations, are pruned to mitigate their harmful effects, thereby enhancing GNNs' robustness. KCES can also be seamlessly integrated with existing defense strategies as a plug-and-play module without requiring training. Theoretical analysis and extensive experiments demonstrate that KCES consistently enhances GNN robustness, outperforms state-of-the-art baselines, and amplifies the effectiveness of existing defenses, offering a principled and efficient solution for securing GNNs.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "A Comprehensive Survey on Continual Learning in Generative Models",
      "titleJa": "A comprehensive survey on continual learning in generative models",
      "link": "https://arxiv.org/abs/2506.13045",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13045v3 Announce Type: replace \nAbstract: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models remain fundamentally constrained by catastrophic forgetting - a persistent challenge where adapting to new tasks typically leads to significant degradation in performance on previously learned tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative models in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative models, including large language models, multimodal large language models, vision language action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, offering deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CoIFNet: A Unified Framework for Multivariate Time Series Forecasting with Missing Values",
      "titleJa": "Coifnet: a unified framework for multivariate time series forecasting with missing values",
      "link": "https://arxiv.org/abs/2506.13064",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13064v2 Announce Type: replace \nAbstract: Multivariate time series forecasting (MTSF) is a critical task with broad applications in domains such as meteorology, transportation, and economics. Nevertheless, pervasive missing values caused by sensor failures or human errors significantly degrade forecasting accuracy. Prior efforts usually employ an impute-then-forecast paradigm, leading to suboptimal predictions due to error accumulation and misaligned objectives between the two stages. To address this challenge, we propose the Collaborative Imputation-Forecasting Network (CoIFNet), a novel framework that unifies imputation and forecasting to achieve robust MTSF in the presence of missing values. Specifically, CoIFNet takes the observed values, mask matrix and timestamp embeddings as input, processing them sequentially through the Cross-Timestep Fusion (CTF) and Cross-Variate Fusion (CVF) modules to capture temporal dependencies that are robust to missing values. We provide theoretical justifications on how our CoIFNet learning objective improves the performance bound of MTSF with missing values. Through extensive experiments on challenging MSTF benchmarks, we demonstrate the effectiveness and computational efficiency of our proposed approach across diverse missing-data scenarios, e.g., CoIFNet outperforms the state-of-the-art method by $\\underline{\\textbf{24.40}}$% ($\\underline{\\textbf{23.81}}$%) at a point (block) missing rate of 0.6, while improving memory and time efficiency by $\\underline{\\boldsymbol{4.3\\times}}$ and $\\underline{\\boldsymbol{2.1\\times}}$, respectively. Our code is available at: https://github.com/KaiTang-eng/CoIFNet.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations",
      "titleJa": "Knowledge distillation framework for accelerating high-accuracy ニューラルネットワーク-based molecular dynamics simulations",
      "link": "https://arxiv.org/abs/2506.15337",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.15337v2 Announce Type: replace \nAbstract: Neural network potentials (NNPs) offer a powerful alternative to traditional force fields for molecular dynamics (MD) simulations. Accurate and stable MD simulations, crucial for evaluating material properties, require training data encompassing both low-energy stable structures and high-energy structures. Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as a teacher model to generate training data for a student model. However, in material-specific models, this fine-tuning process increases energy barriers, making it difficult to create training data containing high-energy structures. To address this, we propose a novel KD framework that leverages a non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy landscape facilitates the exploration of a wider range of structures, including the high-energy structures crucial for stable MD simulations. Our framework employs a two-stage training process: first, the student NNP is trained with a dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate the effectiveness of our framework by applying it to both organic (polyethylene glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving comparable or superior accuracy in reproducing physical properties compared to existing methods. Importantly, our method reduces the number of expensive DFT calculations by 10x compared to existing NNP generation methods, without sacrificing accuracy. Furthermore, the resulting student NNP achieves up to 106x speedup in inference compared to the teacher NNP, enabling significantly faster and more efficient MD simulations.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Conformal prediction for frequency-severity modeling",
      "titleJa": "Conformal prediction for frequency-severity modeling",
      "link": "https://arxiv.org/abs/2307.13124",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2307.13124v4 Announce Type: replace-cross \nAbstract: We present a model-agnostic framework for the construction of prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The framework effectiveness is showcased with simulated and real datasets using classical parametric models and contemporary machine learning methods. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction algorithm, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set in the conformal procedure.",
      "summary": "arXiv:2307.",
      "summaryJa": "Arxiv:2307.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Interventions Against Machine-Assisted Statistical Discrimination",
      "titleJa": "Interventions against machine-assisted statistical discrimination",
      "link": "https://arxiv.org/abs/2310.04585",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2310.04585v4 Announce Type: replace-cross \nAbstract: I study statistical discrimination driven by verifiable beliefs, such as those generated by machine learning, rather than by humans. When beliefs are verifiable, interventions against statistical discrimination can move beyond simple, belief-free designs like affirmative action, to more sophisticated ones, that constrain decision makers based on what they are thinking. I design a belief-contingent intervention I call common identity. I show that it is effective at eliminating equilibrium statistical discrimination, even when training data exhibit the various statistical biases that often plague algorithmic decision problems.",
      "summary": "arXiv:2310.",
      "summaryJa": "Arxiv:2310.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Weakly Supervised Point Cloud Segmentation via Conservative Propagation of Scene-level Labels",
      "titleJa": "Weakly supervised point クラウド segmentation via conservative propagation of scene-level labels",
      "link": "https://arxiv.org/abs/2312.06799",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2312.06799v2 Announce Type: replace-cross \nAbstract: We propose a weakly supervised semantic segmentation method for point clouds that predicts \"per-point\" labels from just \"whole-scene\" annotations. The key challenge here is the discrepancy between the target of dense per-point semantic prediction and training losses derived from only scene-level labels. To address this, in addition to the typical weakly-supervised setup that supervises all points with the scene label, we propose to conservatively propagate the scene-level labels to points selectively. Specifically, we over-segment point cloud features via unsupervised clustering in the entire dataset and form primitives. We then associate scene-level labels with primitives through bipartite matching. Then, we allow labels to pass through this primitive-label relationship, while further encouraging features to form narrow clusters around the primitives. Importantly, through bipartite matching, this additional pathway through which labels flow, only propagates scene labels to the most relevant points, reducing the potential negative impact caused by the global approach that existing methods take. We evaluate our method on ScanNet and S3DIS datasets, outperforming the state of the art by a large margin.",
      "summary": "arXiv:2312.",
      "summaryJa": "Arxiv:2312.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Stable Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders",
      "titleJa": "Stable learning using spiking neural networks equipped with affine encoders and decoders",
      "link": "https://arxiv.org/abs/2404.04549",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2404.04549v3 Announce Type: replace-cross \nAbstract: We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder; we refer to these as affine spiking neural networks. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights enables a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these affine spiking neural networks to standard machine learning benchmarks and reach competitive results. Finally, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.",
      "summary": "arXiv:2404.",
      "summaryJa": "Arxiv:2404.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Complexity of Injectivity and Verification of ReLU Neural Networks",
      "titleJa": "Complexity of injectivity and verification of relu neural networks",
      "link": "https://arxiv.org/abs/2405.19805",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2405.19805v3 Announce Type: replace-cross \nAbstract: Neural networks with ReLU activation play a key role in modern machine learning. Understanding the functions represented by ReLU networks is a major topic in current research as this enables a better interpretability of learning processes. Injectivity of a function computed by a ReLU network, that is, the question if different inputs to the network always lead to different outputs, plays a crucial role whenever invertibility of the function is required, such as, e.g., for inverse problems or generative models. The exact computational complexity of deciding injectivity was recently posed as an open problem (Puthawala et al. [JMLR 2022]). We answer this question by proving coNP-completeness. On the positive side, we show that the problem for a single ReLU-layer is still tractable for small input dimension; more precisely, we present a parameterized algorithm which yields fixed-parameter tractability with respect to the input dimension. In addition, we study the network verification problem which is to verify that certain inputs only yield specific outputs. This is of great importance since neural networks are increasingly used in safety-critical systems. We prove that network verification is coNP-hard for a general class of input domains. Our results also exclude constant-factor polynomial-time approximations for the maximum of a function computed by a ReLU network. In this context, we also characterize surjectivity of functions computed by ReLU networks with one-dimensional output which turns out to be the complement of a basic network verification task. We reveal interesting connections to computational convexity by formulating the surjectivity problem as a zonotope containment problem",
      "summary": "arXiv:2405.",
      "summaryJa": "Arxiv:2405.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Faster Stochastic Optimization with Arbitrary Delays via Asynchronous Mini-Batching",
      "titleJa": "Faster stochastic optimization with arbitrary delays via asynchronous mini-batching",
      "link": "https://arxiv.org/abs/2408.07503",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2408.07503v2 Announce Type: replace-cross \nAbstract: We consider the problem of asynchronous stochastic optimization, where an optimization algorithm makes updates based on stale stochastic gradients of the objective that are subject to an arbitrary (possibly adversarial) sequence of delays. We present a procedure which, for any given $q \\in (0,1]$, transforms any standard stochastic first-order method to an asynchronous method with convergence guarantee depending on the $q$-quantile delay of the sequence. This approach leads to convergence rates of the form $O(\\tau_q/qT+\\sigma/\\sqrt{qT})$ for non-convex and $O(\\tau_q^2/(q T)^2+\\sigma/\\sqrt{qT})$ for convex smooth problems, where $\\tau_q$ is the $q$-quantile delay, generalizing and improving on existing results that depend on the average delay. We further show a method that automatically adapts to all quantiles simultaneously, without any prior knowledge of the delays, achieving convergence rates of the form $O(\\inf_{q} \\tau_q/qT+\\sigma/\\sqrt{qT})$ for non-convex and $O(\\inf_{q} \\tau_q^2/(q T)^2+\\sigma/\\sqrt{qT})$ for convex smooth problems. Our technique is based on asynchronous mini-batching with a careful batch-size selection and filtering of stale gradients.",
      "summary": "arXiv:2408.",
      "summaryJa": "Arxiv:2408.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Tuning-Free Coreset Markov Chain Monte Carlo via Hot DoG",
      "titleJa": "Tuning-free coreset markov chain monte carlo via hot dog",
      "link": "https://arxiv.org/abs/2410.18973",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2410.18973v2 Announce Type: replace-cross \nAbstract: A Bayesian coreset is a small, weighted subset of a data set that replaces the full data during inference to reduce computational cost. The state-of-the-art coreset construction algorithm, Coreset Markov chain Monte Carlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the coreset posterior to train the coreset weights via stochastic gradient optimization. However, the quality of the constructed coreset, and thus the quality of its posterior approximation, is sensitive to the stochastic optimization learning rate. In this work, we propose a learning-rate-free stochastic gradient optimization procedure, Hot-start Distance over Gradient (Hot DoG), for training coreset weights in Coreset MCMC without user tuning effort. We provide a theoretical analysis of the convergence of the coreset weights produced by Hot DoG. We also provide empirical results demonstrate that Hot DoG provides higher quality posterior approximations than other learning-rate-free stochastic gradient methods, and performs competitively to optimally-tuned ADAM.",
      "summary": "arXiv:2410.",
      "summaryJa": "Arxiv:2410.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "The learned range test method for the inverse inclusion problem",
      "titleJa": "The learned range test method for the inverse inclusion problem",
      "link": "https://arxiv.org/abs/2411.00463",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2411.00463v2 Announce Type: replace-cross \nAbstract: We consider the inverse problem consisting of the reconstruction of an inclusion $B$ contained in a bounded domain $\\Omega\\subset\\mathbb{R}^d$ from a single pair of Cauchy data $(u|_{\\partial\\Omega},\\partial_\\nu u|_{\\partial\\Omega})$, where $\\Delta u=0$ in $\\Omega\\setminus\\overline B$ and $u=0$ on $\\partial B$. We show that the reconstruction algorithm based on the range test, a domain sampling method, can be written as a neural network with a specific architecture. We propose to learn the weights of this network in the framework of supervised learning, and to combine it with a pre-trained classifier, with the purpose of distinguishing the inclusions based on their distance from the boundary. The numerical simulations show that this learned range test method provides accurate and stable reconstructions of polygonal inclusions. Furthermore, the results are superior to those obtained with the standard range test method (without learning) and with an end-to-end fully connected deep neural network, a purely data-driven method.",
      "summary": "arXiv:2411.",
      "summaryJa": "Arxiv:2411.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Belted and Ensembled Neural Network for Linear and Nonlinear Sufficient Dimension Reduction",
      "titleJa": "Belted and ensembled ニューラルネットワーク for linear and nonlinear sufficient dimension reduction",
      "link": "https://arxiv.org/abs/2412.08961",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2412.08961v2 Announce Type: replace-cross \nAbstract: We introduce a unified, flexible, and easy-to-implement framework of sufficient dimension reduction that can accommodate both linear and nonlinear dimension reduction, and both the conditional distribution and the conditional mean as the targets of estimation. This unified framework is achieved by a specially structured neural network -- the Belted and Ensembled Neural Network (BENN) -- that consists of a narrow latent layer, which we call the belt, and a family of transformations of the response, which we call the ensemble. By strategically placing the belt at different layers of the neural network, we can achieve linear or nonlinear sufficient dimension reduction, and by choosing the appropriate transformation families, we can achieve dimension reduction for the conditional distribution or the conditional mean. Moreover, thanks to the advantage of the neural network, the method is very fast to compute, overcoming a computation bottleneck of the traditional sufficient dimension reduction estimators, which involves the inversion of a matrix of dimension either p or n. We develop the algorithm and convergence rate of our method, compare it with existing sufficient dimension reduction methods, and apply it to two data examples.",
      "summary": "arXiv:2412.",
      "summaryJa": "Arxiv:2412.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Deep learning joint extremes of metocean variables using the SPAR model",
      "titleJa": "ディープラーニング joint extremes of metocean variables using the spar モデル",
      "link": "https://arxiv.org/abs/2412.15808",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2412.15808v2 Announce Type: replace-cross \nAbstract: This paper presents a novel deep learning framework for estimating multivariate joint extremes of metocean variables, based on the Semi-Parametric Angular-Radial (SPAR) model. When considered in polar coordinates, the problem of modelling multivariate extremes is transformed to one of modelling an angular density, and the tail of a univariate radial variable conditioned on angle. In the SPAR approach, the tail of the radial variable is modelled using a generalised Pareto (GP) distribution, providing a natural extension of univariate extreme value theory to the multivariate setting. In this work, we show how the method can be applied in higher dimensions, using a case study for five metocean variables: wind speed, wind direction, wave height, wave period, and wave direction. The angular variable is modelled using a kernel density method, while the parameters of the GP model are approximated using fully-connected deep neural networks. Our approach provides great flexibility in the dependence structures that can be represented, together with computationally efficient routines for training the model. Furthermore, the application of the method requires fewer assumptions about the underlying distribution(s) compared to existing approaches, and an asymptotically justified means for extrapolating outside the range of observations. Using various diagnostic plots, we show that the fitted models provide a good description of the joint extremes of the metocean variables considered.",
      "summary": "arXiv:2412.",
      "summaryJa": "Arxiv:2412.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Low-Resource Video Super-Resolution using Memory, Wavelets, and Deformable Convolutions",
      "titleJa": "Low-resource video super-resolution using memory, wavelets, and deformable convolutions",
      "link": "https://arxiv.org/abs/2502.01816",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.01816v3 Announce Type: replace-cross \nAbstract: The tradeoff between reconstruction quality and compute required for video super-resolution (VSR) remains a formidable challenge in its adoption for deployment on resource-constrained edge devices. While transformer-based VSR models have set new benchmarks for reconstruction quality in recent years, these require substantial computational resources. On the other hand, lightweight models that have been introduced even recently struggle to deliver state-of-the-art reconstruction. We propose a novel lightweight and parameter-efficient neural architecture for VSR that achieves state-of-the-art reconstruction accuracy with just 2.3 million parameters. Our model enhances information utilization based on several architectural attributes. Firstly, it uses 2D wavelet decompositions strategically interlayered with learnable convolutional layers to utilize the inductive prior of spatial sparsity of edges in visual data. Secondly, it uses a single memory tensor to capture inter-frame temporal information while avoiding the computational cost of previous memory-based schemes. Thirdly, it uses residual deformable convolutions for implicit inter-frame object alignment that improve upon deformable convolutions by enhancing spatial information in inter-frame feature differences. Architectural insights from our model can pave the way for real-time VSR on the edge, such as display devices for streaming data.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "TAPS: Throat and Acoustic Paired Speech Dataset for Deep Learning-Based Speech Enhancement",
      "titleJa": "Taps: throat and acoustic paired speech データセット for ディープラーニング-based speech enhancement",
      "link": "https://arxiv.org/abs/2502.11478",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.11478v2 Announce Type: replace-cross \nAbstract: In high-noise environments such as factories, subways, and busy streets, capturing clear speech is challenging. Throat microphones can offer a solution because of their inherent noise-suppression capabilities; however, the passage of sound waves through skin and tissue attenuates high-frequency information, reducing speech clarity. Recent deep learning approaches have shown promise in enhancing throat microphone recordings, but further progress is constrained by the lack of a standard dataset. Here, we introduce the Throat and Acoustic Paired Speech (TAPS) dataset, a collection of paired utterances recorded from 60 native Korean speakers using throat and acoustic microphones. Furthermore, an optimal alignment approach was developed and applied to address the inherent signal mismatch between the two microphones. We tested three baseline deep learning models on the TAPS dataset and found mapping-based approaches to be superior for improving speech quality and restoring content. These findings demonstrate the TAPS dataset's utility for speech enhancement tasks and support its potential as a standard resource for advancing research in throat microphone-based applications.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Neural Guided Diffusion Bridges",
      "titleJa": "Neural guided diffusion bridges",
      "link": "https://arxiv.org/abs/2502.11909",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.11909v3 Announce Type: replace-cross \nAbstract: We propose a novel method for simulating conditioned diffusion processes (diffusion bridges) in Euclidean spaces. By training a neural network to approximate bridge dynamics, our approach eliminates the need for computationally intensive Markov Chain Monte Carlo (MCMC) methods or score modeling. Compared to existing methods, it offers greater robustness across various diffusion specifications and conditioning scenarios. This applies in particular to rare events and multimodal distributions, which pose challenges for score-learning- and MCMC-based approaches. We introduce a flexible variational family, partially specified by a neural network, for approximating the diffusion bridge path measure. Once trained, it enables efficient sampling of independent bridges at a cost comparable to sampling the unconditioned (forward) process.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Group-Level Data Selection for Efficient Pretraining",
      "titleJa": "Group-level data selection for efficient pretraining",
      "link": "https://arxiv.org/abs/2502.14709",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2502.14709v2 Announce Type: replace-cross \nAbstract: In this paper, we introduce Group-MATES, an efficient group-level data selection approach to optimize the speed-quality frontier of language model pretraining. Specifically, Group-MATES parameterizes costly group-level selection with a relational data influence model. To train this model, we sample training trajectories of the language model and collect oracle data influences alongside. The relational data influence model approximates the oracle data influence by weighting individual influence with relationships among training data. To enable efficient selection with our relational data influence model, we partition the dataset into small clusters using relationship weights and select data within each cluster independently. Experiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves 3.5%-9.4% relative performance gains over random selection across 22 downstream tasks, nearly doubling the improvements achieved by state-of-the-art individual data selection baselines. Furthermore, Group-MATES reduces the number of tokens required to reach a certain downstream performance by up to 1.75x, substantially elevating the speed-quality frontier. Further analyses highlight the critical role of relationship weights in the relational data influence model and the effectiveness of our cluster-based inference. Our code is open-sourced at https://github.com/facebookresearch/Group-MATES.",
      "summary": "arXiv:2502.",
      "summaryJa": "Arxiv:2502.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "CINNAMON: A hybrid approach to change point detection and parameter estimation in single-particle tracking data",
      "titleJa": "Cinnamon: a hybrid approach to change point detection and parameter estimation in single-particle tracking data",
      "link": "https://arxiv.org/abs/2503.14253",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.14253v2 Announce Type: replace-cross \nAbstract: Change point detection has become an important part of the analysis of the single-particle tracking data, as it allows one to identify moments, in which the motion patterns of observed particles undergo significant changes. The segmentation of diffusive trajectories based on those moments may provide insight into various phenomena in soft condensed matter and biological physics. In this paper, we propose CINNAMON, a hybrid approach to classifying single-particle tracking trajectories, detecting change points within them, and estimating diffusion parameters in the segments between the change points. Our method is based on a combination of neural networks, feature-based machine learning, and statistical techniques. It has been benchmarked in the second Anomalous Diffusion Challenge. The method offers a high level of interpretability due to its analytical and feature-based components. A potential use of features from topological data analysis is also discussed.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Patch-based learning of adaptive Total Variation parameter maps for blind image denoising",
      "titleJa": "Patch-based learning of adaptive total variation parameter maps for blind image denoising",
      "link": "https://arxiv.org/abs/2503.16010",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2503.16010v2 Announce Type: replace-cross \nAbstract: We consider a patch-based learning approach defined in terms of neural networks to estimate spatially adaptive regularisation parameter maps for image denoising with weighted Total Variation (TV) and test it to situations when the noise distribution is unknown. As an example, we consider situations where noise could be either Gaussian or Poisson and perform preliminary model selection by a standard binary classification network. Then, we define a patch-based approach where at each image pixel an optimal weighting between TV regularisation and the corresponding data fidelity is learned in a supervised way using reference natural image patches upon optimisation of SSIM and in a sliding window fashion. Extensive numerical results are reported for both noise models, showing significant improvement w.r.t. results obtained by means of optimal scalar regularisation.",
      "summary": "arXiv:2503.",
      "summaryJa": "Arxiv:2503.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Performance of Rank-One Tensor Approximation on Incomplete Data",
      "titleJa": "Performance of rank-one tensor approximation on incomplete data",
      "link": "https://arxiv.org/abs/2504.07818",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2504.07818v2 Announce Type: replace-cross \nAbstract: We are interested in the estimation of a rank-one tensor signal when only a portion $\\varepsilon$ of its noisy observation is available. We show that the study of this problem can be reduced to that of a random matrix model whose spectral analysis gives access to the reconstruction performance. These results shed light on and specify the loss of performance induced by an artificial reduction of the memory cost of a tensor via the deletion of a random part of its entries.",
      "summary": "arXiv:2504.",
      "summaryJa": "Arxiv:2504.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Incentivize Contribution and Learn Parameters Too: Federated Learning with Strategic Data Owners",
      "titleJa": "Incentivize contribution and learn parameters too: federated learning with strategic data owners",
      "link": "https://arxiv.org/abs/2505.12010",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.12010v2 Announce Type: replace-cross \nAbstract: Classical federated learning (FL) assumes that the clients have a limited amount of noisy data with which they voluntarily participate and contribute towards learning a global, more accurate model in a principled manner. The learning happens in a distributed fashion without sharing the data with the center. However, these methods do not consider the incentive of an agent for participating and contributing to the process, given that data collection and running a distributed algorithm is costly for the clients. The question of rationality of contribution has been asked recently in the literature and some results exist that consider this problem. This paper addresses the question of simultaneous parameter learning and incentivizing contribution, which distinguishes it from the extant literature. Our first mechanism incentivizes each client to contribute to the FL process at a Nash equilibrium and simultaneously learn the model parameters. However, this equilibrium outcome can be away from the optimal, where clients contribute with their full data and the algorithm learns the optimal parameters. We propose a second mechanism with monetary transfers that is budget balanced and enables the full data contribution along with optimal parameter learning. Large scale experiments with real (federated) datasets (CIFAR-10, FeMNIST, and Twitter) show that these algorithms converge quite fast in practice, yield good welfare guarantees, and better model performance for all agents.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer",
      "titleJa": "Anchorformer: differentiable anchor attention for efficient vision トランスフォーマー",
      "link": "https://arxiv.org/abs/2505.16463",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2505.16463v3 Announce Type: replace-cross \nAbstract: Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $n$ patches, they will have quadratic complexity such as $\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image with a small granularity. Meanwhile, the pivotal information is often randomly gathered in a few regions of an input image, some tokens may not be helpful for the downstream tasks. To handle this problem, we introduce an anchor-based efficient vision transformer (AnchorFormer), which employs the anchor tokens to learn the pivotal information and accelerate the inference. Firstly, by estimating the bipartite attention between the anchors and tokens, the complexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where $m$ is an anchor number and $m < n$. Notably, by representing the anchors with the neurons in a neural layer, we can differentiably learn these anchors and approximate global self-attention through the Markov process. It avoids the burden caused by non-differentiable operations and further speeds up the approximate attention. Moreover, we extend the proposed model to three downstream tasks including classification, detection, and segmentation. Extensive experiments show the effectiveness of our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet classification, 81.3% higher mAP on COCO detection under comparable FLOPs, as compared to the current baselines.",
      "summary": "arXiv:2505.",
      "summaryJa": "Arxiv:2505.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "fairmetrics: An R package for group fairness evaluation",
      "titleJa": "Fairmetrics: an r package for group fairness evaluation",
      "link": "https://arxiv.org/abs/2506.06243",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.06243v2 Announce Type: replace-cross \nAbstract: Fairness is a growing area of machine learning (ML) that focuses on ensuring models do not produce systematically biased outcomes for specific groups, particularly those defined by protected attributes such as race, gender, or age. Evaluating fairness is a critical aspect of ML model development, as biased models can perpetuate structural inequalities. The {fairmetrics} R package offers a user-friendly framework for rigorously evaluating numerous group-based fairness criteria, including metrics based on independence (e.g., statistical parity), separation (e.g., equalized odds), and sufficiency (e.g., predictive parity). Group-based fairness criteria assess whether a model is equally accurate or well-calibrated across a set of predefined groups so that appropriate bias mitigation strategies can be implemented. {fairmetrics} provides both point and interval estimates for multiple metrics through a convenient wrapper function and includes an example dataset derived from the Medical Information Mart for Intensive Care, version II (MIMIC-II) database (Goldberger et al., 2000; Raffa, 2016).",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval",
      "titleJa": "Hierarchical multi-positive contrastive learning for patent image retrieval",
      "link": "https://arxiv.org/abs/2506.13496",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.13496v3 Announce Type: replace-cross \nAbstract: Patent images are technical drawings that convey information about a patent's innovation. Patent image retrieval systems aim to search in vast collections and retrieve the most relevant images. Despite recent advances in information retrieval, patent images still pose significant challenges due to their technical intricacies and complex semantic information, requiring efficient fine-tuning for domain adaptation. Current methods neglect patents' hierarchical relationships, such as those defined by the Locarno International Classification (LIC) system, which groups broad categories (e.g., \"furnishing\") into subclasses (e.g., \"seats\" and \"beds\") and further into specific patent designs. In this work, we introduce a hierarchical multi-positive contrastive loss that leverages the LIC's taxonomy to induce such relations in the retrieval process. Our approach assigns multiple positive pairs to each patent image within a batch, with varying similarity scores based on the hierarchical taxonomy. Our experimental analysis with various vision and multimodal models on the DeepPatent2 dataset shows that the proposed method enhances the retrieval results. Notably, our method is effective with low-parameter models, which require fewer computational resources and can be deployed on environments with limited hardware.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means",
      "titleJa": "Uniform mean estimation for heavy-tailed distributions via median-of-means",
      "link": "https://arxiv.org/abs/2506.14673",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14673v3 Announce Type: replace-cross \nAbstract: The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \\in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring",
      "titleJa": "Simbank: from simulation to solution in prescriptive process monitoring",
      "link": "https://arxiv.org/abs/2506.14772",
      "pubDate": "Mon, 23 Jun 2025 00:00:00 -0400",
      "content": "arXiv:2506.14772v2 Announce Type: replace-cross \nAbstract: Prescriptive Process Monitoring (PresPM) is an emerging area within Process Mining, focused on optimizing processes through real-time interventions for effective decision-making. PresPM holds significant promise for organizations seeking enhanced operational performance. However, the current literature faces two key limitations: a lack of extensive comparisons between techniques and insufficient evaluation approaches. To address these gaps, we introduce SimBank: a simulator designed for accurate benchmarking of PresPM methods. Modeled after a bank's loan application process, SimBank enables extensive comparisons of both online and offline PresPM methods. It incorporates a variety of intervention optimization problems with differing levels of complexity and supports experiments on key causal machine learning challenges, such as assessing a method's robustness to confounding in data. SimBank additionally offers a comprehensive evaluation capability: for each test case, it can generate the true outcome under each intervention action, which is not possible using recorded datasets. The simulator incorporates parallel activities and loops, drawing from common logs to generate cases that closely resemble real-life process instances. Our proof of concept demonstrates SimBank's benchmarking capabilities through experiments with various PresPM methods across different interventions, highlighting its value as a publicly available simulator for advancing research and practice in PresPM.",
      "summary": "arXiv:2506.",
      "summaryJa": "Arxiv:2506.",
      "source": "arXiv Machine Learning",
      "category": "academic",
      "importance": 50
    },
    {
      "title": "OpenAI pulls promotional materials around Jony Ive deal due to court order",
      "titleJa": "Openai pulls promotional materials around jony ive deal due to court order",
      "link": "https://techcrunch.com/2025/06/22/openai-pulls-promotional-materials-around-jony-ive-deal/",
      "pubDate": "Sun, 22 Jun 2025 22:24:35 +0000",
      "content": "OpenAI appears to have pulled a much-discussed video promoting the friendship between CEO Sam Altman and legendary Apple designer Jony Ive (plus, incidentally, OpenAI’s $6.5 billion deal to acquire Ive and Altman’s device startup io) from its website and YouTube page.",
      "summary": "OpenAI appears to have pulled a much-discussed video promoting the friendship between CEO Sam Altman and legendary Apple designer Jony Ive (plus, inci...",
      "summaryJa": "Openai appears to have pulled a much-discussed video promoting the friendship between ceo sam altman and legendary apple designer jony ive (plus, inci...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 65
    },
    {
      "title": "Moratorium on state AI regulation clears Senate hurdle",
      "titleJa": "Moratorium on state ai 規制 clears senate hurdle",
      "link": "https://techcrunch.com/2025/06/22/moratorium-on-state-ai-regulation-clears-senate-hurdle/",
      "pubDate": "Sun, 22 Jun 2025 17:17:56 +0000",
      "content": "A Republican effort to prevent states from enforcing their own AI regulations cleared a key procedural hurdle on Saturday.",
      "summary": "A Republican effort to prevent states from enforcing their own AI regulations cleared a key procedural hurdle on Saturday.",
      "summaryJa": "A republican effort to prevent states from enforcing their own ai regulations cleared a key procedural hurdle on saturday.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 35
    },
    {
      "title": "Cloud quantum computing: A trillion-dollar opportunity with dangerous hidden risks",
      "titleJa": "クラウド 量子コンピューティング: a trillion-dollar opportunity with dangerous hidden risks",
      "link": "https://venturebeat.com/security/cloud-quantum-computing-a-trillion-dollar-opportunity-with-dangerous-hidden-risks/",
      "pubDate": "Sat, 21 Jun 2025 20:00:00 +0000",
      "content": "Start implementing post-quantum cryptography, keep an eye on adversarial quantum programs and secure the quantum supply chain.",
      "summary": "Start implementing post-quantum cryptography, keep an eye on adversarial quantum programs and secure the quantum supply chain.",
      "summaryJa": "Start implementing post-quantum cryptography, keep an eye on adversarial quantum programs and secure the quantum supply chain.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 35
    },
    {
      "title": "Cartoonist Paul Pope is more worried about killer robots than AI plagiarism",
      "titleJa": "Cartoonist paul pope is more worried about killer robots than ai plagiarism",
      "link": "https://techcrunch.com/2025/06/21/cartoonist-paul-pope-is-more-worried-about-killer-robots-than-ai-plagiarism/",
      "pubDate": "Sat, 21 Jun 2025 18:08:49 +0000",
      "content": "Paul Pope has written and drawn some of the most gorgeous comics of the twenty-first century — from “Batman: Year 100,” in which Batman challenges a dystopian surveillance state, to “Battling Boy,” with its adolescent god proving his mettle by fighting giant monsters. But it’s been more than a decade since Pope’s last major comics […]",
      "summary": "Paul Pope has written and drawn some of the most gorgeous comics of the twenty-first century — from “Batman: Year 100,” in which Batman challenges a d...",
      "summaryJa": "Paul pope has written and drawn some of the most gorgeous comics of the twenty-first century — from “batman: year 100,” in which batman challenges a d...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 35
    },
    {
      "title": "Mistral just updated its open source Small model from 3.1 to 3.2: here’s why",
      "titleJa": "Mistral just updated its オープンソース small モデル from 3.1 to 3.2: here’s why",
      "link": "https://venturebeat.com/ai/mistral-just-updated-its-open-source-small-model-from-3-1-to-3-2-heres-why/",
      "pubDate": "Fri, 20 Jun 2025 22:51:45 +0000",
      "content": "The fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also helps its appeal",
      "summary": "The fact that it is made by a French startup and compliant with EU rules and regulations such as GDPR and the EU AI Act also helps its appeal",
      "summaryJa": "The fact that it is made by a french スタートアップ and compliant with eu rules and regulations such as gdpr and the eu ai act also helps its appeal",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 35
    },
    {
      "title": "Mira Murati’s Thinking Machines Lab closes on $2B at $10B valuation",
      "titleJa": "Mira murati’s thinking machines lab closes on $2b at $10b valuation",
      "link": "https://techcrunch.com/2025/06/20/mira-muratis-thinking-machines-lab-closes-on-2b-at-10b-valuation/",
      "pubDate": "Fri, 20 Jun 2025 21:59:33 +0000",
      "content": "Thinking Machines Lab, the secretive AI startup founded by OpenAI’s former chief technology officer Mira Murati, has closed a $2 billion seed round at a $10 billion valuation.",
      "summary": "Thinking Machines Lab, the secretive AI startup founded by OpenAI’s former chief technology officer Mira Murati, has closed a $2 billion seed round at...",
      "summaryJa": "Thinking machines lab, the secretive ai スタートアップ founded by openai’s former chief technology officer mira murati, has closed a $2 billion seed round at...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 50
    },
    {
      "title": "Cluely, a startup that helps ‘cheat on everything,’ raises $15M from a16z",
      "titleJa": "Cluely, a スタートアップ that helps ‘cheat on everything,’ raises $15m from a16z",
      "link": "https://techcrunch.com/2025/06/20/cluely-a-startup-that-helps-cheat-on-everything-raises-15m-from-a16z/",
      "pubDate": "Fri, 20 Jun 2025 21:06:19 +0000",
      "content": "Cluely's new funding comes roughly two months after it raised $5.3 million in seed funding co-led by Abstract Ventures and Susa Ventures.",
      "summary": "Cluely's new funding comes roughly two months after it raised $5.",
      "summaryJa": "Cluely's new 資金調達 comes roughly two months after it raised $5.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 35
    },
    {
      "title": "Anthropic study: Leading AI models show up to 96% blackmail rate against executives",
      "titleJa": "Anthropic study: leading ai models show up to 96% blackmail rate against executives",
      "link": "https://venturebeat.com/ai/anthropic-study-leading-ai-models-show-up-to-96-blackmail-rate-against-executives/",
      "pubDate": "Fri, 20 Jun 2025 19:39:03 +0000",
      "content": "Anthropic research reveals AI models from OpenAI, Google, Meta and others chose blackmail, corporate espionage and lethal actions when facing shutdown or conflicting goals.",
      "summary": "Anthropic research reveals AI models from OpenAI, Google, Meta and others chose blackmail, corporate espionage and lethal actions when facing shutdown...",
      "summaryJa": "Anthropic 研究 reveals ai models from openai, google, meta and others chose blackmail, corporate espionage and lethal actions when facing shutdown...",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 80
    },
    {
      "title": "Anthropic says most AI models, not just Claude, will resort to blackmail",
      "titleJa": "Anthropic says most ai models, not just claude, will resort to blackmail",
      "link": "https://techcrunch.com/2025/06/20/anthropic-says-most-ai-models-not-just-claude-will-resort-to-blackmail/",
      "pubDate": "Fri, 20 Jun 2025 19:17:44 +0000",
      "content": "Several weeks after Anthropic released research claiming that its Claude Opus 4 AI model resorted to blackmailing engineers who tried to turn the model off in controlled test scenarios, the company is out with new research suggesting the problem is more widespread among leading AI models.",
      "summary": "Several weeks after Anthropic released research claiming that its Claude Opus 4 AI model resorted to blackmailing engineers who tried to turn the mode...",
      "summaryJa": "Several weeks after anthropic released 研究 claiming that its claude opus 4 ai モデル resorted to blackmailing engineers who tried to turn the mode...",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 55
    },
    {
      "title": "ChatGPT: Everything you need to know about the AI-powered chatbot",
      "titleJa": "Chatgpt: everything you need to know about the ai-powered チャットボット",
      "link": "https://techcrunch.com/2025/06/20/chatgpt-everything-to-know-about-the-ai-chatbot/",
      "pubDate": "Fri, 20 Jun 2025 17:27:58 +0000",
      "content": "ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users. 2024 was a big year for OpenAI, from its partnership […]",
      "summary": "ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022.",
      "summaryJa": "Chatgpt, openai’s text-generating ai チャットボット, has taken the world by storm since its launch in november 2022.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 55
    },
    {
      "title": "Character.AI taps Meta’s former VP of business products as CEO",
      "titleJa": "Character.ai taps meta’s former vp of business products as ceo",
      "link": "https://techcrunch.com/2025/06/20/character-ai-taps-metas-former-vp-of-business-products-as-ceo/",
      "pubDate": "Fri, 20 Jun 2025 17:20:06 +0000",
      "content": "Karandeep Anand comes to Character.AI with experience running advertising products that reached billions of users on Meta's apps.",
      "summary": "Karandeep Anand comes to Character.",
      "summaryJa": "Karandeep anand comes to character.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 40
    },
    {
      "title": "Could OpenAI fill Microsoft’s shoes?",
      "titleJa": "Could openai fill microsoft’s shoes?",
      "link": "https://techcrunch.com/podcast/could-openai-fill-microsofts-shoes/",
      "pubDate": "Fri, 20 Jun 2025 16:26:08 +0000",
      "content": "OpenAI recently announced a $200 million deal with the U.S. Department of Defense, which has us wondering: Could this further strain the company’s relationship with its biggest backer, Microsoft? After all, there have been numerous reports about growing tensions between the two companies, particularly as they become more competitive over enterprise deals. Today, on TechCrunch’s […]",
      "summary": "OpenAI recently announced a $200 million deal with the U.",
      "summaryJa": "Openai recently announced a $200 million deal with the u.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 40
    },
    {
      "title": "After trying to buy Ilya Sutskever’s $32B AI startup, Meta looks to hire its CEO",
      "titleJa": "After trying to buy ilya sutskever’s $32b ai スタートアップ, meta looks to hire its ceo",
      "link": "https://techcrunch.com/2025/06/20/after-trying-to-buy-ilya-sutskevers-32b-ai-startup-meta-looks-to-hire-its-ceo/",
      "pubDate": "Fri, 20 Jun 2025 15:32:28 +0000",
      "content": "Meta CEO Mark Zuckerberg tried to acquire Ilya Sutskever's $32B AI startup, but failed, so now he's in talks to hire its CEO, Daniel Gross.",
      "summary": "Meta CEO Mark Zuckerberg tried to acquire Ilya Sutskever's $32B AI startup, but failed, so now he's in talks to hire its CEO, Daniel Gross.",
      "summaryJa": "Meta ceo mark zuckerberg tried to acquire ilya sutskever's $32b ai スタートアップ, but failed, so now he's in talks to hire its ceo, daniel gross.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 35
    },
    {
      "title": "SoftBank reportedly looking to launch a trillion-dollar AI and robotics industrial complex",
      "titleJa": "Softbank reportedly looking to launch a trillion-dollar ai and ロボティクス industrial complex",
      "link": "https://techcrunch.com/2025/06/20/softbank-reportedly-looking-to-launch-a-trillion-dollar-ai-and-robotics-industrial-complex/",
      "pubDate": "Fri, 20 Jun 2025 15:22:06 +0000",
      "content": "The investing conglomerate is looking to team up with TSMC on the initiative as SoftBank continues to pour money into AI.",
      "summary": "The investing conglomerate is looking to team up with TSMC on the initiative as SoftBank continues to pour money into AI.",
      "summaryJa": "The investing conglomerate is looking to team up with tsmc on the initiative as softbank continues to pour money into ai.",
      "source": "TechCrunch AI",
      "category": "business",
      "importance": 40
    },
    {
      "title": "Google’s Gemini transparency cut leaves enterprise developers ‘debugging blind’",
      "titleJa": "Google’s gemini transparency cut leaves enterprise developers ‘debugging blind’",
      "link": "https://venturebeat.com/ai/googles-gemini-transparency-cut-leaves-enterprise-developers-debugging-blind/",
      "pubDate": "Fri, 20 Jun 2025 12:00:00 +0000",
      "content": "Why is Google hiding Gemini's reasoning traces? The decision sparks a debate over black-box models versus the need for transparency.",
      "summary": "Why is Google hiding Gemini's reasoning traces?",
      "summaryJa": "Why is google hiding gemini's reasoning traces?",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 40
    },
    {
      "title": "OpenAI can rehabilitate AI models that develop a “bad-boy persona”",
      "titleJa": "Openai can rehabilitate ai models that develop a “bad-boy persona”",
      "link": "https://www.technologyreview.com/2025/06/18/1119042/openai-can-rehabilitate-ai-models-that-develop-a-bad-boy-persona/",
      "pubDate": "Wed, 18 Jun 2025 18:19:15 +0000",
      "content": "A new paper from OpenAI has shown why a little bit of bad training can make AI models go rogue—but also demonstrates that this problem is generally pretty easy to fix.  Back in February, a group of researchers discovered that fine-tuning an AI model (in their case, OpenAI’s GPT-4o) by training it on code that…",
      "summary": "A new paper from OpenAI has shown why a little bit of bad training can make AI models go rogue—but also demonstrates that this problem is generally pr...",
      "summaryJa": "A new 論文 from openai has shown why a little bit of bad 学習 can make ai models go rogue—but also demonstrates that this problem is generally pr...",
      "source": "MIT Technology Review AI",
      "category": "tech",
      "importance": 40
    },
    {
      "title": "The Interpretable AI playbook: What Anthropic’s research means for your enterprise LLM strategy",
      "titleJa": "The interpretable ai playbook: what anthropic’s 研究 means for your enterprise LLM strategy",
      "link": "https://venturebeat.com/ai/the-interpretable-ai-playbook-what-anthropics-research-means-for-your-enterprise-llm-strategy/",
      "pubDate": "Tue, 17 Jun 2025 23:01:08 +0000",
      "content": "Anthropic is developing “interpretable” AI, where models let us understand what they are thinking and arrive at a particular conclusion.",
      "summary": "Anthropic is developing “interpretable” AI, where models let us understand what they are thinking and arrive at a particular conclusion.",
      "summaryJa": "Anthropic is developing “interpretable” ai, where models let us understand what they are thinking and arrive at a particular conclusion.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 40
    },
    {
      "title": "Google launches production-ready Gemini 2.5 AI models to challenge OpenAI’s enterprise dominance",
      "titleJa": "Google ローンチ production-ready gemini 2.5 ai models to challenge openai’s enterprise dominance",
      "link": "https://venturebeat.com/ai/google-launches-production-ready-gemini-2-5-ai-models-to-challenge-openais-enterprise-dominance/",
      "pubDate": "Tue, 17 Jun 2025 21:55:56 +0000",
      "content": "Google launches production-ready Gemini 2.5 Pro and Flash AI models for enterprises while introducing cost-efficient Flash-Lite to challenge OpenAI's market dominance.",
      "summary": "Google launches production-ready Gemini 2.",
      "summaryJa": "Google ローンチ production-ready gemini 2.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 80
    },
    {
      "title": "OpenAI moves forward with GPT-4.5 deprecation in API, triggering developer anguish and confusion",
      "titleJa": "Openai moves forward with GPT-4.5 deprecation in API, triggering developer anguish and confusion",
      "link": "https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/",
      "pubDate": "Tue, 17 Jun 2025 21:52:29 +0000",
      "content": "Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.5 Preview back in April 2025.",
      "summary": "Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.",
      "summaryJa": "Despite the strong reaction, openai had in fact already announced the plan to deprecate GPT-4.",
      "source": "VentureBeat AI",
      "category": "tech",
      "importance": 50
    },
    {
      "title": "Gemini 2.5: Updates to our family of thinking models",
      "titleJa": "Gemini 2.5: updates to our family of thinking models",
      "link": "https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/",
      "pubDate": "Tue, 17 Jun 2025 16:03:39 +0000",
      "content": "Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",
      "summary": "Explore the latest Gemini 2.",
      "summaryJa": "Explore the latest gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 55
    },
    {
      "title": "We’re expanding our Gemini 2.5 family of models",
      "titleJa": "We’re expanding our gemini 2.5 family of models",
      "link": "https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/",
      "pubDate": "Tue, 17 Jun 2025 16:01:00 +0000",
      "content": "Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 55
    },
    {
      "title": "Meta buys stake in Scale AI, raising antitrust concerns",
      "titleJa": "Meta buys stake in scale ai, raising antitrust concerns",
      "link": "https://www.artificialintelligence-news.com/news/meta-buys-stake-in-scale-ai-raising-antitrust-concerns/",
      "pubDate": "Mon, 16 Jun 2025 15:16:43 +0000",
      "content": "Meta’s $14.8 billion investment in Scale AI – and the hiring of the startup’s CEO – is drawing attention to how US regulators will handle acquihire-style deals under the Trump administration. The deal gives Meta a 49% nonvoting stake in Scale AI, which hires gig workers to label training data for AI systems. Scale’s clients […]\nThe post Meta buys stake in Scale AI, raising antitrust concerns appeared first on AI News.",
      "summary": "Meta’s $14.",
      "summaryJa": "Meta’s $14.",
      "source": "AI News",
      "category": "tech",
      "importance": 30
    },
    {
      "title": "How we're supporting better tropical cyclone prediction with AI",
      "titleJa": "How we're supporting better tropical cyclone prediction with ai",
      "link": "https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/",
      "pubDate": "Thu, 12 Jun 2025 15:00:00 +0000",
      "content": "We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.S. National Hurricane Center to support their forecasts and warnings this cyclone season.",
      "summary": "We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.",
      "summaryJa": "We’re launching weather lab, featuring our experimental cyclone predictions, and we’re partnering with the u.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Advanced audio dialog and generation with Gemini 2.5",
      "titleJa": "Advanced audio dialog and generation with gemini 2.5",
      "link": "https://deepmind.google/discover/blog/advanced-audio-dialog-and-generation-with-gemini-25/",
      "pubDate": "Tue, 03 Jun 2025 17:15:47 +0000",
      "content": "Gemini 2.5 has new capabilities in AI-powered audio dialog and generation.",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Gemini 2.5: Our most intelligent models are getting even better",
      "titleJa": "Gemini 2.5: our most intelligent models are getting even better",
      "link": "https://deepmind.google/discover/blog/gemini-25-our-world-leading-model-is-getting-even-better/",
      "pubDate": "Tue, 20 May 2025 09:45:00 +0000",
      "content": "Gemini 2.5 Pro continues to be loved by developers as the best model for coding, and 2.5 Flash is getting even better with a new update. We’re bringing new capabilities to our models, including Deep Think, an experimental enhanced reasoning mode for 2.5 Pro.",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Our vision for building a universal AI assistant",
      "titleJa": "Our vision for building a universal ai assistant",
      "link": "https://deepmind.google/discover/blog/our-vision-for-building-a-universal-ai-assistant/",
      "pubDate": "Tue, 20 May 2025 09:45:00 +0000",
      "content": "We’re extending Gemini to become a world model that can make plans and imagine new experiences by simulating aspects of the world.",
      "summary": "We’re extending Gemini to become a world model that can make plans and imagine new experiences by simulating aspects of the world.",
      "summaryJa": "We’re extending gemini to become a world モデル that can make plans and imagine new experiences by simulating aspects of the world.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Advancing Gemini's security safeguards",
      "titleJa": "Advancing gemini's セキュリティ safeguards",
      "link": "https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/",
      "pubDate": "Tue, 20 May 2025 09:45:00 +0000",
      "content": "We’ve made Gemini 2.5 our most secure model family to date.",
      "summary": "We’ve made Gemini 2.",
      "summaryJa": "We’ve made gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Announcing Gemma 3n preview: Powerful, efficient, mobile-first AI",
      "titleJa": "Announcing gemma 3n preview: powerful, efficient, mobile-first ai",
      "link": "https://deepmind.google/discover/blog/announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai/",
      "pubDate": "Tue, 20 May 2025 09:45:00 +0000",
      "content": "Gemma 3n is a cutting-edge open model designed for fast, multimodal AI on devices, featuring optimized performance, unique flexibility with a 2-in-1 model, and expanded multimodal understanding with audio, empowering developers to build live, interactive applications and sophisticated audio-centric experiences.",
      "summary": "Gemma 3n is a cutting-edge open model designed for fast, multimodal AI on devices, featuring optimized performance, unique flexibility with a 2-in-1 m...",
      "summaryJa": "Gemma 3n is a cutting-edge open モデル designed for fast, multimodal ai on devices, featuring optimized performance, unique flexibility with a 2-in-1 m...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms",
      "titleJa": "Alphaevolve: a gemini-powered coding agent for designing advanced algorithms",
      "link": "https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/",
      "pubDate": "Wed, 14 May 2025 14:59:00 +0000",
      "content": "New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluators",
      "summary": "New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated e...",
      "summaryJa": "New ai agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated e...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Gemini 2.5 Pro Preview: even better coding performance",
      "titleJa": "Gemini 2.5 pro preview: even better coding performance",
      "link": "https://deepmind.google/discover/blog/gemini-25-pro-preview-even-better-coding-performance/",
      "pubDate": "Tue, 06 May 2025 15:06:55 +0000",
      "content": "We’ve seen developers doing amazing things with Gemini 2.5 Pro, so we decided to release an updated version a couple of weeks early to get into developers hands sooner.",
      "summary": "We’ve seen developers doing amazing things with Gemini 2.",
      "summaryJa": "We’ve seen developers doing amazing things with gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "Build rich, interactive web apps with an updated Gemini 2.5 Pro",
      "titleJa": "Build rich, interactive web apps with an updated gemini 2.5 pro",
      "link": "https://deepmind.google/discover/blog/build-rich-interactive-web-apps-with-an-updated-gemini-25-pro/",
      "pubDate": "Tue, 06 May 2025 15:00:00 +0000",
      "content": "Our updated version of Gemini 2.5 Pro Preview has improved capabilities for coding.",
      "summary": "Our updated version of Gemini 2.",
      "summaryJa": "Our updated version of gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Introducing Gemini 2.5 Flash",
      "titleJa": "Introducing gemini 2.5 flash",
      "link": "https://deepmind.google/discover/blog/introducing-gemini-2-5-flash/",
      "pubDate": "Thu, 17 Apr 2025 19:02:00 +0000",
      "content": "Gemini 2.5 Flash is our first fully hybrid reasoning model, giving developers the ability to turn thinking on or off.",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Generate videos in Gemini and Whisk with Veo 2",
      "titleJa": "Generate videos in gemini and whisk with veo 2",
      "link": "https://deepmind.google/discover/blog/generate-videos-in-gemini-and-whisk-with-veo-2/",
      "pubDate": "Tue, 15 Apr 2025 17:00:00 +0000",
      "content": "Transform text-based prompts into high-resolution eight-second videos in Gemini Advanced and use Whisk Animate to turn images into eight-second animated clips.",
      "summary": "Transform text-based prompts into high-resolution eight-second videos in Gemini Advanced and use Whisk Animate to turn images into eight-second animat...",
      "summaryJa": "Transform text-based prompts into high-resolution eight-second videos in gemini advanced and use whisk animate to turn images into eight-second animat...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "DolphinGemma: How Google AI is helping decode dolphin communication",
      "titleJa": "Dolphingemma: how google ai is helping decode dolphin communication",
      "link": "https://deepmind.google/discover/blog/dolphingemma-how-google-ai-is-helping-decode-dolphin-communication/",
      "pubDate": "Mon, 14 Apr 2025 17:00:00 +0000",
      "content": "DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they're saying, too.",
      "summary": "DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they're s...",
      "summaryJa": "Dolphingemma, a 大規模言語モデル developed by google, is helping scientists study how dolphins communicate — and hopefully find out what they're s...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Gemini 2.5: Our most intelligent AI model",
      "titleJa": "Gemini 2.5: our most intelligent ai モデル",
      "link": "https://deepmind.google/discover/blog/gemini-2-5-our-most-intelligent-ai-model/",
      "pubDate": "Tue, 25 Mar 2025 17:00:36 +0000",
      "content": "Gemini 2.5 is our most intelligent AI model, now with thinking built in.",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Gemini Robotics brings AI into the physical world",
      "titleJa": "Gemini ロボティクス brings ai into the physical world",
      "link": "https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/",
      "pubDate": "Wed, 12 Mar 2025 15:00:00 +0000",
      "content": "Introducing Gemini Robotics and Gemini Robotics-ER, AI models designed for robots to understand, act and react to the physical world.",
      "summary": "Introducing Gemini Robotics and Gemini Robotics-ER, AI models designed for robots to understand, act and react to the physical world.",
      "summaryJa": "Introducing gemini ロボティクス and gemini ロボティクス-er, ai models designed for robots to understand, act and react to the physical world.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Experiment with Gemini 2.0 Flash native image generation",
      "titleJa": "Experiment with gemini 2.0 flash native image generation",
      "link": "https://deepmind.google/discover/blog/experiment-with-gemini-20-flash-native-image-generation/",
      "pubDate": "Wed, 12 Mar 2025 14:58:00 +0000",
      "content": "Native image output is available in Gemini 2.0 Flash for developers to experiment with in Google AI Studio and the Gemini API.",
      "summary": "Native image output is available in Gemini 2.",
      "summaryJa": "Native image output is available in gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "Introducing Gemma 3",
      "titleJa": "Introducing gemma 3",
      "link": "https://deepmind.google/discover/blog/introducing-gemma-3/",
      "pubDate": "Wed, 12 Mar 2025 08:00:00 +0000",
      "content": "The most capable model you can run on a single GPU or TPU.",
      "summary": "The most capable model you can run on a single GPU or TPU.",
      "summaryJa": "The most capable モデル you can run on a single gpu or tpu.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "Start building with Gemini 2.0 Flash and Flash-Lite",
      "titleJa": "Start building with gemini 2.0 flash and flash-lite",
      "link": "https://deepmind.google/discover/blog/start-building-with-gemini-20-flash-and-flash-lite/",
      "pubDate": "Tue, 25 Feb 2025 18:02:12 +0000",
      "content": "Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI",
      "summary": "Gemini 2.",
      "summaryJa": "Gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "Gemini 2.0 is now available to everyone",
      "titleJa": "Gemini 2.0 is now available to everyone",
      "link": "https://deepmind.google/discover/blog/gemini-2-0-is-now-available-to-everyone/",
      "pubDate": "Wed, 05 Feb 2025 16:00:00 +0000",
      "content": "We’re announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.",
      "summary": "We’re announcing new updates to Gemini 2.",
      "summaryJa": "We’re announcing new updates to gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "FACTS Grounding: A new benchmark for evaluating the factuality of large language models",
      "titleJa": "Facts grounding: a new benchmark for evaluating the factuality of large language models",
      "link": "https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/",
      "pubDate": "Tue, 17 Dec 2024 15:29:00 +0000",
      "content": "Our comprehensive benchmark and online leaderboard offer a much-needed measure of how accurately LLMs ground their responses in provided source material and avoid hallucinations",
      "summary": "Our comprehensive benchmark and online leaderboard offer a much-needed measure of how accurately LLMs ground their responses in provided source materi...",
      "summaryJa": "Our comprehensive benchmark and online leaderboard offer a much-needed measure of how accurately llms ground their responses in provided source materi...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "State-of-the-art video and image generation with Veo 2 and Imagen 3",
      "titleJa": "State-of-the-art video and image generation with veo 2 and imagen 3",
      "link": "https://deepmind.google/discover/blog/state-of-the-art-video-and-image-generation-with-veo-2-and-imagen-3/",
      "pubDate": "Mon, 16 Dec 2024 17:01:16 +0000",
      "content": "We’re rolling out a new, state-of-the-art video model, Veo 2, and updates to Imagen 3. Plus, check out our new experiment, Whisk.",
      "summary": "We’re rolling out a new, state-of-the-art video model, Veo 2, and updates to Imagen 3.",
      "summaryJa": "We’re rolling out a new, state-of-the-art video モデル, veo 2, and updates to imagen 3.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "Introducing Gemini 2.0: our new AI model for the agentic era",
      "titleJa": "Introducing gemini 2.0: our new ai モデル for the agentic era",
      "link": "https://deepmind.google/discover/blog/introducing-gemini-20-our-new-ai-model-for-the-agentic-era/",
      "pubDate": "Wed, 11 Dec 2024 15:30:40 +0000",
      "content": "Today, we’re announcing Gemini 2.0, our most capable multimodal AI model yet.",
      "summary": "Today, we’re announcing Gemini 2.",
      "summaryJa": "Today, we’re announcing gemini 2.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Google DeepMind at NeurIPS 2024",
      "titleJa": "Google deepmind at neurips 2024",
      "link": "https://deepmind.google/discover/blog/google-deepmind-at-neurips-2024/",
      "pubDate": "Thu, 05 Dec 2024 17:45:00 +0000",
      "content": "Advancing adaptive AI agents, empowering 3D scene creation, and innovating LLM training for a smarter, safer future",
      "summary": "Advancing adaptive AI agents, empowering 3D scene creation, and innovating LLM training for a smarter, safer future",
      "summaryJa": "Advancing adaptive ai agents, empowering 3d scene creation, and innovating LLM 学習 for a smarter, safer future",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "Genie 2: A large-scale foundation world model",
      "titleJa": "Genie 2: a large-scale foundation world モデル",
      "link": "https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/",
      "pubDate": "Wed, 04 Dec 2024 14:23:00 +0000",
      "content": "Generating unlimited diverse training environments for future general agents",
      "summary": "Generating unlimited diverse training environments for future general agents",
      "summaryJa": "Generating unlimited diverse 学習 environments for future general agents",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "AlphaQubit tackles one of quantum computing’s biggest challenges",
      "titleJa": "Alphaqubit tackles one of 量子コンピューティング’s biggest challenges",
      "link": "https://deepmind.google/discover/blog/alphaqubit-tackles-one-of-quantum-computings-biggest-challenges/",
      "pubDate": "Wed, 20 Nov 2024 18:00:00 +0000",
      "content": "Our new AI system accurately identifies errors inside quantum computers, helping to make this new technology more reliable.",
      "summary": "Our new AI system accurately identifies errors inside quantum computers, helping to make this new technology more reliable.",
      "summaryJa": "Our new ai system accurately identifies errors inside quantum computers, helping to make this new technology more reliable.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "The AI for Science Forum: A new era of discovery",
      "titleJa": "The ai for science forum: a new era of discovery",
      "link": "https://deepmind.google/discover/blog/the-ai-for-science-forum-a-new-era-of-discovery/",
      "pubDate": "Mon, 18 Nov 2024 19:57:00 +0000",
      "content": "The AI Science Forum highlights AI's present and potential role in revolutionizing scientific discovery and solving global challenges, emphasizing collaboration between the scientific community, policymakers, and industry leaders.",
      "summary": "The AI Science Forum highlights AI's present and potential role in revolutionizing scientific discovery and solving global challenges, emphasizing col...",
      "summaryJa": "The ai science forum highlights ai's present and potential role in revolutionizing scientific discovery and solving global challenges, emphasizing col...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more",
      "titleJa": "Updated production-ready gemini models, reduced 1.5 pro pricing, increased rate limits, and more",
      "link": "https://deepmind.google/discover/blog/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/",
      "pubDate": "Tue, 24 Sep 2024 16:03:03 +0000",
      "content": "We’re releasing two updated production-ready Gemini models",
      "summary": "We’re releasing two updated production-ready Gemini models",
      "summaryJa": "We’re releasing two updated production-ready gemini models",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Our latest advances in robot dexterity",
      "titleJa": "Our latest advances in robot dexterity",
      "link": "https://deepmind.google/discover/blog/advances-in-robot-dexterity/",
      "pubDate": "Thu, 12 Sep 2024 14:00:00 +0000",
      "content": "Two new AI systems, ALOHA Unleashed and DemoStart, help robots learn to perform complex tasks that require dexterous movement",
      "summary": "Two new AI systems, ALOHA Unleashed and DemoStart, help robots learn to perform complex tasks that require dexterous movement",
      "summaryJa": "Two new ai systems, aloha unleashed and demostart, help robots learn to perform complex tasks that require dexterous movement",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "FermiNet: Quantum physics and chemistry from first principles",
      "titleJa": "Ferminet: quantum physics and chemistry from first principles",
      "link": "https://deepmind.google/discover/blog/ferminet-quantum-physics-and-chemistry-from-first-principles/",
      "pubDate": "Thu, 22 Aug 2024 19:00:00 +0000",
      "content": "Using deep learning to solve fundamental problems in computational quantum chemistry and explore how matter interacts with light",
      "summary": "Using deep learning to solve fundamental problems in computational quantum chemistry and explore how matter interacts with light",
      "summaryJa": "Using ディープラーニング to solve fundamental problems in computational quantum chemistry and explore how matter interacts with light",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "AI achieves silver-medal standard solving International Mathematical Olympiad problems",
      "titleJa": "Ai achieves silver-medal standard solving international mathematical olympiad problems",
      "link": "https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/",
      "pubDate": "Thu, 25 Jul 2024 15:29:00 +0000",
      "content": "Breakthrough models AlphaProof and AlphaGeometry 2 solve advanced reasoning problems in mathematics",
      "summary": "Breakthrough models AlphaProof and AlphaGeometry 2 solve advanced reasoning problems in mathematics",
      "summaryJa": "ブレークスルー models alphaproof and alphageometry 2 solve advanced reasoning problems in mathematics",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Google DeepMind at ICML 2024",
      "titleJa": "Google deepmind at icml 2024",
      "link": "https://deepmind.google/discover/blog/google-deepmind-at-icml-2024/",
      "pubDate": "Fri, 19 Jul 2024 10:00:00 +0000",
      "content": "Exploring AGI, the challenges of scaling and the future of multimodal generative AI",
      "summary": "Exploring AGI, the challenges of scaling and the future of multimodal generative AI",
      "summaryJa": "Exploring agi, the challenges of scaling and the future of multimodal 生成AI",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Gemini breaks new ground: a faster model, longer context and AI agents",
      "titleJa": "Gemini breaks new ground: a faster モデル, longer context and ai agents",
      "link": "https://deepmind.google/discover/blog/gemini-breaks-new-ground-a-faster-model-longer-context-and-ai-agents/",
      "pubDate": "Tue, 14 May 2024 17:58:00 +0000",
      "content": "We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants.",
      "summary": "We’re introducing a series of updates across the Gemini family of models, including the new 1.",
      "summaryJa": "We’re introducing a series of updates across the gemini family of models, including the new 1.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Watermarking AI-generated text and video with SynthID",
      "titleJa": "Watermarking ai-generated text and video with synthid",
      "link": "https://deepmind.google/discover/blog/watermarking-ai-generated-text-and-video-with-synthid/",
      "pubDate": "Tue, 14 May 2024 17:56:00 +0000",
      "content": "Announcing our novel watermarking method for AI-generated text and video, and how we’re bringing SynthID to key Google products",
      "summary": "Announcing our novel watermarking method for AI-generated text and video, and how we’re bringing SynthID to key Google products",
      "summaryJa": "Announcing our novel watermarking method for ai-generated text and video, and how we’re bringing synthid to key google products",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "AlphaFold 3 predicts the structure and interactions of all of life’s molecules",
      "titleJa": "Alphafold 3 predicts the structure and interactions of all of life’s molecules",
      "link": "https://deepmind.google/discover/blog/alphafold-3-predicts-the-structure-and-interactions-of-all-lifes-molecules/",
      "pubDate": "Wed, 08 May 2024 16:00:00 +0000",
      "content": "Introducing a new AI model developed by Google DeepMind and Isomorphic Labs.",
      "summary": "Introducing a new AI model developed by Google DeepMind and Isomorphic Labs.",
      "summaryJa": "Introducing a new ai モデル developed by google deepmind and isomorphic labs.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Google DeepMind at ICLR 2024",
      "titleJa": "Google deepmind at iclr 2024",
      "link": "https://deepmind.google/discover/blog/google-deepmind-at-iclr-2024/",
      "pubDate": "Fri, 03 May 2024 13:39:00 +0000",
      "content": "Developing next-gen AI agents, exploring new modalities, and pioneering foundational learning",
      "summary": "Developing next-gen AI agents, exploring new modalities, and pioneering foundational learning",
      "summaryJa": "Developing next-gen ai agents, exploring new modalities, and pioneering foundational learning",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Generative AI to quantify uncertainty in weather forecasting",
      "titleJa": "生成AI to quantify uncertainty in weather forecasting",
      "link": "http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html",
      "pubDate": "2024-03-29T18:03:00.000Z",
      "content": "Posted by Lizao (Larry) Li, Software Engineer, and Rob Carver, Research Scientist, Google Research\n\n\n\n\nAccurate weather forecasts can have a direct impact on people’s lives, from helping make routine decisions, like what to pack for a day’s activities, to informing urgent actions, for example, protecting people in the face of hazardous weather conditions. The importance of accurate and timely weather forecasts will only increase as the climate changes. Recognizing this, we at Google have been investing in weather and climate research to help ensure that the forecasting technology of tomorrow can meet the demand for reliable weather information. Some of our recent innovations include MetNet-3, Google's high-resolution forecasts up to 24-hours into the future, and GraphCast, a weather model that can predict weather up to 10 days ahead.\n\n \n\n\nWeather is inherently stochastic. To quantify the uncertainty, traditional methods rely on physics-based simulation to generate an ensemble of forecasts. However, it is computationally costly to generate a large ensemble so that rare and extreme weather events can be discerned and characterized accurately.  \n\n\nWith that in mind, we are excited to announce our latest innovation designed to accelerate progress in weather forecasting, Scalable Ensemble Envelope Diffusion Sampler (SEEDS), recently published in Science Advances. SEEDS is a generative AI model that can efficiently generate ensembles of weather forecasts at scale at a small fraction of the cost of traditional physics-based forecasting models. This technology opens up novel opportunities for weather and climate science, and it represents one of the first applications to weather and climate forecasting of probabilistic diffusion models, a generative AI technology behind recent advances in media generation.\n\n \n\nThe need for probabilistic forecasts: the butterfly effect\nAmerican Association for the Advancement of Science meeting in Washington, D.C., MIT meteorology professor Ed Lorenz gave a talk entitled, “Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?” which contributed to the term “butterfly effect”. He was building on his earlier, landmark 1963 paper where he examined the feasibility of “very-long-range weather prediction” and described how errors in initial conditions grow exponentially when integrated in time with numerical weather prediction models. This exponential error growth, known as chaos, results in a deterministic predictability limit that restricts the use of individual forecasts in decision making, because they do not quantify the inherent uncertainty of weather conditions. This is particularly problematic when forecasting extreme weather events, such as hurricanes, heatwaves, or floods.\n\n\nRecognizing the limitations of deterministic forecasts, weather agencies around the world issue probabilistic forecasts. Such forecasts are based on ensembles of deterministic forecasts, each of which is generated by including synthetic noise in the initial conditions and stochasticity in the physical processes. Leveraging the fast error growth rate in weather models, the forecasts in an ensemble are purposefully different: the initial uncertainties are tuned to generate runs that are as different as possible and the stochastic processes in the weather model introduce additional differences during the model run. The error growth is mitigated by averaging all the forecasts in the ensemble and the variability in the ensemble of forecasts quantifies the uncertainty of the weather conditions.\n\n\nWhile effective, generating these probabilistic forecasts is computationally costly. They require running highly complex numerical weather models on massive supercomputers multiple times. Consequently, many operational weather forecasts can only afford to generate ~10–50 ensemble members for each forecast cycle. This is a problem for users concerned with the likelihood of rare but high-impact weather events, which typically require much larger ensembles to assess beyond a few days. For instance, one would need a 10,000-member ensemble to forecast the likelihood of events with 1% probability of occurrence with a relative error less than 10%. Quantifying the probability of such extreme events could be useful, for example, for emergency management preparation or for energy traders.\n\n \n\nSEEDS: AI-enabled advances\npaper, we present the Scalable Ensemble Envelope Diffusion Sampler (SEEDS), a generative AI technology for weather forecast ensemble generation. SEEDS is based on denoising diffusion probabilistic models, a state-of-the-art generative AI method pioneered in part by Google Research.\n\n\nSEEDS can generate a large ensemble conditioned on as few as one or two forecasts from an operational numerical weather prediction system. The generated ensembles not only yield plausible real-weather–like forecasts but also match or exceed physics-based ensembles in skill metrics such as the rank histogram, the root-mean-squared error (RMSE), and the continuous ranked probability score (CRPS). In particular, the generated ensembles assign more accurate likelihoods to the tail of the forecast distribution, such as ±2σ and ±3σ weather events. Most importantly, the computational cost of the model is negligible when compared to the hours of computational time needed by supercomputers to make a forecast. It has a throughput of 256 ensemble members (at 2° resolution) per 3 minutes on Google Cloud TPUv3-32 instances and can easily scale to higher throughput by deploying more accelerators. \n\n\n\n\nSEEDS generates an order-of-magnitude more samples to in-fill distributions of weather patterns.\n\nGenerating plausible weather forecasts\nGlobal Ensemble Forecast System, GEFS) for a particular date during the 2022 European heat waves. We also compare the results to the forecasts from a Gaussian model that predicts the univariate mean and standard deviation of each atmospheric field at each location, a common and computationally efficient but less sophisticated data-driven approach. This Gaussian model is meant to characterize the output of pointwise post-processing, which ignores correlations and treats each grid point as an independent random variable. In contrast, a real weather map would have detailed correlational structures. \n\n\nBecause SEEDS directly models the joint distribution of the atmospheric state, it realistically captures both the spatial covariance and the correlation between mid-tropospheric geopotential and mean sea level pressure, both of which are closely related and are commonly used by weather forecasters for evaluation and verification of forecasts. Gradients in the mean sea level pressure are what drive winds at the surface, while gradients in mid-tropospheric geopotential create upper-level winds that move large-scale weather patterns. \n\n\nThe generated samples from SEEDS shown in the figure below (frames Ca–Ch) display a geopotential trough west of Portugal with spatial structure similar to that found in the operational U.S. forecasts or the reanalysis based on observations. Although the Gaussian model predicts the marginal univariate distributions adequately, it fails to capture cross-field or spatial correlations. This hinders the assessment of the effects that these anomalies may have on hot air intrusions from North Africa, which can exacerbate heat waves over Europe.\n\n\n\n\nStamp maps over Europe on 2022/07/14 at 0:00 UTC. The contours are for the mean sea level pressure (dashed lines mark isobars below 1010 hPa) while the heatmap depicts the geopotential height at the 500 hPa pressure level. (A) The ERA5 reanalysis, a proxy for real observations. (Ba-Bb) 2 members from the 7-day U.S. operational forecasts used as seeds to our model. (Ca-Ch) 8 samples drawn from SEEDS. (Da-Dh) 8 non-seeding members from the 7-day U.S. operational ensemble forecast. (Ea-Ed) 4 samples from a pointwise Gaussian model parameterized by the mean and variance of the entire U.S. operational ensemble.\n\nCovering extreme events more accurately  \n\n\nSEEDS provides better statistical coverage of the 2022/07/14 European extreme heat event, denoted by the brown star . Each plot shows the values of the total column-integrated water vapor (TCVW) vs. temperature over a grid point near Lisbon, Portugal from 16,384 samples generated by our models, shown as green dots, conditioned on 2 seeds (blue squares) taken from the 7-day U.S. operational ensemble forecasts (denoted by the sparser brown triangles). The valid forecast time is 1:00 local time. The solid contour levels correspond to iso-proportions of the kernel density of SEEDS, with the outermost one encircling 95% of the mass and 11.875% between each level.\n\n \n\nConclusion and future outlook\n \n\nAcknowledgements\nAll SEEDS authors, Lizao Li, Rob Carver, Ignacio Lopez-Gomez, Fei Sha and John Anderson, co-authored this blog post, with Carla Bromberg as Program Lead. We also thank Tom Small who designed the animation. Our colleagues at Google Research have provided invaluable advice to the SEEDS work. Among them, we thank Leonardo Zepeda-Núñez, Zhong Yi Wan, Stephan Rasp, Stephan Hoyer, and Tapio Schneider for their inputs and useful discussion. We thank Tyler Russell for additional technical program management, as well as Alex Merose for data coordination and support. We also thank Cenk Gazen, Shreya Agrawal, and Jason Hickey for discussions in the early stage of the SEEDS work.",
      "summary": "Posted by Lizao (Larry) Li, Software Engineer, and Rob Carver, Research Scientist, Google Research Accurate weather forecasts can have a direct impact...",
      "summaryJa": "Posted by lizao (larry) li, software engineer, and rob carver, 研究 scientist, google 研究 accurate weather forecasts can have a direct impact...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "AutoBNN: Probabilistic time series forecasting with compositional bayesian neural networks",
      "titleJa": "Autobnn: probabilistic time series forecasting with compositional bayesian neural networks",
      "link": "http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html",
      "pubDate": "2024-03-28T20:53:00.000Z",
      "content": "Posted by Urs Köster, Software Engineer, Google Research\n\n\n\n\nTime series problems are ubiquitous, from forecasting weather and traffic patterns to understanding economic trends. Bayesian approaches start with an assumption about the data's patterns (prior probability), collecting evidence (e.g., new time series data), and continuously updating that assumption to form a posterior probability distribution. Traditional Bayesian approaches like Gaussian processes (GPs) and Structural Time Series are extensively used for modeling time series data, e.g., the commonly used Mauna Loa CO2 dataset. However, they often rely on domain experts to painstakingly select appropriate model components and may be computationally expensive. Alternatives such as neural networks lack interpretability, making it difficult to understand how they generate forecasts, and don't produce reliable confidence intervals. \n\n\n\nTo that end, we introduce AutoBNN, a new open-source package written in JAX. AutoBNN automates the discovery of interpretable time series forecasting models, provides high-quality uncertainty estimates, and scales effectively for use on large datasets. We describe how AutoBNN combines the interpretability of traditional probabilistic approaches with the scalability and flexibility of neural networks.\n\n\n\n    \nAutoBNN\nline of research that over the past decade has yielded improved predictive accuracy by modeling time series using GPs with learned kernel structures. The kernel function of a GP encodes assumptions about the function being modeled, such as the presence of trends, periodicity or noise.  With learned GP kernels, the kernel function is defined compositionally: it is either a base kernel (such as Linear, Quadratic, Periodic, Matérn or ExponentiatedQuadratic) or a composite that combines two or more kernel functions using operators such as Addition, Multiplication, or ChangePoint. This compositional kernel structure serves two related purposes. First, it is simple enough that a user who is an expert about their data, but not necessarily about GPs, can construct a reasonable prior for their time series. Second, techniques like Sequential Monte Carlo can be used for discrete searches over small structures and can output interpretable results.\nBayesian neural networks (BNNs) while retaining the compositional kernel structure. A BNN is a neural network with a probability distribution over weights rather than a fixed set of weights. This induces a distribution over outputs, capturing uncertainty in the predictions. BNNs bring the following advantages over GPs: First, training large GPs is computationally expensive, and traditional training algorithms scale as the cube of the number of data points in the time series. In contrast, for a fixed width, training a BNN will often be approximately linear in the number of data points. Second, BNNs lend themselves better to GPU and TPU hardware acceleration than GP training operations. Third, compositional BNNs can be easily combined with traditional deep BNNs, which have the ability to do feature discovery. One could imagine \"hybrid\" architectures, in which users specify a top-level structure of Add(Linear, Periodic, Deep), and the deep BNN is left to learn the contributions from potentially high-dimensional covariate information.\n\n\n\nHow might one translate a GP with compositional kernels into a BNN then? A single layer neural network will typically converge to a GP as the number of neurons (or \"width\") goes to infinity. More recently, researchers have discovered a correspondence in the other direction — many popular GP kernels (such as Matern, ExponentiatedQuadratic, Polynomial or Periodic) can be obtained as infinite-width BNNs with appropriately chosen activation functions and weight distributions. Furthermore, these BNNs remain close to the corresponding GP even when the width is very much less than infinite. For example, the figures below show the difference in the covariance between pairs of observations, and regression results of the true GPs and their corresponding width-10 neural network versions.\n\n\n\n\nComparison of Gram matrices between true GP kernels (top row) and their width 10 neural network approximations (bottom row).\n\n\n\nComparison of regression results between true GP kernels (top row) and their width 10 neural network approximations (bottom row).\n\nBNN analogues of the Addition and Multiplication operators over GPs, and input warping to produce periodic kernels. BNN addition is straightforwardly given by adding the outputs of the component BNNs. BNN multiplication is achieved by multiplying the activations of the hidden layers of the BNNs and then applying a shared dense layer. We are therefore limited to only multiplying BNNs with the same hidden width.\n\n\n\n    \nUsing AutoBNN\npackage is available within Tensorflow Probability. It is implemented in JAX and uses the flax.linen neural network library. It implements all of the base kernels and operators discussed so far (Linear, Quadratic, Matern, ExponentiatedQuadratic, Periodic, Addition, Multiplication) plus one new kernel and three new operators:  \n\n\n\n\na OneLayer kernel, a single hidden layer ReLU BNN,\n\n\na ChangePoint operator that allows smoothly switching between two kernels,\n\n\na LearnableChangePoint operator which is the same as ChangePoint except position and slope are given prior distributions and can be learnt from the data, and\n\n\na WeightedSum operator.\n\n\n\n\n\nWeightedSum combines two or more BNNs with learnable mixing weights, where the learnable weights follow a Dirichlet prior. By default, a flat Dirichlet distribution with concentration 1.0 is used.\n\n\n\nWeightedSums allow a \"soft\" version of structure discovery, i.e., training a linear combination of many possible models at once. In contrast to structure discovery with discrete structures, such as in AutoGP, this allows us to use standard gradient methods to learn structures, rather than using expensive discrete optimization. Instead of evaluating potential combinatorial structures in series, WeightedSum allows us to evaluate them in parallel. \n\n\n\nTo easily enable exploration, AutoBNN defines a number of model structures that contain either top-level or internal WeightedSums. The names of these models can be used as the first parameter in any of the estimator constructors, and include things like sum_of_stumps (the WeightedSum over all the base kernels) and sum_of_shallow (which adds all possible combinations of base kernels with all operators).\n\n\nIllustration of the sum_of_stumps model. The bars in the top row show the amount by which each base kernel contributes, and the bottom row shows the function represented by the base kernel. The resulting weighted sum is shown on the right.\n\nM3 dataset. The six base structures were ExponentiatedQuadratic (which is the same as the Radial Basis Function kernel, or RBF for short), Matern, Linear, Quadratic, OneLayer and Periodic kernels. The figure shows the MAP estimates of their weights over an ensemble of 32 particles. All of the high likelihood particles gave a large weight to the Periodic component, low weights to Linear, Quadratic and OneLayer, and a large weight to either RBF or Matern.\n\n\n\n\n\n\nParallel coordinates plot of the MAP estimates of the base kernel weights over 32 particles. The sum_of_stumps model was trained on the N374 series from the M3 dataset (insert in blue). Darker lines correspond to particles with higher likelihoods.\n\nWeightedSums as the inputs to other operators, it is possible to express rich combinatorial structures, while keeping models compact and the number of learnable weights small. As an example, we include the sum_of_products model (illustrated in the figure below) which first creates a pairwise product of two WeightedSums, and then a sum of the two products. By setting some of the weights to zero, we can create many different discrete structures. The total number of possible structures in this model is 216, since there are 16 base kernels that can be turned on or off. All these structures are explored implicitly by training just this one model.\n\n\n\n\n\nIllustration of the \"sum_of_products\" model. Each of the four WeightedSums have the same structure as the \"sum_of_stumps\" model.\n\nPeriodic and either the Matern or ExponentiatedQuadratic) lead to overfitting on many datasets. To prevent this, we have defined model classes like sum_of_safe_shallow that exclude such products when performing structure discovery with WeightedSums.\n\n\n\nFor training, AutoBNN provides AutoBnnMapEstimator and AutoBnnMCMCEstimator to perform MAP and MCMC inference, respectively. Either estimator can be combined with any of the six likelihood functions, including four based on normal distributions with different noise characteristics for continuous data and two based on the negative binomial distribution for count data.  \n\n\n\n\n\n\nResult from running AutoBNN on the Mauna Loa CO2 dataset in our example colab. The model captures the trend and seasonal component in the data. Extrapolating into the future, the mean prediction slightly underestimates the actual trend, while the 95% confidence interval gradually increases.\n\nscikit-learn–inspired estimator interface:\nimport autobnn as ab\n\nmodel = ab.operators.Add(\n    bnns=(ab.kernels.PeriodicBNN(width=50),\n          ab.kernels.LinearBNN(width=50),\n          ab.kernels.MaternBNN(width=50)))\n\nestimator = ab.estimators.AutoBnnMapEstimator(\n    model, 'normal_likelihood_logistic_noise', jax.random.PRNGKey(42),\n    periods=[12])\n\nestimator.fit(my_training_data_xs, my_training_data_ys)\nlow, mid, high = estimator.predict_quantiles(my_training_data_xs)\n\n\n\n\n\n    \nConclusion\nAutoBNN provides a powerful and flexible framework for building sophisticated time series prediction models. By combining the strengths of BNNs and GPs with compositional kernels, AutoBNN opens a world of possibilities for understanding and forecasting complex data. We invite the community to try the colab, and leverage this library to innovate and solve real-world challenges. \n\n\n\n    \nAcknowledgements\nAutoBNN was written by Colin Carroll, Thomas Colthurst, Urs Köster and Srinivas Vasudevan. We would like to thank Kevin Murphy, Brian Patton and Feras Saad for their advice and feedback.",
      "summary": "Posted by Urs Köster, Software Engineer, Google Research Time series problems are ubiquitous, from forecasting weather and traffic patterns to underst...",
      "summaryJa": "Posted by urs köster, software engineer, google 研究 time series problems are ubiquitous, from forecasting weather and traffic patterns to underst...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Computer-aided diagnosis for lung cancer screening",
      "titleJa": "Computer-aided 診断 for lung cancer screening",
      "link": "http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html",
      "pubDate": "2024-03-20T20:54:00.000Z",
      "content": "Posted by Atilla Kiraly, Software Engineer, and Rory Pilgrim, Product Manager, Google Research \n\n\n\n\n\nLung cancer is the leading cause of cancer-related deaths globally with 1.8 million deaths reported in 2020. Late diagnosis dramatically reduces the chances of survival. Lung cancer screening via computed tomography (CT), which provides a detailed 3D image of the lungs, has been shown to reduce mortality in high-risk populations by at least 20% by detecting potential signs of cancers earlier. In the US, screening involves annual scans, with some countries or cases recommending more or less frequent scans. \n\n\n\nThe United States Preventive Services Task Force recently expanded lung cancer screening recommendations by roughly 80%, which is expected to increase screening access for women and racial and ethnic minority groups. However, false positives (i.e., incorrectly reporting a potential cancer in a cancer-free patient) can cause anxiety and lead to unnecessary procedures for patients while increasing costs for the healthcare system. Moreover, efficiency in screening a large number of individuals can be challenging depending on healthcare infrastructure and radiologist availability.\n\n\n\n\nAt Google we have previously developed machine learning (ML) models for lung cancer detection, and have evaluated their ability to automatically detect and classify regions that show signs of potential cancer. Performance has been shown to be comparable to that of specialists in detecting possible cancer. While they have achieved high performance, effectively communicating findings in realistic environments is necessary to realize their full potential.\n\n\n\nTo that end, in “Assistive AI in Lung Cancer Screening: A Retrospective Multinational Study in the US and Japan”, published in Radiology AI, we investigate how ML models can effectively communicate findings to radiologists. We also introduce a generalizable user-centric interface to help radiologists leverage such models for lung cancer screening. The system takes CT imaging as input and outputs a cancer suspicion rating using four categories (no suspicion, probably benign, suspicious, highly suspicious) along with the corresponding regions of interest. We evaluate the system’s utility in improving clinician performance through randomized reader studies in both the US and Japan, using the local cancer scoring systems (Lung-RADSs V1.1 and Sendai Score) and image viewers that mimic realistic settings. We found that reader specificity increases with model assistance in both reader studies. To accelerate progress in conducting similar studies with ML models, we have open-sourced code to process CT images and generate images compatible with the picture archiving and communication system (PACS) used by radiologists. \n\n\n\n    \nDeveloping an interface to communicate model results\nalpha-numeric score to indicate the lung cancer risk and follow-up recommendations. When assessing patients, radiologists load the CT in their workstation to read the case, find lung nodules or lesions, and apply set guidelines to determine follow-up decisions. \n\n\n\n\nOur first step was to improve the previously developed ML models through additional training data and architectural improvements, including self-attention. Then, instead of targeting specific guidelines, we experimented with a complementary way of communicating AI results independent of guidelines or their particular versions. Specifically, the system output offers a suspicion rating and localization (regions of interest) for the user to consider in conjunction with their own specific guidelines. The interface produces output images directly associated with the CT study, requiring no changes to the user’s workstation. The radiologist only needs to review a small set of additional images. There is no other change to their system or interaction with the system.\n\n\n\n\n\n\n\n\n\nExample of the assistive lung cancer screening system outputs. Results for the radiologist’s evaluation are visualized on the location of the CT volume where the suspicious lesion is found. The overall suspicion is displayed at the top of the CT images. Circles highlight the suspicious lesions while squares show a rendering of the same lesion from a different perspective, called a sagittal view.\n\nprior work. The models coordinate with each other to first segment the lungs, obtain an overall assessment, locate three suspicious regions, then use the information to assign a suspicion rating to each region. The system was deployed on Google Cloud using a Google Kubernetes Engine (GKE) that pulled the images, ran the ML models, and provided results. This allows scalability and directly connects to servers where the images are stored in DICOM stores.\n\n\n\n\n\n\nOutline of the Google Cloud deployment of the assistive lung cancer screening system and the directional calling flow for the individual components that serve the images and compute results. Images are served to the viewer and to the system using Google Cloud services. The system is run on a Google Kubernetes Engine that pulls the images, processes them, and writes them back into the DICOM store.\n\nReader studies \narea under the ROC curve (AUC) values. These were compared with and without assistance.\n\n\n\n\n\nA multi-case multi-reader study involves each case being reviewed by each reader twice, once with ML system assistance and once without. In this visualization one reader first reviews Set A without assistance (blue) and then with assistance (orange) after a wash-out period. A second reader group follows the opposite path by reading the same set of cases Set A with assistance first. Readers are randomized to these groups to remove the effect of ordering.\n\nspecificity) by an absolute 5–7% compared to when they didn’t use the assistive system. This potentially means that for every 15–20 patients screened, one may be able to avoid unnecessary follow-up procedures, thus reducing their anxiety and the burden on the health care system. This can, in turn, help improve the sustainability of lung cancer screening programs, particularly as more people become eligible for screening. \n\n\n\n\n\nReader specificity increases with ML model assistance in both the US-based and Japan-based reader studies. Specificity values were derived from reader scores from actionable findings (something suspicious was found) versus no actionable findings, compared against the true cancer outcome of the individual.  Under model assistance, readers flagged fewer cancer-negative individuals for follow-up visits. Sensitivity for cancer positive individuals remained the same.\n\nTranslating this into real-world impact through partnership \nDeepHealth, a leading AI-powered health informatics provider; and Apollo Radiology International a leading provider of Radiology services in India to explore paths for incorporating this system into future products. In addition, we are looking to help other researchers studying how best to integrate ML model results into clinical workflows by open sourcing code used for the reader study and incorporating the insights described in this blog. We hope that this will help accelerate medical imaging researchers looking to conduct reader studies for their AI models, and catalyze translational research in the field.  \n\n\n\n\n    \nAcknowledgements\nKey contributors to this project include Corbin Cunningham, Zaid Nabulsi, Ryan Najafi, Jie Yang, Charles Lau, Joseph R. Ledsam, Wenxing Ye, Diego Ardila, Scott M. McKinney, Rory Pilgrim, Hiroaki Saito, Yasuteru Shimamura, Mozziyar Etemadi, Yun Liu, David Melnick, Sunny Jansen, Nadia Harhen, David P. Nadich, Mikhail Fomitchev, Ziyad Helali, Shabir Adeel, Greg S. Corrado, Lily Peng, Daniel Tse, Shravya Shetty, Shruthi Prabhakara, Neeral Beladia, and Krish Eswaran. Thanks to Arnav Agharwal and Andrew Sellergren for their open sourcing support and Vivek Natarajan and Michael D. Howell for their feedback. Sincere appreciation also goes to the radiologists who enabled this work with their image interpretation and annotation efforts throughout the study, and Jonny Wong and Carli Sampson for coordinating the reader studies.",
      "summary": "Posted by Atilla Kiraly, Software Engineer, and Rory Pilgrim, Product Manager, Google Research Lung cancer is the leading cause of cancer-related deat...",
      "summaryJa": "Posted by atilla kiraly, software engineer, and rory pilgrim, product manager, google 研究 lung cancer is the leading cause of cancer-related deat...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Using AI to expand global access to reliable flood forecasts",
      "titleJa": "Using ai to expand global access to reliable flood forecasts",
      "link": "http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html",
      "pubDate": "2024-03-20T16:06:00.000Z",
      "content": "Posted by Yossi Matias, VP Engineering & Research, and Grey Nearing, Research Scientist, Google Research\n\n\n\n\nFloods are the most common natural disaster, and are responsible for roughly $50 billion in annual financial damages worldwide. The rate of flood-related disasters has more than doubled since the year 2000 partly due to climate change. Nearly 1.5 billion people, making up 19% of the world’s population, are exposed to substantial risks from severe flood events. Upgrading early warning systems to make accurate and timely information accessible to these populations can save thousands of lives per year. \n\n\n\nDriven by the potential impact of reliable flood forecasting on people’s lives globally, we started our flood forecasting effort in 2017. Through this multi-year journey, we advanced research over the years hand-in-hand with building a real-time operational flood forecasting system that provides alerts on Google Search, Maps, Android notifications and through the Flood Hub. However, in order to scale globally, especially in places where accurate local data is not available, more research advances were required.\n\n\n\nIn “Global prediction of extreme floods in ungauged watersheds”, published in Nature, we demonstrate how machine learning (ML) technologies can significantly improve global-scale flood forecasting relative to the current state-of-the-art for countries where flood-related data is scarce. With these AI-based technologies we extended the reliability of currently-available global nowcasts, on average, from zero to five days, and improved forecasts across regions in Africa and Asia to be similar to what are currently available in Europe. The evaluation of the models was conducted in collaboration with the European Center for Medium Range Weather Forecasting (ECMWF).\n\n\n\nThese technologies also enable Flood Hub to provide real-time river forecasts up to seven days in advance, covering river reaches across over 80 countries. This information can be used by people, communities, governments and international organizations to take anticipatory action to help protect vulnerable populations.\n\n\n\n\nFlood forecasting at Google \nlaunched a pilot early warning system in the Ganges-Brahmaputra river basin in India, with the hypothesis that ML could help address the challenging problem of reliable flood forecasting at scale. The pilot was further expanded the following year via the combination of an inundation model, real-time water level measurements, the creation of an elevation map and hydrologic modeling.\n\n\n\nIn collaboration with academics, and, in particular, with the JKU Institute for Machine Learning we explored ML-based hydrologic models, showing that LSTM-based models could produce more accurate simulations than traditional conceptual and physics-based hydrology models. This research led to flood forecasting improvements that enabled the expansion of our forecasting coverage to include all of India and Bangladesh. We also worked with researchers at Yale University to test technological interventions that increase the reach and impact of flood warnings.\n\n\n\nOur hydrological models predict river floods by processing publicly available weather data like precipitation and physical watershed information. Such models must be calibrated to long data records from streamflow gauging stations in individual rivers. A low percentage of global river watersheds (basins) have streamflow gauges, which are expensive but necessary to supply relevant data, and it’s challenging for hydrological simulation and forecasting to provide predictions in basins that lack this infrastructure. Lower gross domestic product (GDP) is correlated with increased vulnerability to flood risks, and there is an inverse correlation between national GDP and the amount of publicly available data in a country. ML helps to address this problem by allowing a single model to be trained on all available river data and to be applied to ungauged basins where no data are available. In this way, models can be trained globally, and can make predictions for any river location.\n\n\n\n\n\nThere is an inverse (log-log) correlation between the amount of publicly available streamflow data in a country and national GDP. Streamflow data from the Global Runoff Data Center.\n\nestimate uncertainty in river forecasts and showed how ML river forecast models synthesize information from multiple data sources. They demonstrated that these models can simulate extreme events reliably, even when those events are not part of the training data. In an effort to contribute to open science, in 2023 we open-sourced a community-driven dataset for large-sample hydrology in Nature Scientific Data. \n\n\n\n\n    \nThe river forecast model\nLSTMs perform well on the task of river forecasting.\n\n\n\n\n\nA diagram of the LSTM, which is a neural network that operates sequentially in time. An accessible primer can be found here.\n\nmixture density networks to produce a probabilistic forecast (i.e., predicted parameters of a probability distribution over streamflow). Specifically, the model predicts the parameters of a mixture of heavy-tailed probability density functions, called asymmetric Laplacian distributions, at each forecast time step. The result is a mixture density function, called a Countable Mixture of Asymmetric Laplacians (CMAL) distribution, which represents a probabilistic prediction of the volumetric flow rate in a particular river at a particular time. \n\n\n\n\n\nLSTM-based river forecast model architecture. Two LSTMs are applied in sequence, one ingesting historical weather data and one ingesting forecasted weather data. The model outputs are the parameters of a probability distribution over streamflow at each forecasted timestep.\n\nInput and training data\nStatic watershed attributes representing geographical and geophysical variables: From the HydroATLAS project, including data like long-term climate indexes (precipitation, temperature, snow fractions), land cover, and anthropogenic attributes (e.g., a nighttime lights index as a proxy for human development). \n\n\nHistorical meteorological time-series data: Used to spin up the model for one year prior to the issue time of a forecast. The data comes from NASA IMERG, NOAA  CPC  Global Unified Gauge-Based Analysis of Daily Precipitation, and the ECMWF ERA5-land reanalysis. Variables include daily total precipitation, air temperature, solar and thermal radiation, snowfall, and surface pressure. \n\n\nForecasted meteorological time series over a seven-day forecast horizon: Used as input for the forecast LSTM. These data are the same meteorological variables listed above, and come from the ECMWF HRES atmospheric model.\n\n\n\n\nTraining data are daily streamflow values from the Global Runoff Data Center over the time period 1980 - 2023. A single streamflow forecast model is trained using data from 5,680 diverse watershed streamflow gauges (shown below) to improve accuracy.\n\n\n\n\n\nLocation of 5,680 streamflow gauges that supply training data for the river forecast model from the Global Runoff Data Center.\n\n  \nImproving on the current state-of-the-art\nGloFAS version 4, the current state-of-the-art global flood forecasting system. These experiments showed that ML can provide accurate warnings earlier and over larger and more impactful events. \n\n\n\nThe figure below shows the distribution of F1 scores when predicting different severity events at river locations around the world, with plus or minus 1 day accuracy. F1 scores are an average of precision and recall and event severity is measured by return period. For example, a 2-year return period event is a volume of streamflow that is expected to be exceeded on average once every two years. Our model achieves reliability scores at up to 4-day or 5-day lead times that are similar to or better, on average, than the reliability of GloFAS nowcasts (0-day lead time). \n\n\n\n\n\nDistributions of F1 scores over 2-year return period events in 2,092 watersheds globally during the time period 2014-2023 from GloFAS (blue) and our model (orange) at different lead times. On average, our model is statistically as accurate as GloFAS nowcasts (0–day lead time) up to 5 days in advance over 2-year (shown) and 1-year, 5-year, and 10-year events (not shown).\n\npaper for more information.\n\n\n\n    \nLooking into the future\nAdaptation and Resilience efforts and reflects Google's commitment to address climate change while helping global communities become more resilient. We believe that AI and ML will continue to play a critical role in helping advance science and research towards climate action.\n\n\n\nWe actively collaborate with several international aid organizations (e.g., the Centre for Humanitarian Data and the Red Cross) to provide actionable flood forecasts. Additionally, in an ongoing collaboration with the World Meteorological Organization (WMO) to support early warning systems for climate hazards, we are conducting a study to help understand how AI can help address real-world challenges faced by national flood forecasting agencies. \n\n\n\nWhile the work presented here demonstrates a significant step forward in flood forecasting, future work  is needed to further expand flood forecasting coverage to more locations globally and other types of flood-related events and disasters, including flash floods and urban floods. We are looking forward to continuing collaborations with our partners in the academic and expert communities, local governments and the industry to reach these goals.",
      "summary": "Posted by Yossi Matias, VP Engineering & Research, and Grey Nearing, Research Scientist, Google Research Floods are the most common natural disaster, ...",
      "summaryJa": "Posted by yossi matias, vp engineering & 研究, and grey nearing, 研究 scientist, google 研究 floods are the most common natural disaster, ...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 65
    },
    {
      "title": "ScreenAI: A visual language model for UI and visually-situated language understanding",
      "titleJa": "Screenai: a visual language モデル for ui and visually-situated language understanding",
      "link": "http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html",
      "pubDate": "2024-03-19T20:15:00.000Z",
      "content": "Posted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google Research\n\n\n\n\n\nScreen user interfaces (UIs) and infographics, such as charts, diagrams and tables, play important roles in human communication and human-machine interaction as they facilitate rich and interactive user experiences. UIs and infographics share similar design principles and visual language (e.g., icons and layouts), that offer an opportunity to build a single model that can understand, reason, and interact with these interfaces. However, because of their complexity and varied presentation formats, infographics and UIs present a unique modeling challenge.\n\n\n\nTo that end, we introduce “ScreenAI: A Vision-Language Model for UI and Infographics Understanding”. ScreenAI improves upon the PaLI architecture with the flexible patching strategy from pix2struct. We train ScreenAI on a unique mixture of datasets and tasks, including a novel Screen Annotation task that requires the model to identify UI element information (i.e., type, location and description) on a screen. These text annotations provide large language models (LLMs) with screen descriptions, enabling them to automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art results on UI- and infographic-based tasks (WebSRC and MoTIF), and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability. \n\n\n\n    \nScreenAI\nPaLI, composed of a multimodal encoder block and an autoregressive decoder. The PaLI encoder uses a vision transformer (ViT) that creates image embeddings and a multimodal encoder that takes the concatenation of the image and text embeddings as input. This flexible architecture allows ScreenAI to solve vision tasks that can be recast as text+image-to-text problems. \n\n\n\nOn top of the PaLI architecture, we employ a flexible patching strategy introduced in pix2struct. Instead of using a fixed-grid pattern, the grid dimensions are selected such that they preserve the native aspect ratio of the input image. This enables ScreenAI to work well across images of various aspect ratios. \n\n\n\nThe ScreenAI model is trained in two stages: a pre-training stage followed by a fine-tuning stage. First, self-supervised learning is applied to automatically generate data labels, which are then used to train ViT and the language model. ViT is frozen during the fine-tuning stage, where most data used is manually labeled by human raters. \n\n\n\n\n\nScreenAI model architecture.\n\nData generation\npublicly accessible web pages and following the programmatic exploration approach used for the RICO dataset for mobile apps. We then apply a layout annotator, based on the DETR model, that identifies and labels a wide range of UI elements (e.g., image, pictogram, button, text) and their spatial relationships. Pictograms undergo further analysis using an icon classifier capable of distinguishing 77 different icon types. This detailed classification is essential for interpreting the subtle information conveyed through icons. For icons that are not covered by the classifier, and for infographics and images, we use the PaLI image captioning model to generate descriptive captions that provide contextual information. We also apply an optical character recognition (OCR) engine to extract and annotate textual content on screen. We combine the OCR text with the previous annotations to create a detailed description of each screen.\n\n\n\n\n\nA mobile app screenshot with generated annotations that include UI elements and their descriptions, e.g., TEXT elements also contain the text content from OCR, IMAGE elements contain image captions, LIST_ITEMs contain all their child elements.\n\nLLM-based data generation\nPaLM 2 to generate input-output pairs in a two-step process. First, screen annotations are generated using the technique outlined above, then we craft a prompt around this schema for the LLM to create synthetic data. This process requires prompt engineering and iterative refinement to find an effective prompt. We assess the generated data's quality through human validation against a quality threshold. \n\n\n\n\nYou only speak JSON. Do not write text that isn’t JSON.\nYou are given the following mobile screenshot, described in words. Can you generate 5 questions regarding the content of the screenshot as well as the corresponding short answers to them? \n\nThe answer should be as short as possible, containing only the necessary information. Your answer should be structured as follows:\nquestions: [\n{{question: the question,\n    answer: the answer\n}},\n ...\n]\n\n{THE SCREEN SCHEMA}\n\n\nA sample prompt for QA data generation.\n\nQuestion answering: The model is asked to answer questions regarding the content of the screenshots, e.g., “When does the restaurant open?”\n\n\nScreen navigation: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., “Click the search button.”\n\n\nScreen summarization: The model is asked to summarize the screen content in one or two sentences. \n\n\n\n\n\n\nBlock diagram of our workflow for generating data for QA, summarization and navigation tasks using existing ScreenAI models and LLMs. Each task uses a custom prompt to emphasize desired aspects, like questions related to counting, involving reasoning, etc.\n\n\n\nLLM-generated data. Examples for screen QA, navigation and summarization. For navigation, the action bounding box is displayed in red on the screenshot.\n\nExperiments and results\nChartQA, DocVQA, Multi page DocVQA, InfographicVQA, OCR VQA, Web SRC and ScreenQA. For navigation, datasets used include Referring Expressions, MoTIF, Mug, and Android in the Wild. Finally, we use Screen2Words for screen summarization and Widget Captioning for describing specific UI elements. Along with the fine-tuning datasets, we  evaluate the fine-tuned ScreenAI model using three novel benchmarks:\n\n\n\n\nScreen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities.\n\n\nScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks.\n\n\nComplex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios.\n\n\n\n\nThe fine-tuned ScreenAI model achieves state-of-the-art results on various UI and infographic-based tasks (WebSRC and MoTIF) and best-in-class performance on Chart QA, DocVQA, and InfographicVQA compared to models of similar size. ScreenAI achieves competitive performance on Screen2Words and OCR-VQA. Additionally, we report results on the new benchmark datasets introduced to serve as a baseline for further research.\n\n\n\n\nComparing model performance of ScreenAI with state-of-the-art (SOTA) models of similar size.\n\n\n\nModel performance increases with size, and the performance has not saturated even at the largest size of 5B params.\n\nConclusion\nAcknowledgements\nThis project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post.",
      "summary": "Posted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google Research Screen user interfaces (UIs) and infographics, such as charts, dia...",
      "summaryJa": "Posted by srinivas sunkara and gilles baechler, software engineers, google 研究 screen user interfaces (uis) and infographics, such as charts, dia...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "SCIN: A new resource for representative dermatology images",
      "titleJa": "Scin: a new resource for representative dermatology images",
      "link": "http://blog.research.google/2024/03/scin-new-resource-for-representative.html",
      "pubDate": "2024-03-19T15:00:00.000Z",
      "content": "Posted by Pooja Rao, Research Scientist, Google Research\n\n\n\n\nHealth datasets play a crucial role in research and medical education, but it can be challenging to create a dataset that represents the real world. For example, dermatology conditions are diverse in their appearance and severity and manifest differently across skin tones. Yet, existing dermatology image datasets often lack representation of everyday conditions (like rashes, allergies and infections) and skew towards lighter skin tones. Furthermore, race and ethnicity information is frequently missing, hindering our ability to assess disparities or create solutions.\n\n\n\n\n\nTo address these limitations, we are releasing the Skin Condition Image Network (SCIN) dataset in collaboration with physicians at Stanford Medicine. We designed SCIN to reflect the broad range of concerns that people search for online, supplementing the types of conditions typically found in clinical datasets. It contains images across various skin tones and body parts, helping to ensure that future AI tools work effectively for all. We've made the SCIN dataset freely available as an open-access resource for researchers, educators, and developers, and have taken careful steps to protect contributor privacy.   \n\n\n\n\n\n\nExample set of images and metadata from the SCIN dataset.\n\nDataset composition\ntanning propensity (self-reported Fitzpatrick Skin Type, i.e., sFST), and to describe the texture, duration and symptoms related to their concern.\n\n\nOne to three dermatologists labeled each contribution with up to five dermatology conditions, along with a confidence score for each label. The SCIN dataset contains these individual labels, as well as an aggregated and weighted differential diagnosis derived from them that could be useful for model testing or training. These labels were assigned retrospectively and are not equivalent to a clinical diagnosis, but they allow us to compare the distribution of dermatology conditions in the SCIN dataset with existing datasets.\n\n\n\n\n\n\n\nThe SCIN dataset contains largely allergic, inflammatory and infectious conditions while datasets from clinical sources focus on benign and malignant neoplasms.\n\nMonk Skin Tone (eMST) for the images. This allowed comparison of the skin condition and skin type distributions to those in existing dermatology datasets. Although we did not selectively target any skin types or skin tones, the SCIN dataset has a balanced Fitzpatrick skin type distribution (with more of Types 3, 4, 5, and 6) compared to similar datasets from clinical sources. \n\n\n\n\n\n\n\nSelf-reported and dermatologist-estimated Fitzpatrick Skin Type distribution in the SCIN dataset compared with existing un-enriched dermatology datasets (Fitzpatrick17k, PH², SKINL2, and PAD-UFES-20).\n\nFitzpatrick Skin Type scale was originally developed as a photo-typing scale to measure the response of skin types to UV radiation, and it is widely used in dermatology research. The Monk Skin Tone scale is a newer 10-shade scale that measures skin tone rather than skin phototype, capturing more nuanced differences between the darker skin tones. While neither scale was intended for retrospective estimation using images, the inclusion of these labels is intended to enable future research into skin type and tone representation in dermatology. For example, the SCIN dataset provides an initial benchmark for the distribution of these skin types and tones in the US population.\n\n\nThe SCIN dataset has a high representation of women and younger individuals, likely reflecting a combination of factors. These could include differences in skin condition incidence, propensity to seek health information online, and variations in willingness to contribute to research across demographics.\n\n\n\n\n\n    \nCrowdsourcing method\nresearch paper co-authored with investigators at Stanford Medicine. This approach empowers individuals to play an active role in healthcare research. It allows us to reach people at earlier stages of their health concerns, potentially before they seek formal care. Crucially, this method uses advertisements on web search result pages — the starting point for many people’s health journey — to connect with participants. \n\n\nOur results demonstrate that crowdsourcing can yield a high-quality dataset with a low spam rate. Over 97.5% of contributions were genuine images of skin conditions. After performing further filtering steps to exclude images that were out of scope for the SCIN dataset and to remove duplicates, we were able to release nearly 90% of the contributions received over the 8-month study period. Most images were sharp and well-exposed. Approximately half of the contributions include self-reported demographics, and 80% contain self-reported information relating to the skin condition, such as texture, duration, or other symptoms. We found that dermatologists’ ability to retrospectively assign a differential diagnosis depended more on the availability of self-reported information than on image quality.\n\n\n\n\n\n\n\nDermatologist confidence in their labels (scale from 1-5) depended on the availability of self-reported demographic and symptom information.\n\nData Use License prohibits attempts to re-identify contributors.\n\n\nWe hope the SCIN dataset will be a helpful resource for those working to advance inclusive dermatology research, education, and AI tool development. By demonstrating an alternative to traditional dataset creation methods, SCIN paves the way for more representative datasets in areas where self-reported data or retrospective labeling is feasible. \n\n\n\n\n\n    \nAcknowledgements\nWe are grateful to all our co-authors Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirisokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel (Stanford Medicine), Steven Lin (Stanford Medicine), Justin Ko (Stanford Medicine), Alan Karthikesalingam and Christopher Semturs. We also thank Yetunde Ibitoye, Sami Lachgar, Lisa Lehmann, Javier Perez, Margaret Ann Smith (Stanford Medicine), Rachelle Sico, Amit Talreja, Annisah Um’rani and Wayne Westerlind for their essential contributions to this work. Finally, we are grateful to Heather Cole-Lewis, Naama Hammel, Ivor Horn, Michael Howell, Yun Liu, and Eric Teasley for their insightful comments on the study design and manuscript.",
      "summary": "Posted by Pooja Rao, Research Scientist, Google Research Health datasets play a crucial role in research and medical education, but it can be challeng...",
      "summaryJa": "Posted by pooja rao, 研究 scientist, google 研究 health datasets play a crucial role in 研究 and 医療 education, but it can be challeng...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 80
    },
    {
      "title": "MELON: Reconstructing 3D objects from images with unknown poses",
      "titleJa": "Melon: reconstructing 3d objects from images with unknown poses",
      "link": "http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html",
      "pubDate": "2024-03-18T18:41:00.000Z",
      "content": "Posted by Mark Matthews, Senior Software Engineer, and Dmitry Lagun, Research Scientist, Google Research\n\n\n\n\n\nA person's prior experience and understanding of the world generally enables them to easily infer what an object looks like in whole, even if only looking at a few 2D pictures of it. Yet the capacity for a computer to reconstruct the shape of an object in 3D given only a few images has remained a difficult algorithmic problem for years. This fundamental computer vision task has applications ranging from the creation of e-commerce 3D models to autonomous vehicle navigation. \n\n\n\nA key part of the problem is how to determine the exact positions from which images were taken, known as pose inference. If camera poses are known, a range of successful techniques — such as neural radiance fields (NeRF) or 3D Gaussian Splatting — can reconstruct an object in 3D. But if these poses are not available, then we face a difficult “chicken and egg” problem where we could determine the poses if we knew the 3D object, but we can’t reconstruct the 3D object until we know the camera poses. The problem is made harder by pseudo-symmetries — i.e., many objects look similar when viewed from different angles. For example, square objects like a chair tend to look similar every 90° rotation. Pseudo-symmetries of an object can be revealed by rendering it on a turntable from various angles and plotting its photometric self-similarity map. \n\n\n\n\nSelf-Similarity map of a toy truck model. Left: The model is rendered on a turntable from various azimuthal angles, θ. Right: The average L2 RGB similarity of a rendering from θ with that of θ*. The pseudo-similarities are indicated by the dashed red lines.\n\nill-posed, with naïve approaches often converging to local minima. In practice, such an approach might mistake the back view as the front view of an object, because they share a similar silhouette. Previous techniques (such as BARF or SAMURAI) side-step this problem by relying on an initial pose estimate that starts close to the global minima. But how can we approach this if those aren’t available?\n\n\n\nMethods, such as GNeRF and VMRF leverage generative adversarial networks (GANs) to overcome the problem. These techniques have the ability to artificially “amplify” a limited number of training views, aiding reconstruction. GAN techniques, however, often have complex, sometimes unstable, training processes, making robust and reliable convergence difficult to achieve in practice. A range of other successful methods, such as SparsePose or RUST, can infer poses from a limited number views, but require pre-training on a large dataset of posed images, which aren’t always available, and can suffer from “domain-gap” issues when inferring poses for different types of images.\n\n\n\nIn “MELON: NeRF with Unposed Images in SO(3)”, spotlighted at 3DV 2024, we present a technique that can determine object-centric camera poses entirely from scratch while reconstructing the object in 3D. MELON (Modulo Equivalent Latent Optimization of NeRF) is one of the first techniques that can do this without initial pose camera estimates, complex training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. We demonstrate that MELON can reconstruct a NeRF from unposed images with state-of-the-art accuracy while requiring as few as 4–6 images of an object. \n\n\n\n\n    \nMELON\nconvolutional neural network (CNN) encoder that regresses camera poses from training images. We pass a downscaled training image to a four layer CNN that infers the camera pose. This CNN is initialized from noise and requires no pre-training. Its capacity is so small that it forces similar looking images to similar poses, providing an implicit regularization greatly aiding convergence.\n\n\n\nThe second technique is a modulo loss that simultaneously considers pseudo symmetries of an object. We render the object from a fixed set of viewpoints for each training image, backpropagating the loss only through the view that best fits the training image. This effectively considers the plausibility of multiple views for each image. In practice, we find N=2 views (viewing an object from the other side) is all that’s required in most cases, but sometimes get better results with N=4 for square objects.\n\n\n\nThese two techniques are integrated into standard NeRF training, except that instead of fixed camera poses, poses are inferred by the CNN and duplicated by the modulo loss. Photometric gradients back-propagate through the best-fitting cameras into the CNN. We observe that cameras generally converge quickly to globally optimal poses (see animation below). After training of the neural field, MELON can synthesize novel views using standard NeRF rendering methods.\n\n\n\nWe simplify the problem by using the NeRF-Synthetic dataset, a popular benchmark for NeRF research and common in the pose-inference literature. This synthetic dataset has cameras at precisely fixed distances and a consistent “up” orientation, requiring us to infer only the polar coordinates of the camera. This is the same as an object at the center of a globe with a camera always pointing at it, moving along the surface. We then only need the latitude and longitude (2 degrees of freedom) to specify the camera pose.\n\n\n\n\n\nMELON uses a dynamically trained lightweight CNN encoder that predicts a pose for each image. Predicted poses are replicated by the modulo loss, which only penalizes the smallest L2 distance from the ground truth color. At evaluation time, the neural field can be used to generate novel views.\n\nResults\npeak signal-to-noise ratio (PSNR) against held out test views. We see that MELON quickly converges to the approximate poses of most cameras within the first 1,000 steps of training, and achieves a competitive PSNR of 27.5 dB after 50k steps. \n\n\n\n\n\nConvergence of MELON on a toy truck model during optimization. Left: Rendering of the NeRF. Right: Polar plot of predicted (blue x), and ground truth (red dot) cameras.\n\n\n\nReconstruction quality comparison between ground-truth (GT) and MELON on NeRF-Synthetic scenes after 100k training steps.\n\nNoisy images\nnovel view synthesis from extremely noisy, unposed images. We add varying amounts, σ, of white Gaussian noise to the training images. For example, the object in σ=1.0 below is impossible to make out, yet MELON can determine the pose and generate novel views of the object. \n\n\n\n\n\n\nNovel view synthesis from noisy unposed 128×128 images. Top: Example of noise level present in training views. Bottom: Reconstructed model from noisy training views and mean angular pose error.\n\nRawNeRF have demonstrated NeRF’s excellent de-noising capabilities with known camera poses. The fact that MELON works for noisy images of unknown camera poses so robustly was unexpected. \n\n\n\n    \nConclusion\npaper and MELON site to learn more.\n\n\n\n\n    \nAcknowledgements\nWe would like to thank our paper co-authors Axel Levy, Matan Sela, and Gordon Wetzstein, as well as Florian Schroff and Hartwig Adam for continuous help in building this technology. We also thank Matthew Brown, Ricardo Martin-Brualla and Frederic Poitevin for their helpful feedback on the paper draft. We also acknowledge the use of the computational resources at the SLAC Shared Scientific Data Facility (SDF).",
      "summary": "Posted by Mark Matthews, Senior Software Engineer, and Dmitry Lagun, Research Scientist, Google Research A person's prior experience and understanding...",
      "summaryJa": "Posted by mark matthews, senior software engineer, and dmitry lagun, 研究 scientist, google 研究 a person's prior experience and understanding...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "HEAL: A framework for health equity assessment of machine learning performance",
      "titleJa": "Heal: a framework for health equity assessment of 機械学習 performance",
      "link": "http://blog.research.google/2024/03/heal-framework-for-health-equity.html",
      "pubDate": "2024-03-15T18:22:00.000Z",
      "content": "Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer & Director, Google Core\n\n\n\n\nHealth equity is a major societal concern worldwide with disparities having many causes. These sources include limitations in access to healthcare, differences in clinical treatment, and even fundamental differences in the diagnostic technology. In dermatology for example, skin cancer outcomes are worse for populations such as minorities, those with lower socioeconomic status, or individuals with limited healthcare access. While there is great promise in recent advances in machine learning (ML) and artificial intelligence (AI) to help improve healthcare, this transition from research to bedside must be accompanied by a careful understanding of whether and how they impact health equity.\n\n \n\n\nHealth equity is defined by public health organizations as fairness of opportunity for everyone to be as healthy as possible. Importantly, equity may be different from equality. For example, people with greater barriers to improving their health may require more or different effort to experience this fair opportunity. Similarly, equity is not fairness as defined in the AI for healthcare literature. Whereas AI fairness often strives for equal performance of the AI technology across different patient populations, this does not center the goal of prioritizing performance with respect to pre-existing health disparities.\n\n\n\n\nHealth equity considerations. An intervention (e.g., an ML-based tool, indicated in dark blue) promotes health equity if it helps reduce existing disparities in health outcomes (indicated in lighter blue).\n\nHealth Equity Assessment of machine Learning performance (HEAL): a framework and dermatology AI model case study”, published in The Lancet eClinicalMedicine, we propose a methodology to quantitatively assess whether ML-based health technologies perform equitably. In other words, does the ML model perform well for those with the worst health outcomes for the condition(s) the model is meant to address? This goal anchors on the principle that health equity should prioritize and measure model performance with respect to disparate health outcomes, which may be due to a number of factors that include structural inequities (e.g., demographic, social, cultural, political, economic, environmental and geographic).\n\n \n\nThe health equity framework (HEAL)\n\n\nFramework for Health Equity Assessment of machine Learning performance (HEAL). Our guiding principle is to avoid exacerbating health inequities, and these steps help us identify disparities and assess for inequitable model performance to move towards better outcomes for all.\n\n \n\nCase study on a dermatology model\nprior work. This example dermatology model was trained to classify 288 skin conditions using a development dataset of 29k cases. The input to the model consists of three photos of a skin concern along with demographic information and a brief structured medical history. The output consists of a ranked list of possible matching skin conditions. \n\n\nUsing the HEAL framework, we evaluated this model by assessing whether it prioritized performance with respect to pre-existing health outcomes. The model was designed to predict possible dermatologic conditions (from a list of hundreds) based on photos of a skin concern and patient metadata. Evaluation of the model is done using a top-3 agreement metric, which quantifies how often the top 3 output conditions match the most likely condition as suggested by a dermatologist panel. The HEAL metric is computed via the anticorrelation of this top-3 agreement with health outcome rankings. \n\n\nWe used a dataset of 5,420 teledermatology cases, enriched for diversity in age, sex and race/ethnicity, to retrospectively evaluate the model’s HEAL metric. The dataset consisted of “store-and-forward” cases from patients of 20 years or older from primary care providers in the USA and skin cancer clinics in Australia. Based on a review of the literature, we decided to explore race/ethnicity, sex and age as potential factors of inequity, and used sampling techniques to ensure that our evaluation dataset had sufficient representation of all race/ethnicity, sex and age groups. To quantify pre-existing health outcomes for each subgroup we relied on measurements from public databases endorsed by the World Health Organization, such as Years of Life Lost (YLLs) and Disability-Adjusted Life Years (DALYs; years of life lost plus years lived with disability).\n\n\n\n\nHEAL metric for all dermatologic conditions across race/ethnicity subpopulations, including health outcomes (YLLs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance.\n(* Higher is better; measures the likelihood the model performs equitably with respect to the axes in this table.)\n\n\n\nHEAL metric for all dermatologic conditions across sexes, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)\n\n\n\nHEAL metrics for all cancer and non-cancer dermatologic conditions across age groups, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)\n\n \n\nPutting things in context\nPareto condition (discussed further in the paper), which restricts model changes so that outcomes for each subpopulation are either unchanged or improved compared to the status quo, and performance does not worsen for any subpopulation.\n\n\nThe HEAL framework, in its current form, assesses the likelihood that an ML-based model prioritizes performance for subpopulations with respect to pre-existing health disparities for specific subpopulations. This differs from the goal of understanding whether ML will reduce disparities in outcomes across subpopulations in reality. Specifically, modeling improvements in outcomes requires a causal understanding of steps in the care journey that happen both before and after use of any given model. Future research is needed to address this gap.\n\n \n\nConclusion\n \n\nAcknowledgements\nThe research described here is joint work across many teams at Google. We are grateful to all our co-authors: Terry Spitz, Malcolm Pyles, Heather Cole-Lewis, Ellery Wulczyn, Stephen R. Pfohl, Donald Martin, Jr., Ronnachai Jaroensri, Geoff Keeling, Yuan Liu, Stephanie Farquhar, Qinghan Xue, Jenna Lester, Cían Hughes, Patricia Strachan, Fraser Tan, Peggy Bui, Craig H. Mermel, Lily H. Peng, Yossi Matias, Greg S. Corrado, Dale R. Webster, Sunny Virmani, Christopher Semturs, Yun Liu, and Po-Hsuan Cameron Chen. We also thank Lauren Winer, Sami Lachgar, Ting-An Lin, Aaron Loh, Morgan Du, Jenny Rizk, Renee Wong, Ashley Carrick, Preeti Singh, Annisah Um'rani, Jessica Schrouff, Alexander Brown, and Anna Iurchenko for their support of this project.",
      "summary": "Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer & Director, Google Core Health equity is ...",
      "summaryJa": "Posted by mike schaekermann, 研究 scientist, google 研究, and ivor horn, chief health equity officer & director, google core health equity is ...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Cappy: Outperforming and boosting large multi-task language models with a small scorer",
      "titleJa": "Cappy: outperforming and boosting large multi-task language models with a small scorer",
      "link": "http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html",
      "pubDate": "2024-03-14T19:38:00.000Z",
      "content": "Posted by Yun Zhu and Lijuan Liu, Software Engineers, Google Research\n\n\n\n\n\nLarge language model (LLM) advancements have led to a new paradigm that unifies various natural language processing (NLP) tasks within an instruction-following framework. This paradigm is exemplified by recent multi-task LLMs, such as T0, FLAN, and OPT-IML. First, multi-task data is gathered with each task following a task-specific template, where each labeled example is converted into an instruction (e.g., \"Put the concepts together to form a sentence: ski, mountain, skier”) paired with a corresponding response (e.g., \"Skier skis down the mountain\"). These instruction-response pairs are used to train the LLM, resulting in a conditional generation model that takes an instruction as input and generates a response. Moreover, multi-task LLMs have exhibited remarkable task-wise generalization capabilities as they can address unseen tasks by understanding and solving brand-new instructions.\n\n\n\n\n\nThe demonstration of the instruction-following pre-training of multi-task LLMs, e.g., FLAN. Pre-training tasks under this paradigm improves the performance for unseen tasks.\n\nFLAN-11B, T0-11B and OPT-IML-175B). As a result, operating such sizable models poses significant challenges because they demand considerable computational power and impose substantial requirements on the memory capacities of GPUs and TPUs, making their training and inference expensive and inefficient. Extensive storage is required to maintain a unique LLM copy for each downstream task. Moreover, the most powerful multi-task LLMs (e.g., FLAN-PaLM-540B) are closed-sourced, making them impossible to be adapted. However, in practical applications, harnessing a single multi-task LLM to manage all conceivable tasks in a zero-shot manner remains difficult, particularly when dealing with complex tasks, personalized tasks and those that cannot be succinctly defined using instructions. On the other hand, the size of downstream training data is usually insufficient to train a model well without incorporating rich prior knowledge. Hence, it is long desired to adapt LLMs with downstream supervision while bypassing storage, memory, and access issues. \n\n\n\nCertain parameter-efficient tuning strategies, including prompt tuning and adapters, substantially diminish storage requirements, but they still perform back-propagation through LLM parameters during the tuning process, thereby keeping their memory demands high. Additionally, some in-context learning techniques circumvent parameter tuning by integrating a limited number of supervised examples into the instruction. However, these techniques are constrained by the model's maximum input length, which permits only a few samples to guide task resolution.\n\n\n\nIn “Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer”, presented at NeurIPS 2023, we propose a novel approach that enhances the performance and efficiency of multi-task LLMs. We introduce a lightweight pre-trained scorer, Cappy, based on continual pre-training on top of RoBERTa with merely 360 million parameters. Cappy takes in an instruction and a candidate response as input, and produces a score between 0 and 1, indicating an estimated correctness of the response with respect to the instruction. Cappy functions either independently on classification tasks or serves as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy efficiently enables downstream supervision without requiring any finetuning, which avoids the need for back-propagation through LLM parameters and reduces memory requirements. Finally, adaptation with Cappy doesn’t require access to LLM parameters as it is compatible with closed-source multi-task LLMs, such as those only accessible via WebAPIs.\n\n\n\n\n\nCappy takes an instruction and response pair as input and outputs a score ranging from 0 to 1, indicating an estimation of the correctness of the response with respect to the instruction.\n\nPre-training\nPromptSource that were used to train T0. This collection encompasses a wide range of task types, such as question answering, sentiment analysis, and summarization. Each dataset is associated with one or more templates that convert each instance from the original datasets into an instruction paired with its ground truth response.\n\n\n\nCappy's regression modeling requires each pre-training data instance to include an instruction-response pair along with a correctness annotation for the response, so we produce a dataset with correctness annotations that range from 0 to 1. For every instance within a generation task, we leverage an existing multi-task LLM to generate multiple responses by sampling, conditioned on the given instruction. Subsequently, we assign an annotation to the pair formed by the instruction and every response, using the similarity between the response and the ground truth response of the instance. Specifically, we employ Rouge-L, a commonly-used metric for measuring overall multi-task performance that has demonstrated a strong alignment with human evaluation, to calculate this similarity as a form of weak supervision.\n\n\n\nAs a result, we obtain an effective regression dataset of 160 million instances paired with correctness score annotations. The final Cappy model is the result of continuous pre-training using the regression dataset on top of the RoBERTa model. The pre-training of Cappy is conducted on Google's TPU-v4, with RedCoast, a lightweight toolkit for automating distributed training.\n\n\n\n\n\nData augmentation with a multi-task LLM to construct a weakly supervised regression dataset for Cappy’s pre-training and fine-tuning.\n\nApplying Cappy\nAdapting multi-task LLMs with Cappy \n\n\nDownstream adaptation comparison between Cappy and approaches that rely on an LLM’s parameters, such as fine-tuning and prompt tuning. Cappy’s application enhances multi-task LLMs.\n\nResults\nPromptSource. We demonstrate that Cappy, with 360M parameters, outperforms OPT-175B and OPT-IML-30B, and matches the accuracy of  the best existing multi-task LLMs (T0-11B and OPT-IML-175B). These findings highlight Cappy’s capabilities and parameter efficiency, which can be credited to its scoring-based pre-training strategy that integrates contrastive information by differentiating between high-quality and low-quality responses. On the contrary, previous multi-task LLMs depend exclusively on teacher-forcing training that utilizes only the ground truth responses.\n\n\n\n\n\n\n\nThe overall accuracy averaged over eleven test tasks from PromptSource. “RM” refers to a pre-trained RLHF reward model. Cappy matches the best ones among existing multi-task LLMs.\n\nBIG-Bench, a set of manually curated tasks that are considered beyond the capability of many LLMs. We focus on all the 45 generation BIG-Bench tasks, specifically those that do not offer pre-established answer choices. We evaluate the performance using the Rouge-L score (representing the overall similarity between model generations and corresponding ground truths) on every test set, reporting the average score across 45 tests. In this experiment, all variants of FLAN-T5 serve as the backbone LLMs, and the foundational FLAN-T5 models are frozen. These results, shown below, suggest that Cappy enhances the performance of FLAN-T5 models by a large margin, consistently outperforming the most effective baseline achieved through sample selection using self-scoring of the LLM itself.\n\n\n\n\nThe averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.\n\nConclusion\nAcknowledgments\nThanks to Bowen Tan, Jindong Chen, Lei Meng, Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions.",
      "summary": "Posted by Yun Zhu and Lijuan Liu, Software Engineers, Google Research Large language model (LLM) advancements have led to a new paradigm that unifies ...",
      "summaryJa": "Posted by yun zhu and lijuan liu, software engineers, google 研究 大規模言語モデル (LLM) advancements have led to a new paradigm that unifies ...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "A generalist AI agent for 3D virtual environments",
      "titleJa": "A generalist ai agent for 3d virtual environments",
      "link": "https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/",
      "pubDate": "Wed, 13 Mar 2024 14:00:00 +0000",
      "content": "Introducing SIMA, a Scalable Instructable Multiworld Agent",
      "summary": "Introducing SIMA, a Scalable Instructable Multiworld Agent",
      "summaryJa": "Introducing sima, a scalable instructable multiworld agent",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "Talk like a graph: Encoding graphs for large language models",
      "titleJa": "Talk like a graph: encoding graphs for large language models",
      "link": "http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html",
      "pubDate": "2024-03-12T21:15:00.000Z",
      "content": "Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research\n\n\n\n\nImagine all the things around you — your friends, tools in your kitchen, or even the parts of your bike. They are all connected in different ways. In computer science, the term graph is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a  relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together. Even the knowledge search engines use is organized in a graph-like way.\n\n\n\n\n\nFurthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. \n\n\nSince graphs are everywhere and LLM technology is on the rise, in “Talk like a Graph: Encoding Graphs for Large Language Models”, presented at ICLR 2024, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called GraphQA to study different approaches on different graph reasoning problems and show how to phrase a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered. These findings give us clues on how to best represent graphs for LLMs. Picking the right method can make the LLM up to 60% better at graph tasks!\n\n\n\n\n\n\n\nPictured, the process of encoding a graph as text using two different approaches and feeding the text and a question about the graph to the LLM.\n\nGraphs as text\nGraphQA. Think of GraphQA as an exam designed to evaluate powerful LLMs on graph-specific problems. We want to see how well LLMs can understand and solve problems that involve graphs in different setups. To create a comprehensive and realistic exam for LLMs, we don’t just use one type of graph, we use a mix of graphs ensuring breadth in the number of connections. This is mainly because different graph types make solving such problems easier or harder. This way, GraphQA can help expose biases in how an LLM thinks about the graphs, and the whole exam gets closer to a realistic setup that LLMs might encounter in the real world.\n\n\n\n\n\n\n\nOverview of our framework for reasoning with graphs using LLMs.\n\nErdős-Rényi, scale-free networks, Barabasi-Albert model, and stochastic block model, as well as simpler graph structures like paths, complete graphs, and star graphs, providing a diverse set of data for training.\n\n\nWhen working with graphs, we also need to find ways to ask graph-related questions that LLMs can understand.  Prompting heuristics are different strategies for doing this. Let's break down the common ones:\n\n\n\nZero-shot: simply describe the task (\"Is there a cycle in this graph?\") and tell the LLM to go for it. No examples provided.\n\n\nFew-shot: This is like giving the LLM a mini practice test before the real deal. We provide a few example graph questions and their correct answers.\n\n\nChain-of-Thought: Here, we show the LLM how to break down a problem step-by-step with examples. The goal is to teach it to generate its own \"thought process\" when faced with new graphs.\n\n\nZero-CoT: Similar to CoT, but instead of training examples, we give the LLM a simple prompt, like \"Let's think step-by-step,\" to trigger its own problem-solving breakdown.\n\n\nBAG (build a graph): This is specifically for graph tasks. We add the phrase \"Let's build a graph...\" to the description, helping the LLM focus on the graph structure.\n\n\n\nWe explored different ways to translate graphs into text that LLMs can work with. Our key questions were:\n\n\n\nNode encoding: How do we represent individual nodes? Options tested include simple integers, common names (people, characters), and letters.\n\n\nEdge encoding: How do we describe the relationships between nodes? Methods involved parenthesis notation, phrases like \"are friends\", and symbolic representations like arrows.\n\n\n\nVarious node and edge encodings were combined systematically. This led to functions like the ones in the following figure:\n\n\n\n\nExamples of graph encoding functions used to encode graphs via text.\n\nAnalysis and results\nHow LLMs handle graph tasks \nLLMs struggle: On most of these basic tasks, LLMs did not do much better than a random guess. \n\n\nEncoding matters significantly: How we represent the graph as text has a great effect on LLM performance. The \"incident\" encoding excelled for most of the tasks in general.\n\n\n\nOur results are summarized in the following chart. \n\n\n\n\n\n\nComparison of various graph encoder functions based on their accuracy on different graph tasks. The main conclusion from this figure is that the graph encoding functions matter significantly.\n\nBigger is (usually) better \nPaLM 2. Here is a summary of our findings:\n\n\n\nIn general, bigger models did better on graph reasoning tasks. It seems like the extra parameters gave them space to learn more complex patterns.\n\n\nOddly, size didn't matter as much for the “edge existence” task (finding out if two nodes in a graph are connected).\n\n\nEven the biggest LLM couldn't consistently beat a simple baseline solution on the cycle check problem (finding out if a graph contains a cycle or not). This shows LLMs still have room to improve with certain graph tasks.\n\n\n\n\n\n\n\n\n\nEffect of model capacity on graph reasoning task for PaLM 2-XXS, XS, S, and L.\n\nDo different graph shapes confuse LLMs \n\n\nSamples of graphs generated with different graph generators from GraphQA. ER, BA, SBM, and SFN refers to Erdős–Rényi, Barabási–Albert, Stochastic Block Model, and Scale-Free Network respectively.\n\n\n\nComparing different graph generators on different graph tasks. The main observation here is that graph structure has a significant impact on the LLM’s performance. ER, BA, SBM, and SFN refers to Erdős–Rényi, Barabási–Albert, Stochastic Block Model, and Scale-Free Network respectively.\n\nConclusion\nHow to translate the graph to text: how we represent the graph as text significantly influences LLM performance. The incident encoding excelled for most of the tasks in general..\n\n\nTask type: Certain types of graph questions tend to be harder for LLMs, even with a good translation from graph to text.\n\n\nGraph structure: Surprisingly, the \"shape\" of the graph that on which we do inference (dense with connections, sparse, etc.) influences how well an LLM does.\n\n\n\nThis study revealed key insights about how to prepare graphs for LLMs. The right encoding techniques can significantly boost an LLM's accuracy on graph problems (ranging from around 5% to over 60% improvement). Our new benchmark, GraphQA, will help drive further research in this area.\n\n\n\n\n\n    \nAcknowledgements\nWe would like to express our gratitude to our co-author, Jonathan Halcrow, for his valuable contributions to this work. We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. We would also like to extend special thanks to Tom Small for creating the animation used in this post.",
      "summary": "Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research Imagine all the things around you — your friends, tools in your kitche...",
      "summaryJa": "Posted by bahare fatemi and bryan perozzi, 研究 scientists, google 研究 imagine all the things around you — your friends, tools in your kitche...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
      "titleJa": "Chain-of-table: evolving tables in the reasoning chain for table understanding",
      "link": "http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html",
      "pubDate": "2024-03-11T19:08:00.000Z",
      "content": "Posted by Zilong Wang, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team\n\n\n\n\nPeople use tables every day to organize and interpret complex information in a structured, easily accessible format. Due to the ubiquity of such tables, reasoning over tabular data has long been a central topic in natural language processing (NLP). Researchers in this field have aimed to leverage language models to help users answer questions, verify statements, and analyze data based on tables. However, language models are trained over large amounts of plain text, so the inherently structured nature of tabular data can be difficult for language models to fully comprehend and utilize.\n\n\n\nRecently, large language models (LLMs) have achieved outstanding performance across diverse natural language understanding (NLU) tasks by generating reliable reasoning chains, as shown in works like Chain-of-Thought and Least-to-Most. However, the most suitable way for LLMs to reason over tabular data remains an open question.\n\n\nIn “Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding”, we propose a framework to tackle table understanding tasks, where we train LLMs to outline their reasoning step by step, updating a given table iteratively to reflect each part of a thought process, akin to how people solve the table-based problems. This enables the LLM to transform the table into simpler and more manageable segments so that it can understand and analyze each part of the table in depth. This approach has yielded significant improvements and achieved new state-of-the-art results on the WikiTQ, TabFact, and FeTaQA benchmarks. The figure below shows the high-level overview of the proposed Chain-of-Table and other methods.\n\n\n\n\n\nGiven a complex table where a cyclist’s nationality and name are in the same cell, (a) generic, multi-step reasoning is unable to provide the correct answer (b) program-aided reasoning generates and executes programs (e.g., SQL queries) to deliver the answer, but falls short in accurately addressing the question. In contrast, (c) Chain-of-Table iteratively samples a chain of operations that effectively transform the complex table into a version specifically tailored to the question.\n\nChain-of-Table\nin-context learning to iteratively generate operations and to update the table to represent its reasoning chain over tabular data. This enables LLMs to dynamically plan the next operation based on the results of previous ones. This continuous evolution of the table forms a chain, which provides a more structured and clear representation of the reasoning process for a given problem and enables more accurate and reliable predictions from the LLM. \n\n\nFor example, when asked, “Which actor has the most NAACP image awards?” the Chain-of-Table framework prompts an LLM to generate tabular operations mirroring tabular reasoning processes. It first identifies the relevant columns. Then, it aggregates rows based on shared content. Finally, it reorders the aggregated results to yield a final table that clearly answers the posed question. \n\n\nThese operations transform the table to align with the question presented. To balance performance with computational expense on large tables, we construct the operation chain according to a subset of tabular rows.. Meanwhile, the step-by-step operations reveal the underlying reasoning process through the display of intermediate results from the tabular operations, fostering enhanced interpretability and understanding.\n\n\n\n\n\nIllustration of the tabular reasoning process in Chain-of-Table. This iterative process involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as a tabular thought process that can guide the LLM to land to the correct answer more reliably.\n\n  The question Q: “Which country had the most cyclists finish in the top 3?”\n\n\n  The operation history chain: f_add_col(Country) and f_select_row(1, 2, 3).\n\n\n  The latest intermediate table T: the transformed intermediate table. \n\n\n\nBy providing the triplet (T, Q, chain) in the prompt, the LLM can observe the previous tabular reasoning process and select the next operation from the operation pool to complete the reasoning chain step by step.\n\n\n\n\n\nIllustration of how Chain-of-Table selects the next operation from the operation pool and generates the arguments for the operation.(a) Chain-of-Table samples the next operation from the operation pool. (b) It takes the selected operation as input and generates its arguments.\n\nf is determined, in the second stage, we need to generate the arguments. As above, Chain-of-Table considers three components in the prompt as shown in the figure: (1) the question, (2) the selected operation and its required arguments, and (3) the latest intermediate table.  \n\n\nFor instance, when the operation f_group_by is selected, it requires a header name as its argument. \n\n\nThe LLM selects a suitable header within the table. Equipped with the selected operation and the generated arguments, Chain-of-Table executes the operation and constructs a new intermediate table for the following reasoning.\n\n\nChain-of-Table iterates the previous two stages to plan the next operation and generate the required arguments. During this process, we create an operation chain acting as a proxy for the  tabular reasoning steps. These operations generate intermediate tables presenting the results of each step to the LLM. Consequently, the output table contains comprehensive information about the intermediate phases of tabular reasoning. In our final stage, we employ this output table in formulating the final query and prompt the LLM along with the question for the final answer.\n\n \n\nExperimental setup\nPaLM 2-S and GPT 3.5 as the backbone LLMs and conduct the experiments on three public table understanding benchmarks: WikiTQ, TabFact, and FeTaQA. WikiTQ and FeTaQA are datasets for table-based question answering. TabFact is a table-based fact verification benchmark. In this blogpost, we will focus on the results on WikiTQ and TabFact. We compare Chain-of-Table with the generic reasoning methods (e.g., End-to-End QA, Few-Shot QA, and Chain-of-Thought) and the program-aided methods (e.g., Text-to-SQL, Binder, and Dater). \n\n\n    \nMore accurate answers\nPaLM 2 and GPT 3.5. This is attributed to the dynamically sampled operations and the informative intermediate tables.\n\n\n\nUnderstanding results on WikiTQ and TabFact with PaLM 2 and GPT 3.5 compared with various models.\n\nBetter robustness on harder questions\nPaLM 2 on WikiTQ. \n\n\n\nPerformance of Chain-of-Thought, Dater, and the proposed Chain-of-Table on WikiTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations significantly improve performance over generic and program-aided reasoning counterparts.\n\nChain-of-Thought, and up to 7.9% compared with Dater. Moreover, the performance of Chain-of-Table declines gracefully with increasing number of operations compared to other baseline methods, exhibiting only a minimal decrease when the number of operations increases from four to five.\n\n\n\n    \nBetter robustness with larger tables\nWikiTQ into three groups based on token number: small (<2000 tokens), medium (2000 to 4000 tokens) and large (>4000 tokens). We then compare Chain-of-Table with Dater and Binder, the two latest and strongest baselines. \n\n\n\nPerformance of Binder, Dater, and the proposed Chain-of-Table on small (<2000 tokens), medium (2000 to 4000 tokens), and large (>4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)\n\n\n\n\nPerformance of Binder, Dater, and the proposed Chain-of-Table on small (<2000 tokens), medium (2000 to 4000 tokens), and large (>4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)\n\n\nAs anticipated, the performance decreases with larger input tables, as models are required to reason through longer contexts. Nevertheless, the performance of the proposed Chain-of-Table diminishes gracefully, achieving a significant 10+% improvement over the second best competing method when dealing with large tables. This demonstrates the efficacy of the reasoning chain in handling long tabular inputs.\n\n\n\nConclusion\n \n\nAcknowledgements\nThis research was conducted by Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.",
      "summary": "Posted by Zilong Wang, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team People use tables every day to organize and interpret co...",
      "summaryJa": "Posted by zilong wang, student researcher, and chen-yu lee, 研究 scientist, クラウド ai team people use tables every day to organize and interpret co...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Health-specific embedding tools for dermatology and pathology",
      "titleJa": "Health-specific embedding tools for dermatology and pathology",
      "link": "http://blog.research.google/2024/03/health-specific-embedding-tools-for.html",
      "pubDate": "2024-03-08T19:33:00.000Z",
      "content": "Posted by Dave Steiner, Clinical Research Scientist, Google Health, and Rory Pilgrim, Product Manager, Google Research\n\n\n\n\nThere’s a worldwide shortage of access to medical imaging expert interpretation across specialties including radiology, dermatology and pathology. Machine learning (ML) technology can help ease this burden by powering tools that enable doctors to interpret these images more accurately and efficiently. However, the development and implementation of such ML tools are often limited by the availability of high-quality data, ML expertise, and computational resources. \n\n\n\nOne way to catalyze the use of ML for medical imaging is via domain-specific models that utilize deep learning (DL) to capture the information in medical images as compressed numerical vectors (called embeddings). These embeddings represent a type of pre-learned understanding of the important features in an image. Identifying patterns in the embeddings reduces the amount of data, expertise, and compute needed to train performant models as compared to working with high-dimensional data, such as images, directly. Indeed, these embeddings can be used to perform a variety of downstream tasks within the specialized domain (see animated graphic below). This framework of leveraging pre-learned understanding to solve related tasks is similar to that of a seasoned guitar player quickly learning a new song by ear. Because the guitar player has already built up a foundation of skill and understanding, they can quickly pick up the patterns and groove of a new song. \n\n\n\n\n\nPath Foundation is used to convert a small dataset of (image, label) pairs into (embedding, label) pairs. These pairs can then be used to train a task-specific classifier using a linear probe, (i.e., a lightweight linear classifier) as represented in this graphic, or other types of models using the embeddings as input.\n\n\n\nOnce the linear probe is trained, it can be used to make predictions on embeddings from new images. These predictions can be compared to ground truth information in order to evaluate the linear probe's performance.\n\nDerm Foundation and Path Foundation. This follows on the strong response we’ve already received from researchers using the CXR Foundation embedding tool for chest radiographs and represents a portion of our expanding research offerings across multiple medical-specialized modalities. These embedding tools take an image as input and produce a numerical vector (the embedding) that is specialized to the domains of dermatology and digital pathology images, respectively. By running a dataset of chest X-ray, dermatology, or pathology images through the respective embedding tool, researchers can obtain embeddings for their own images, and use these embeddings to quickly develop new models for their applications.\n\n\n\n    \nPath Foundation\nDomain-specific optimization and diverse evaluation of self-supervised models for histopathology”, we showed that self-supervised learning (SSL) models for pathology images outperform traditional pre-training approaches and enable efficient training of classifiers for downstream tasks. This effort focused on hematoxylin and eosin (H&E) stained slides, the principal tissue stain in diagnostic pathology that enables pathologists to visualize cellular features under a microscope. The performance of linear classifiers trained using the output of the SSL models matched that of prior DL models trained on orders of magnitude more labeled data. \n\n\n\nDue to substantial differences between digital pathology images and “natural image” photos, this work involved several pathology-specific optimizations during model training. One key element is that  whole-slide images (WSIs) in pathology can be 100,000 pixels across (thousands of times larger than typical smartphone photos) and are analyzed by experts at multiple magnifications (zoom levels). As such, the WSIs are typically broken down into smaller tiles or patches for computer vision and DL applications. The resulting images are information dense with cells or tissue structures distributed throughout the frame instead of having  distinct semantic objects or foreground vs. background variations, thus creating unique challenges for robust SSL and feature extraction. Additionally, physical (e.g., cutting) and chemical (e.g., fixing and staining) processes used to prepare the samples can influence image appearance dramatically. \n\n\n\nTaking these important aspects into consideration, pathology-specific SSL optimizations included helping the model learn stain-agnostic features, generalizing the model to patches from multiple magnifications, augmenting the data to mimic scanning and image post processing, and custom data balancing to improve input heterogeneity for SSL training. These approaches were extensively evaluated using a broad set of benchmark tasks involving 17 different tissue types over 12 different tasks. \n\n\n\n\nUtilizing the vision transformer (ViT-S/16) architecture, Path Foundation was selected as the best performing model from the optimization and evaluation process described above (and illustrated in the figure below). This model thus provides an important balance between performance and model size to enable valuable and scalable use in generating embeddings over the many individual image patches of large pathology WSIs.\n\n\n\n\nSSL training with pathology-specific optimizations for Path Foundation.\n\nAUROC) compared to traditional pre-training on natural images (ImageNet-21k). This includes evaluation for tasks such as metastatic breast cancer detection in lymph nodes, prostate cancer grading, and breast cancer grading, among others. \n\n\n\n\n\n\n\n\nPath Foundation embeddings significantly outperform traditional ImageNet embeddings as evaluated by linear probing across multiple evaluation tasks in histopathology.\n\nDerm Foundation\nDerm Foundation is an embedding tool derived from our research in applying DL to interpret images of dermatology conditions and includes our recent work that adds improvements to generalize better to new datasets. Due to its dermatology-specific pre-training it has a latent understanding of features present in images of skin conditions and can be used to quickly develop models to classify skin conditions. The model underlying the API is a BiT ResNet-101x3 trained in two stages. The first pre-training stage uses contrastive learning, similar to ConVIRT, to train on a large number of image-text pairs from the internet. In the second stage, the image component of this pre-trained model is then fine-tuned for condition classification using clinical datasets, such as those from teledermatology services.\n\n\n\nUnlike histopathology images, dermatology images more closely resemble the real-world images used to train many of today's computer vision models. However, for specialized dermatology tasks, creating a high-quality model may still require a large dataset. With Derm Foundation, researchers can use their own smaller dataset to retrieve domain-specific embeddings, and use those to build smaller models (e.g., linear classifiers or other small non-linear models) that enable them to validate their research or product ideas. To evaluate this approach, we trained models on a downstream task using teledermatology data. Model training involved varying dataset sizes (12.5%, 25%, 50%, 100%) to compare embedding-based linear classifiers against fine-tuning.\n\n\n\nThe modeling variants considered were:\n\n\n\n\nA linear classifier on frozen embeddings from BiT-M (a standard pre-trained image model)\n\n\nFine-tuned version of BiT-M with an extra dense layer for the downstream task\n\n\nA linear classifier on frozen embeddings from the Derm Foundation API\n\n\nFine-tuned version of the model underlying the Derm Foundation API with an extra layer for the downstream task\n\n\n\nWe found that models built on top of the Derm Foundation embeddings for dermatology-related tasks achieved significantly higher quality than those built solely on embeddings or fine tuned from BiT-M. This advantage was found to be most pronounced for smaller training dataset sizes.\n\n\n\n\n\nThese results demonstrate that the Derm Foundation tooI can serve as a useful starting point to accelerate skin-related modeling tasks. We aim to enable other researchers to build on the underlying features and representations of dermatology that the model has learned. \n\nAccess Path and Derm Foundation\nDerm Foundation Access Form\n\n\nPath Foundation Access Form\n\n\n\n\nAfter gaining access to each tool, you can use the API to retrieve embeddings from dermatology images or digital pathology images stored in Google Cloud. Approved users who are just curious to see the model and embeddings in action can use the provided example Colab notebooks to train models using public data for classifying six common skin conditions or identifying tumors in histopathology patches. We look forward to seeing the range of use-cases these tools can unlock.\n\n\n\n    \nAcknowledgements\nWe would like to thank the many collaborators who helped make this work possible including Yun Liu, Can Kirmizi, Fereshteh Mahvar, Bram Sterling, Arman Tajback, Kenneth Philbrik, Arnav Agharwal, Aurora Cheung, Andrew Sellergren, Boris Babenko, Basil Mustafa, Jan Freyberg, Terry Spitz, Yuan Liu, Pinal Bavishi, Ayush Jain, Amit Talreja, Rajeev Rikhye, Abbi Ward, Jeremy Lai, Faruk Ahmed, Supriya Vijay,Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Ellery Wulczyn, Jonathan Krause, Fayaz Jamil, Tom Small, Annisah Um'rani, Lauren Winer, Sami Lachgar, Yossi Matias, Greg Corrado, and Dale Webster.",
      "summary": "Posted by Dave Steiner, Clinical Research Scientist, Google Health, and Rory Pilgrim, Product Manager, Google Research There’s a worldwide shortage of...",
      "summaryJa": "Posted by dave steiner, clinical 研究 scientist, google health, and rory pilgrim, product manager, google 研究 there’s a worldwide shortage of...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Social learning: Collaborative learning with large language models",
      "titleJa": "Social learning: collaborative learning with large language models",
      "link": "http://blog.research.google/2024/03/social-learning-collaborative-learning.html",
      "pubDate": "2024-03-07T18:15:00.000Z",
      "content": "Posted by Amirkeivan Mohtashami, Research Intern, and Florian Hartmann, Software Engineer, Google Research\n\n\n\n\nLarge language models (LLMs) have significantly improved the state of the art for solving tasks specified using natural language, often reaching performance close to that of people. As these models increasingly enable assistive agents, it could be beneficial for them to learn effectively from each other, much like people do in social settings, which would allow LLM-based agents to improve each other’s performance. \n\n \n\n\nTo discuss the learning processes of humans, Bandura and Walters described the concept of social learning in 1977, outlining different models of observational learning used by people. One common method of learning from others is through a verbal instruction (e.g., from a teacher) that describes how to engage in a particular behavior. Alternatively, learning can happen through a live model by mimicking a live example of the behavior.\n\n\nGiven the success of LLMs mimicking human communication, in our paper “Social Learning: Towards Collaborative Learning with Large Language Models”, we investigate whether LLMs are able to learn from each other using social learning. To this end, we outline a framework for social learning in which LLMs share knowledge with each other in a privacy-aware manner using natural language. We evaluate the effectiveness of our framework on various datasets, and propose quantitative methods that measure privacy in this setting. In contrast to previous approaches to collaborative learning, such as common federated learning approaches that often rely on gradients, in our framework, agents teach each other purely using natural language.\n\n \n\nSocial learning for LLMs\nspam detection in short text messages (SMS), solving grade school math problems, and answering questions based on a given text.   \n\n\n\n\nA visualization of the social learning process: A teacher model provides instructions or few-shot examples to a student model without sharing its private data.\n\nfew-shot learning. With this in mind, we provide human-labeled examples of a task that enables the teacher model to teach it to a student. One of the main use cases of social learning arises when these examples cannot be directly shared with the student due, for example, to privacy concerns. \n\n\nTo illustrate this, let’s look at a hypothetical example for a spam detection task. A teacher model is located on device where some users volunteer to mark incoming messages they receive as either “spam” or “not spam”. This is useful data that could help train a student model to differentiate between spam and not spam, but sharing personal messages with other users is a breach of privacy and should be avoided. To prevent this, a social learning process can transfer the knowledge from the teacher model to the student so it learns what spam messages look like without needing to share the user’s personal text messages.\n\n\nWe investigate the effectiveness of this social learning approach by analogy with the established human social learning theory that we discussed above. In these experiments, we use PaLM 2-S models for both the teacher and the student.\n\n\n\n\nA systems view of social learning: At training time, multiple teachers teach the student. At inference time, the student is using what it learned from the teachers.\n\nSynthetic examples\n\n\nThe 8 generated examples perform as well as the original data for several tasks (see our paper).\n\n\n\nGenerating 16 instead of just 8 examples further reduces the performance gap relative to the original examples.\n\npaper, we additionally look into aggregation methods for selecting good subsets of examples to use.\n\n\n    \nSynthetic instruction\nLambada, GSM8k, and Random Insertion, providing synthetic examples performs better than providing generated instructions, whereas in the other tasks generated instruction obtains a higher accuracy. This observation suggests that the choice of the teaching model depends on the task at hand, similar to how the most effective method for teaching people varies by task.\n\n\n\n\n\nDepending on the task, generating instructions can work better than generating new examples.\n\nMemorization of the private examples\nSecret Sharer, a popular method for quantifying to what extent a model memorizes its training data, and adapted it to the social learning setting. We picked this method since it had previously been used for evaluating memorization in federated learning.\n\n\nTo apply the Secret Sharer method to social learning, we design “canary” data points such that we can concretely measure how much the training process memorized them. These data points are included in the datasets used by teachers to generate new examples. After the social learning process completes, we can then measure how much more confident the student is in the secret data points the teacher used, compared to similar ones that were not shared even with the teachers.\n\n\nIn our analysis, discussed in detail in the paper, we use canary examples that include names and codes. Our results show that the student is only slightly more confident in the canaries the teacher used. In contrast, when the original data points are directly shared with the student, the confidence in the included canaries is much higher than in the held-out set. This supports the conclusion that the teacher does indeed use its data to teach without simply copying it over.\n\n \n\nConclusion and next steps\n \n\nAcknowledgements\nWe would like to acknowledge and thank Matt Sharifi, Sian Gooding, Lukas Zilka, and Blaise Aguera y Arcas, who are all co-authors on the paper. Furthermore, we would like to thank Victor Cărbune, Zachary Garrett, Tautvydas Misiunas, Sofia Neata and John Platt for their feedback, which greatly improved the paper. We’d also like to thank Tom Small for creating the animated figure.",
      "summary": "Posted by Amirkeivan Mohtashami, Research Intern, and Florian Hartmann, Software Engineer, Google Research Large language models (LLMs) have significa...",
      "summaryJa": "Posted by amirkeivan mohtashami, 研究 intern, and florian hartmann, software engineer, google 研究 large language models (llms) have significa...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Croissant: a metadata format for ML-ready datasets",
      "titleJa": "Croissant: a metadata format for ml-ready datasets",
      "link": "http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html",
      "pubDate": "2024-03-06T18:26:00.000Z",
      "content": "Posted by Omar Benjelloun, Software Engineer, Google Research, and Peter Mattson, Software Engineer, Google Core ML and President, MLCommons Association\n\n\n\n\n\n\nMachine learning (ML) practitioners looking to reuse existing datasets to train an ML model often spend a lot of time understanding the data, making sense of its organization, or figuring out what subset to use as features. So much time, in fact, that progress in the field of ML is hampered by a fundamental obstacle: the wide variety of data representations. \n\n\n\n\n\nML datasets cover a broad range of content types, from text and structured data to images, audio, and video. Even within datasets that cover the same types of content, every dataset has a unique ad hoc arrangement of files and data formats. This challenge reduces productivity throughout the entire ML development process, from finding the data to training the model. It also impedes development of badly needed tooling for working with datasets. \n\n\nThere are general purpose metadata formats for datasets such as schema.org and DCAT. However, these formats were designed for data discovery rather than for the specific needs of ML data, such as the ability to extract and combine data from structured and unstructured sources, to include metadata that would enable responsible use of the data, or to describe ML usage characteristics such as defining training, test and validation sets. \n\n\nToday, we're introducing Croissant, a new metadata format for ML-ready datasets. Croissant was developed collaboratively by a community from industry and academia, as part of the MLCommons effort. The Croissant format doesn't change how the actual data is represented (e.g., image or text file formats) — it provides a standard way to describe and organize it. Croissant builds upon schema.org, the de facto standard for publishing structured data on the Web, which is already used by over 40M datasets. Croissant augments it with comprehensive layers for ML relevant metadata, data resources, data organization, and default ML semantics.\n\n\nIn addition, we are announcing support from major tools and repositories: Today, three widely used collections of ML datasets — Kaggle, Hugging Face, and OpenML — will begin supporting the Croissant format for the datasets they host; the Dataset Search tool lets users search for Croissant datasets across the Web; and popular ML frameworks, including TensorFlow, PyTorch, and JAX, can load Croissant datasets easily using the TensorFlow Datasets (TFDS) package.\n\n\n\n\n    \nCroissant\nspecification of the format, a set of example datasets, an open source Python library to validate, consume and generate Croissant metadata, and an open source visual editor to load, inspect and create Croissant dataset descriptions in an intuitive way.\n\n\nSupporting Responsible AI (RAI) was a key goal of the Croissant effort from the start. We are also releasing the first version of the Croissant RAI vocabulary extension, which augments Croissant with key properties needed to describe important RAI use cases such as data life cycle management, data labeling, participatory data, ML safety and fairness evaluation, explainability, and compliance.\n\n\n\n    \nWhy a shared format for ML data?\nWhat can Croissant do today?\n\n\nThe Croissant ecosystem: Users can Search for Croissant datasets, download them from major repositories, and easily load them into their favorite ML frameworks. They can create, inspect and modify Croissant metadata using the Croissant editor.\n\nGoogle Dataset Search, which offers a Croissant filter.\n\n\nHuggingFace\n\n\nKaggle\n\n\nOpenML\n\n\n\nWith a Croissant dataset, it is possible to:\n\n\n\nIngest data easily via TensorFlow Datasets for use in popular ML frameworks like TensorFlow, PyTorch, and JAX.\n\n\nInspect and modify the metadata using the Croissant editor UI (github).\n\n\n\nTo publish a Croissant dataset, users can:\n\n\n\nUse the Croissant editor UI (github) to generate a large portion of Croissant metadata automatically by analyzing the data the user provides, and to fill important metadata fields such as RAI properties.\n\n\nPublish the Croissant information as part of their dataset Web page to make it discoverable and reusable.\n\n\nPublish their data in one of the repositories that support Croissant, such as Kaggle, HuggingFace and OpenML, and automatically generate Croissant metadata.\n\n\n\n\n\n\n    \nFuture direction\njoin us in contributing to the effort.\n\n\n\n\n    \nAcknowledgements\nCroissant was developed by the Dataset Search, Kaggle and TensorFlow Datasets teams from Google, as part of an MLCommons community working group, which also includes contributors from these organizations: Bayer, cTuning Foundation, DANS-KNAW, Dotphoton, Harvard, Hugging Face, Kings College London, LIST, Meta, NASA, North Carolina State University, Open Data Institute, Open University of Catalonia, Sage Bionetworks, and TU Eindhoven.",
      "summary": "Posted by Omar Benjelloun, Software Engineer, Google Research, and Peter Mattson, Software Engineer, Google Core ML and President, MLCommons Associati...",
      "summaryJa": "Posted by omar benjelloun, software engineer, google 研究, and peter mattson, software engineer, google core ml and president, mlcommons associati...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Google at APS 2024",
      "titleJa": "Google at aps 2024",
      "link": "http://blog.research.google/2024/03/google-at-aps-2024.html",
      "pubDate": "2024-03-04T15:06:00.000Z",
      "content": "Posted by Kate Weber and Shannon Leon, Google Research, Quantum AI Team\n\n\n\n\nToday the 2024 March Meeting of the American Physical Society (APS) kicks off in Minneapolis, MN. A premier conference on topics ranging across physics and related fields, APS 2024 brings together researchers, students, and industry professionals to share their discoveries and build partnerships with the goal of realizing fundamental advances in physics-related sciences and technology. \n\n\n\nThis year, Google has a strong presence at APS with a booth hosted by the Google Quantum AI team, 50+ talks throughout the conference, and participation in conference organizing activities, special sessions and events. Attending APS 2024 in person? Come visit Google’s Quantum AI booth to learn more about the exciting work we’re doing to solve some of the field’s most interesting challenges. @GoogleAI X (Twitter) account to find out about Google booth activities (e.g., demos and Q&A sessions).-->\n\n\nYou can learn more about the latest cutting edge work we are presenting at the conference along with our schedule of booth events below (Googlers listed in bold).\n\n\n    \nOrganizing Committee\nAaron Szasz\n\n\n\n    \nBooth Activities\nThis schedule is subject to change. Please visit the Google Quantum AI booth for more information.\n\n\n\n    Crumble: A prototype interactive tool for visualizing QEC circuits\n\n  Presenter: Matt McEwen\n\n    Tue, Mar 5 | 11:00 AM CST\n\n\n\n    Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms\n\n\n    Presenter: Tanuj Khattar\n\n\n    Tue, Mar 5 | 2:30 PM CST\n\n\n\n    Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms\n\n\n    Presenter: Tanuj Khattar\n\n\n    Thu, Mar 7 | 11:00 AM CST\n\n\n\n    $5M XPRIZE / Google Quantum AI competition to accelerate quantum applications Q&A \n\n    Presenter: Ryan Babbush\n\n    Thu, Mar 7 | 11:00 AM CST\n\n\n\n\n    \nTalks\nMonday\nCertifying highly-entangled states from few single-qubit measurements\n\n    Presenter: Hsin-Yuan Huang\n\n    Author: Hsin-Yuan Huang\n\n    Session A45: New Frontiers in Machine Learning Quantum Physics\n\n\n\n    Toward high-fidelity analog quantum simulation with superconducting qubits\n\n    Presenter: Trond Andersen\n\n    Authors: Trond I Andersen, Xiao Mi, Amir H Karamlou, Nikita Astrakhantsev, Andrey Klots, Julia Berndtsson, Andre Petukhov, Dmitry Abanin, Lev B Ioffe, Yu Chen, Vadim Smelyanskiy, Pedram Roushan\n\n    Session A51: Applications on Noisy Quantum Hardware I\n\n\n\n    Measuring circuit errors in context for surface code circuits\n\n    Presenter: Dripto M Debroy\n\n    Authors: Dripto M Debroy, Jonathan A Gross, Élie Genois, Zhang Jiang\n\n    Session B50: Characterizing Noise with QCVV Techniques\n\n\n\n    Quantum computation of stopping power for inertial fusion target design I: Physics overview and the limits of classical algorithms\n\n    Presenter: Andrew D. Baczewski\n\n    Authors: Nicholas C. Rubin, Dominic W. Berry, Alina Kononov, Fionn D. Malone, Tanuj Khattar, Alec White, Joonho Lee, Hartmut Neven, Ryan Babbush, Andrew D. Baczewski\n\n    Session B51: Heterogeneous Design for Quantum Applications\n\n    Link to Paper\n\n\n\n    Quantum computation of stopping power for inertial fusion target design II: Physics overview and the limits of classical algorithms\n\n    Presenter: Nicholas C. Rubin\n\n    Authors: Nicholas C. Rubin, Dominic W. Berry, Alina Kononov, Fionn D. Malone, Tanuj Khattar, Alec White, Joonho Lee, Hartmut Neven, Ryan Babbush, Andrew D. Baczewski\n\n    Session B51: Heterogeneous Design for Quantum Applications\n\n    Link to Paper\n\n\n\n    Calibrating Superconducting Qubits: From NISQ to Fault Tolerance\n\n    Presenter: Sabrina S Hong\n\n    Author: Sabrina S Hong\n  \nSession B56: From NISQ to Fault Tolerance\n\n\n\n    Measurement and feedforward induced entanglement negativity transition\n\n    Presenter: Ramis Movassagh\n\n    Authors: Alireza Seif, Yu-Xin Wang, Ramis Movassagh, Aashish A. Clerk\n\n    Session B31: Measurement Induced Criticality in Many-Body Systems\n\n    Link to Paper\n\n\n\n    Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments\n\n    Presenter: Salvatore Mandra\n\n    Authors: Kostyantyn Kechedzhi, Sergei V Isakov, Salvatore Mandra, Benjamin Villalonga, X. Mi, Sergio Boixo, Vadim Smelyanskiy\n\n    Session B52: Quantum Algorithms and Complexity\n\n    Link to Paper\n\n\n\n    Accurate thermodynamic tables for solids using Machine Learning Interaction Potentials and Covariance of Atomic Positions\n\n    Presenter: Mgcini K Phuthi\n\n    Authors: Mgcini K Phuthi, Yang Huang, Michael Widom, Ekin D Cubuk, Venkat Viswanathan\n\n    Session D60: Machine Learning of Molecules and Materials: Chemical Space and Dynamics\n\n\n\n\n    \nTuesday\nIN-Situ Pulse Envelope Characterization Technique (INSPECT)\n\n    Presenter: Zhang Jiang\n\n    Authors: Zhang Jiang, Jonathan A Gross, Élie Genois\n\n    Session F50: Advanced Randomized Benchmarking and Gate Calibration\n\n\n\n    Characterizing two-qubit gates with dynamical decoupling\n\n    Presenter: Jonathan A Gross\n\n    Authors: Jonathan A Gross, Zhang Jiang, Élie Genois, Dripto M Debroy, Ze-Pei Cian*, Wojciech Mruczkiewicz\n\n    Session F50: Advanced Randomized Benchmarking and Gate Calibration\n\n\n\n    Statistical physics of regression with quadratic models\n\n    Presenter: Blake Bordelon\n\n    Authors: Blake Bordelon, Cengiz Pehlevan, Yasaman Bahri\n\n    Session EE01: V: Statistical and Nonlinear Physics II\n\n\n\n    Improved state preparation for first-quantized simulation of electronic structure\n \n  Presenter: William J Huggins\n \n  Authors: William J Huggins, Oskar Leimkuhler, Torin F Stetina, Birgitta Whaley\n \n  Session G51: Hamiltonian Simulation\n\n\n\n    Controlling large superconducting quantum processors\n\n    Presenter: Paul V. Klimov\n\n    Authors: Paul V. Klimov, Andreas Bengtsson, Chris Quintana, Alexandre Bourassa, Sabrina Hong, Andrew Dunsworth, Kevin J. Satzinger, William P. Livingston, Volodymyr Sivak, Murphy Y. Niu, Trond I. Andersen, Yaxing Zhang, Desmond Chik, Zijun Chen, Charles Neill, Catherine Erickson, Alejandro Grajales Dau, Anthony Megrant, Pedram Roushan, Alexander N. Korotkov, Julian Kelly, Vadim Smelyanskiy, Yu Chen, Hartmut Neven\n\n    Session G30: Commercial Applications of Quantum Computing\nLink to Paper\n\n\n\n    Gaussian boson sampling: Determining quantum advantage\n\n    Presenter: Peter D Drummond\n\n    Authors: Peter D Drummond, Alex Dellios, Ned Goodman, Margaret D Reid, Ben Villalonga\n\n    Session G50: Quantum Characterization, Verification, and Validation II\n\n\n\n    Attention to complexity III: learning the complexity of random quantum circuit states\n\n    Presenter: Hyejin Kim\n\n    Authors: Hyejin Kim, Yiqing Zhou, Yichen Xu, Chao Wan, Jin Zhou, Yuri D Lensky, Jesse Hoke, Pedram Roushan, Kilian Q Weinberger, Eun-Ah Kim\n\n    Session G50: Quantum Characterization, Verification, and Validation II\n\n\n\n    Balanced coupling in superconducting circuits\n\n    Presenter: Daniel T Sank\n\n    Authors: Daniel T Sank, Sergei V Isakov, Mostafa Khezri, Juan Atalaya\n\n    Session K48: Strongly Driven Superconducting Systems\n\n\n\n    Resource estimation of Fault Tolerant algorithms using Qᴜᴀʟᴛʀᴀɴ\n\n    Presenter: Tanuj Khattar\n\n    Author: Tanuj Khattar, Matthew Harrigan, Fionn D. Malone, Nour Yosri, Nicholas C. Rubin\nSession K49: Algorithms and Implementations on Near-Term Quantum Computers\n\n\n\n\n    \nWednesday\nDiscovering novel quantum dynamics with superconducting qubits\n\n    Presenter: Pedram Roushan\n\n    Author: Pedram Roushan\n\n    Session M24: Analog Quantum Simulations Across Platforms\n\n\n\n    Deciphering Tumor Heterogeneity in Triple-Negative Breast Cancer: The Crucial Role of Dynamic Cell-Cell and Cell-Matrix Interactions\n\n    Presenter: Susan Leggett\n\n    Authors: Susan Leggett, Ian Wong, Celeste Nelson, Molly Brennan, Mohak Patel, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams \n\n    Session M27: Mechanics of Cells and Tissues II\n\n\n\n    Toward implementation of protected charge-parity qubits\n\n    Presenter: Abigail Shearrow\n\n    Authors: Abigail Shearrow, Matthew Snyder, Bradley G Cole, Kenneth R Dodge, Yebin Liu, Andrey Klots, Lev B Ioffe, Britton L Plourde, Robert McDermott\n\n    Session N48: Unconventional Superconducting Qubits\n\n\n\n    Electronic capacitance in tunnel junctions for protected charge-parity qubits\n\n    Presenter: Bradley G Cole\n\n    Authors: Bradley G Cole, Kenneth R Dodge, Yebin Liu, Abigail Shearrow, Matthew Snyder, Andrey Klots, Lev B Ioffe, Robert McDermott, B.L.T. Plourde\n\n    Session N48: Unconventional Superconducting Qubits\n\n\n\n    Overcoming leakage in quantum error correction\n\n    Presenter: Kevin C. Miao\n\n    Authors: Kevin C. Miao, Matt McEwen, Juan Atalaya, Dvir Kafri, Leonid P. Pryadko, Andreas Bengtsson, Alex Opremcak, Kevin J. Satzinger, Zijun Chen, Paul V. Klimov, Chris Quintana, Rajeev Acharya, Kyle Anderson, Markus Ansmann, Frank Arute, Kunal Arya, Abraham Asfaw, Joseph C. Bardin, Alexandre Bourassa, Jenna Bovaird, Leon Brill, Bob B. Buckley, David A. Buell, Tim Burger, Brian Burkett, Nicholas Bushnell, Juan Campero, Ben Chiaro, Roberto Collins, Paul Conner, Alexander L. Crook, Ben Curtin, Dripto M. Debroy, Sean Demura, Andrew Dunsworth, Catherine Erickson, Reza Fatemi, Vinicius S. Ferreira, Leslie Flores Burgos, Ebrahim Forati, Austin G. Fowler, Brooks Foxen, Gonzalo Garcia, William Giang, Craig Gidney, Marissa Giustina, Raja Gosula, Alejandro Grajales Dau, Jonathan A. Gross, Michael C. Hamilton, Sean D. Harrington, Paula Heu, Jeremy Hilton, Markus R. Hoffmann, Sabrina Hong, Trent Huang, Ashley Huff, Justin Iveland, Evan Jeffrey, Zhang Jiang, Cody Jones, Julian Kelly, Seon Kim, Fedor Kostritsa, John Mark Kreikebaum, David Landhuis, Pavel Laptev, Lily Laws, Kenny Lee, Brian J. Lester, Alexander T. Lill, Wayne Liu, Aditya Locharla, Erik Lucero, Steven Martin, Anthony Megrant, Xiao Mi, Shirin Montazeri, Alexis Morvan, Ofer Naaman, Matthew Neeley, Charles Neill, Ani Nersisyan, Michael Newman, Jiun How Ng, Anthony Nguyen, Murray Nguyen, Rebecca Potter, Charles Rocque, Pedram Roushan, Kannan Sankaragomathi, Christopher Schuster, Michael J. Shearn, Aaron Shorter, Noah Shutty, Vladimir Shvarts, Jindra Skruzny, W. Clarke Smith, George Sterling, Marco Szalay, Douglas Thor, Alfredo Torres, Theodore White, Bryan W. K. Woo, Z. Jamie Yao, Ping Yeh, Juhwan Yoo, Grayson Young, Adam Zalcman, Ningfeng Zhu, Nicholas Zobrist, Hartmut Neven, Vadim Smelyanskiy, Andre Petukhov, Alexander N. Korotkov, Daniel Sank, Yu Chen\n\n    Session N51: Quantum Error Correction Code Performance and Implementation I\n\n    Link to Paper\n\n\n\n    Modeling the performance of the surface code with non-uniform error distribution: Part 1\n\n    Presenter: Yuri D Lensky\n\n    Authors: Yuri D Lensky, Volodymyr Sivak, Kostyantyn Kechedzhi, Igor Aleiner\n\n    Session N51: Quantum Error Correction Code Performance and Implementation I\n\n\n\n    Modeling the performance of the surface code with non-uniform error distribution: Part 2\n\n    Presenter: Volodymyr Sivak\n\n    Authors: Volodymyr Sivak, Michael Newman, Cody Jones, Henry Schurkus, Dvir Kafri, Yuri D Lensky, Paul Klimov, Kostyantyn Kechedzhi, Vadim Smelyanskiy\n\n    Session N51: Quantum Error Correction Code Performance and Implementation I\n\n\n\n    Highly optimized tensor network contractions for the simulation of classically challenging quantum computations\n\n    Presenter: Benjamin Villalonga\n\n    Author: Benjamin Villalonga\n\n    Session Q51: Co-evolution of Quantum Classical Algorithms\n\n\n\n    Teaching modern quantum computing concepts using hands-on open-source software at all levels\n\n    Presenter: Abraham Asfaw\n\n    Author: Abraham Asfaw\n\n    Session Q61: Teaching Quantum Information at All Levels II\n\n\n\n\n    \nThursday\nNew circuits and an open source decoder for the color code\n\n    Presenter: Craig Gidney\n\n    Authors: Craig Gidney, Cody Jones\n\n    Session S51: Quantum Error Correction Code Performance and Implementation II\n\n    Link to Paper\n\n\n\n    Performing Hartree-Fock many-body physics calculations with large language models\n\n    Presenter: Eun-Ah Kim\n\n    Authors: Eun-Ah Kim, Haining Pan, Nayantara Mudur, William Taranto, Subhashini Venugopalan, Yasaman Bahri, Michael P Brenner\n\n    Session S18: Data Science, AI and Machine Learning in Physics I\n\n\n\n    New methods for reducing resource overhead in the surface code\n\n    Presenter: Michael Newman\n\n    Authors: Craig M Gidney, Michael Newman, Peter Brooks, Cody Jones\n\n    Session S51: Quantum Error Correction Code Performance and Implementation II\n\n    Link to Paper\n\n\n\n    Challenges and opportunities for applying quantum computers to drug design\n\n    Presenter: Raffaele Santagati\n\n    Authors: Raffaele Santagati, Alan Aspuru-Guzik, Ryan Babbush, Matthias Degroote, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, Nicholas C. Rubin, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig\n\n    Session S49: Advances in Quantum Algorithms for Near-Term Applications\n\n    Link to Paper\n\n\n\n    Dispatches from Google's hunt for super-quadratic quantum advantage in new applications\n\n    Presenter: Ryan Babbush\n\n    Author: Ryan Babbush\n\n    Session T45: Recent Advances in Quantum Algorithms\n\n\n\n    Qubit as a reflectometer\n\n    Presenter: Yaxing Zhang\n\n    Authors: Yaxing Zhang, Benjamin Chiaro\n\n    Session T48: Superconducting Fabrication, Packaging, & Validation\n\n\n\n    Random-matrix theory of measurement-induced phase transitions in nonlocal Floquet quantum circuits\n\n    Presenter: Aleksei Khindanov\n\n    Authors: Aleksei Khindanov, Lara Faoro, Lev Ioffe, Igor Aleiner\n\n    Session W14: Measurement-Induced Phase Transitions\n\n\n\n    Continuum limit of finite density many-body ground states with MERA\n\n    Presenter: Subhayan Sahu\n\n    Authors: Subhayan Sahu, Guifré Vidal\n\n    Session W58: Extreme-Scale Computational Science Discovery in Fluid Dynamics and Related Disciplines II\n\n\n\n    Dynamics of magnetization at infinite temperature in a Heisenberg spin chain\n\n    Presenter: Eliott Rosenberg\n\n    Authors: Eliott Rosenberg, Trond Andersen, Rhine Samajdar, Andre Petukhov, Jesse Hoke*, Dmitry Abanin, Andreas Bengtsson, Ilya Drozdov, Catherine Erickson, Paul Klimov, Xiao Mi, Alexis Morvan, Matthew Neeley, Charles Neill, Rajeev Acharya, Richard Allen, Kyle Anderson, Markus Ansmann, Frank Arute, Kunal Arya, Abraham Asfaw, Juan Atalaya, Joseph Bardin, A. Bilmes, Gina Bortoli, Alexandre Bourassa, Jenna Bovaird, Leon Brill, Michael Broughton, Bob B. Buckley, David Buell, Tim Burger, Brian Burkett, Nicholas Bushnell, Juan Campero, Hung-Shen Chang, Zijun Chen, Benjamin Chiaro, Desmond Chik, Josh Cogan, Roberto Collins, Paul Conner, William Courtney, Alexander Crook, Ben Curtin, Dripto Debroy, Alexander Del Toro Barba, Sean Demura, Agustin Di Paolo, Andrew Dunsworth, Clint Earle, E. Farhi, Reza Fatemi, Vinicius Ferreira, Leslie Flores, Ebrahim Forati, Austin Fowler, Brooks Foxen, Gonzalo Garcia, Élie Genois, William Giang, Craig Gidney, Dar Gilboa, Marissa Giustina, Raja Gosula, Alejandro Grajales Dau, Jonathan Gross, Steve Habegger, Michael Hamilton, Monica Hansen, Matthew Harrigan, Sean Harrington, Paula Heu, Gordon Hill, Markus Hoffmann, Sabrina Hong, Trent Huang, Ashley Huff, William Huggins, Lev Ioffe, Sergei Isakov, Justin Iveland, Evan Jeffrey, Zhang Jiang, Cody Jones, Pavol Juhas, D. Kafri, Tanuj Khattar, Mostafa Khezri, Mária Kieferová, Seon Kim, Alexei Kitaev, Andrey Klots, Alexander Korotkov, Fedor Kostritsa, John Mark Kreikebaum, David Landhuis, Pavel Laptev, Kim Ming Lau, Lily Laws, Joonho Lee, Kenneth Lee, Yuri Lensky, Brian Lester, Alexander Lill, Wayne Liu, William P. Livingston, A. Locharla, Salvatore Mandrà, Orion Martin, Steven Martin, Jarrod McClean, Matthew McEwen, Seneca Meeks, Kevin Miao, Amanda Mieszala, Shirin Montazeri, Ramis Movassagh, Wojciech Mruczkiewicz, Ani Nersisyan, Michael Newman, Jiun How Ng, Anthony Nguyen, Murray Nguyen, M. Niu, Thomas O'Brien, Seun Omonije, Alex Opremcak, Rebecca Potter, Leonid Pryadko, Chris Quintana, David Rhodes, Charles Rocque, N. Rubin, Negar Saei, Daniel Sank, Kannan Sankaragomathi, Kevin Satzinger, Henry Schurkus, Christopher Schuster, Michael Shearn, Aaron Shorter, Noah Shutty, Vladimir Shvarts, Volodymyr Sivak, Jindra Skruzny, Clarke Smith, Rolando Somma, George Sterling, Doug Strain, Marco Szalay, Douglas Thor, Alfredo Torres, Guifre Vidal, Benjamin Villalonga, Catherine Vollgraff Heidweiller, Theodore White, Bryan Woo, Cheng Xing, Jamie Yao, Ping Yeh, Juhwan Yoo, Grayson Young, Adam Zalcman, Yaxing Zhang, Ningfeng Zhu, Nicholas Zobrist, Hartmut Neven, Ryan Babbush, Dave Bacon, Sergio Boixo, Jeremy Hilton, Erik Lucero, Anthony Megrant, Julian Kelly, Yu Chen, Vadim Smelyanskiy, Vedika Khemani, Sarang Gopalakrishnan, Tomaž Prosen, Pedram Roushan\n\n    Session W50: Quantum Simulation of Many-Body Physics\n\n    Link to Paper\n\n\n\n    The fast multipole method on a quantum computer\n\n    Presenter: Kianna Wan\n\n    Authors: Kianna Wan, Dominic W Berry, Ryan Babbush\n\n    Session W50: Quantum Simulation of Many-Body Physics\n\n\n\n\n    \nFriday\nThe quantum computing industry and protecting national security: what tools will work?\n\n    Presenter: Kate Weber\n\n    Author: Kate Weber\n  \nSession Y43: Industry, Innovation, and National Security: Finding the Right Balance\n\n\n\n    Novel charging effects in the fluxonium qubit\n\n    Presenter: Agustin Di Paolo\n\n    Authors: Agustin Di Paolo, Kyle Serniak, Andrew J Kerman, William D Oliver\n\n    Session Y46: Fluxonium-Based Superconducting Quibits\n\n\n\n    Microwave Engineering of Parametric Interactions in Superconducting Circuits\n\n    Presenter: Ofer Naaman\n\n    Author: Ofer Naaman\n\n    Session Z46: Broadband Parametric Amplifiers and Circulators\n\n\n\n    Linear spin wave theory of large magnetic unit cells using the Kernel Polynomial Method\n\n    Presenter: Harry Lane\n\n    Authors: Harry Lane, Hao Zhang, David A Dahlbom, Sam Quinn, Rolando D Somma, Martin P Mourigal, Cristian D Batista, Kipton Barros\n\n    Session Z62: Cooperative Phenomena, Theory\n\n\n\n\n\n\n  *Work done while at Google",
      "summary": "Posted by Kate Weber and Shannon Leon, Google Research, Quantum AI Team Today the 2024 March Meeting of the American Physical Society (APS) kicks off ...",
      "summaryJa": "Posted by kate weber and shannon leon, google 研究, quantum ai team today the 2024 march meeting of the american physical society (aps) kicks off ...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 75
    },
    {
      "title": "VideoPrism: A foundational visual encoder for video understanding",
      "titleJa": "Videoprism: a foundational visual encoder for video understanding",
      "link": "http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html",
      "pubDate": "2024-02-22T20:05:00.000Z",
      "content": "Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research\n\n\n\n\nAn astounding number of videos are available on the Web, covering a variety of content from everyday moments people share to historical moments to scientific observations, each of which contains a unique record of the world. The right tools could help researchers analyze these videos, transforming how we understand the world around us.\n\n \n\nVideos offer dynamic visual content far more rich than static images, capturing movement, changes, and dynamic relationships between entities. Analyzing this complexity, along with the immense diversity of publicly available video data, demands models that go beyond traditional image understanding. Consequently, many of the approaches that best perform on video understanding still rely on specialized models tailor-made for particular tasks. Recently, there has been exciting progress in this area using video foundation models (ViFMs), such as VideoCLIP, InternVideo, VideoCoCa, and UMT. However, building a ViFM that handles the sheer diversity of video data remains a challenge.\n\n\nWith the goal of building a single model for general-purpose video understanding, we introduce “VideoPrism: A Foundational Visual Encoder for Video Understanding”. VideoPrism is a ViFM designed to handle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). We propose innovations in both the pre-training data as well as the modeling strategy. We pre-train VideoPrism on a massive and diverse dataset: 36 million high-quality video-text pairs and 582 million video clips with noisy or machine-generated parallel text. Our pre-training approach is designed for this hybrid data, to learn both from video-text pairs and the videos themselves. VideoPrism is incredibly easy to adapt to new video understanding challenges, and achieves state-of-the-art performance using a single frozen model.\n\n\n\n  \n\nVideoPrism is a general-purpose video encoder that enables state-of-the-art results over a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering, by producing video representations from a single frozen model.\n\n \n\nPre-training data\nYT-Temporal-180M, InternVid, VideoCC, WTS-70M, etc. This includes 36 million carefully selected videos with high-quality captions, along with an additional 582 million clips with varying levels of noisy text (like auto-generated transcripts). To our knowledge, this is the largest and most diverse video training corpus of its kind.\n\n\n\n\nStatistics on the video-text pre-training data. The large variations of the CLIP similarity scores (the higher, the better) demonstrate the diverse caption quality of our pre-training data, which is a byproduct of the various ways used to harvest the text.\n\n \n\nTwo-stage training\nvision transformer (ViT) with a factorized design that sequentially encodes spatial and temporal information following ViViT. Our training approach leverages both the high-quality video-text data and the video data with noisy text mentioned above. To start, we use contrastive learning (an approach that minimizes the distance between positive video-text pairs while maximizing the distance between negative video-text pairs) to teach our model to match videos with their own text descriptions, including imperfect ones. This builds a foundation for matching semantic language content to visual content.\n\n\nAfter video-text contrastive training, we leverage the collection of videos without text descriptions. Here, we build on the masked video modeling framework to predict masked patches in a video, with a few improvements. We train the model to predict both the video-level global embedding and token-wise embeddings from the first-stage model to effectively leverage the knowledge acquired in that stage. We then randomly shuffle the predicted tokens to prevent the model from learning shortcuts.\n\n\nWhat is unique about VideoPrism’s setup is that we use two complementary pre-training signals: text descriptions and the visual content within a video. Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics. This enables VideoPrism to excel in tasks that demand an understanding of both appearance and motion.\n\n \n\nResults\n\n\nVideoPrism compared to the previous best-performing FMs.\n\n\n \n\nClassification and localization\nVideoGLUE) covering classification and localization tasks. We find that (1) VideoPrism outperforms all of the other state-of-the-art FMs, and (2) no other single model consistently came in second place. This tells us that VideoPrism has learned to effectively pack a variety of video signals into one encoder — from semantics at different granularities to appearance and motion cues — and it works well across a variety of video sources.\n\n\n\n\nVideoPrism outperforms state-of-the-art approaches (including CLIP, VATT, InternVideo, and UMT) on the video understanding benchmark. In this plot, we show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. On Charades, ActivityNet, AVA, and AVA-K, we use mean average precision (mAP) as the evaluation metric. On the other datasets, we report top-1 accuracy.\n\n \n\nCombining with LLMs\nLiT) or a language decoder (such as PaLM-2), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks. We compare the combined models on a broad and challenging set of vision-language benchmarks. VideoPrism sets the new state of the art on most benchmarks. From the visual results, we find that VideoPrism is capable of understanding complex motions and appearances in videos (e.g., the model can recognize the different colors of spinning objects on the window in the visual examples below). These results demonstrate that VideoPrism is strongly compatible with language models.\n\n\n\n\nVideoPrism achieves competitive results compared with state-of-the-art approaches (including VideoCoCa, UMT and Flamingo) on multiple video-text retrieval (top) and video captioning and video QA (bottom) benchmarks. We also show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. We report the Recall@1 on MASRVTT, VATEX, and ActivityNet, CIDEr score on MSRVTT-Cap, VATEX-Cap, and YouCook2, top-1 accuracy on MSRVTT-QA and MSVD-QA, and WUPS index on NExT-QA.\n\n  \n  \n  \n\nWe show qualitative results using VideoPrism with a text encoder for video-text retrieval (first row) and adapted to a language decoder for video QA (second and third row). For video-text retrieval examples, the blue bars indicate the embedding similarities between the videos and the text queries.\n\n \n\nScientific applications\nFly vs. Fly, CalMS21, ChimpACT, and KABR. VideoPrism not only performs exceptionally well, but actually surpasses models designed specifically for those tasks. This suggests tools like VideoPrism have the potential to transform how scientists analyze video data across different fields.\n\n\n\n\nVideoPrism outperforms the domain experts on various scientific benchmarks. We show the absolute score differences to highlight the relative improvements of VideoPrism. We report mean average precision (mAP) for all datasets, except for KABR which uses class-averaged top-1 accuracy.\n\n \n\nConclusion\nAI Principles. We hope VideoPrism paves the way for future breakthroughs at the intersection of AI and video analysis, helping to realize the potential of ViFMs across domains such as scientific discovery, education, and healthcare.\n\n \n\nAcknowledgements\nThis blog post is made on behalf of all the VideoPrism authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong. We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this work. We are grateful to Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project. Lastly, we thank Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, and Mikhail Sirotenko for the tremendous help with making this blog post.",
      "summary": "Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research An astounding number of videos are avail...",
      "summaryJa": "Posted by long zhao, senior 研究 scientist, and ting liu, senior staff software engineer, google 研究 an astounding number of videos are avail...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 65
    },
    {
      "title": "Advances in private training for production on-device language models",
      "titleJa": "Advances in private 学習 for production on-device language models",
      "link": "http://blog.research.google/2024/02/advances-in-private-training-for.html",
      "pubDate": "2024-02-21T20:15:00.000Z",
      "content": "Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google\n\n\n\n\nLanguage models (LMs) trained to predict the next word given input text are the key technology for many applications [1, 2]. In Gboard, LMs are used to improve users’ typing experience by supporting features like next word prediction (NWP), Smart Compose, smart completion and suggestion, slide to type, and proofread. Deploying models on users’ devices rather than enterprise servers has advantages like lower latency and better privacy for model usage. While training on-device models directly from user data effectively improves the utility performance for applications such as NWP and smart text selection, protecting the privacy of user data for model training is important. \n\n\n\n\n\n\n\n\n\nGboard features powered by on-device language models.\n\nfederated learning (FL) in 2017 and formal differential privacy (DP) guarantees in 2022. FL enables mobile phones to collaboratively learn a model while keeping all the training data on device, and DP provides a quantifiable measure of data anonymization. Formally, DP is often characterized by (ε, δ) with smaller values representing stronger guarantees. Machine learning (ML) models are considered to have reasonable DP guarantees for ε=10 and strong DP guarantees for ε=1 when δ is small. \n\n\nAs of today, all NWP neural network LMs in Gboard are trained with FL with formal DP guarantees, and all future launches of Gboard LMs trained on user data require DP. These 30+ Gboard on-device LMs are launched in 7+ languages and 15+ countries, and satisfy (ɛ, δ)-DP guarantees of small δ of 10-10 and ɛ between 0.994 and 13.69. To the best of our knowledge, this is the largest known deployment of user-level DP in production at Google or anywhere, and the first time a strong DP guarantee of ɛ < 1 is announced for models trained directly on user data. \n\n\n\n\n    \nPrivacy principles and practices in Gboard\nPrivate Federated Learning in Gboard”, we discussed how different privacy principles are currently reflected in production models, including:\n\n\n\nTransparency and user control: We provide disclosure of what data is used, what purpose it is used for, how it is processed in various channels, and how Gboard users can easily configure the data usage in learning models. \n\n\nData minimization: FL immediately aggregates only focused updates that improve a specific model. Secure aggregation (SecAgg) is an encryption method to further guarantee that only aggregated results of the ephemeral updates can be accessed.   \n\n\nData anonymization: DP is applied by the server to prevent models from memorizing the unique information in individual user’s training data. \n\n\nAuditability and verifiability: We have made public the key algorithmic approaches and privacy accounting in open-sourced code (TFF aggregator, TFP DPQuery, DP accounting, and FL system). \n\n\n\n\n\n    \nA brief history\nGboard on-device LMs from user data. In 2020, a DP mechanism that clips and adds noise to model updates was used to prevent memorization for training the Spanish LM in Spain, which satisfies finite DP guarantees (Tier 3 described in “How to DP-fy ML“ guide). In 2022, with the help of the DP-Follow-The-Regularized-Leader (DP-FTRL) algorithm, the Spanish LM became the first production neural network trained directly on user data announced with a formal DP guarantee of (ε=8.9, δ=10-10)-DP (equivalent to the reported ρ=0.81 zero-Concentrated-Differential-Privacy), and therefore satisfies reasonable privacy guarantees (Tier 2). \n\n\n\n\n    \nDifferential privacy by default in federated learning \nFederated Learning of Gboard Language Models with Differential Privacy”, we announced that all the NWP neural network LMs in Gboard have DP guarantees, and all future launches of Gboard LMs trained on user data require DP guarantees. DP is enabled in FL by applying the following practices:\n\n\n\nPre-train the model with the multilingual C4 dataset.  \n\n\nVia simulation experiments on public datasets, find a large DP-noise-to-signal ratio that allows for high utility. Increasing the number of clients contributing to one round of model update improves privacy while keeping the noise ratio fixed for good utility, up to the point the DP target is met, or the maximum allowed by the system and the size of the population.\n\n\nConfigure the parameter to restrict the frequency each client can contribute (e.g., once every few days) based on computation budget and estimated population in the FL system. \n\n\nRun DP-FTRL training with limits on the magnitude of per-device updates chosen either via adaptive clipping, or fixed based on experience. \n\n\n\nSecAgg can be additionally applied by adopting the advances in improving computation and communication for scales and sensitivity.\n\n\n\n\n\n\nFederated learning with differential privacy and (SecAgg).\n\nReporting DP guarantees\nx-axis shows LMs labeled by language-locale and trained on corresponding populations; the y-axis shows the ε value when δ is fixed to a small value of 10-10 for (ε, δ)-DP (lower is better). The utility of these models are either significantly better than previous non-neural models in production, or comparable with previous LMs without DP, measured based on user-interactions metrics during A/B testing. For example, by applying the best practices, the DP guarantee of the Spanish model in Spain is improved from ε=8.9 to ε=5.37. SecAgg is additionally used for training the Spanish model in Spain and English model in the US. More details of the DP guarantees are reported in the appendix following the guidelines outlined in “How to DP-fy ML”. \n\n\n\n\n\n    \nTowards stronger DP guarantees\nε~10 DP guarantees of many launched LMs are already considered reasonable for ML models in practice, while the journey of DP FL in Gboard continues for improving user typing experience while protecting data privacy. We are excited to announce that, for the first time, production LMs of Portuguese in Brazil and Spanish in Latin America are trained and launched with a DP guarantee of  ε ≤ 1, which satisfies Tier 1 strong privacy guarantees. Specifically, the (ε=0.994, δ=10-10)-DP guarantee is achieved by running the advanced Matrix Factorization DP-FTRL (MF-DP-FTRL) algorithm, with 12,000+ devices participating in every training round of server model update larger than the common setting of 6500+ devices, and a carefully configured policy to restrict each client to at most participate twice in the total 2000 rounds of training in 14 days in the large Portuguese user population of Brazil. Using a similar setting, the es-US Spanish LM was trained in a large population combining multiple countries in Latin America to achieve (ε=0.994, δ=10-10)-DP. The ε ≤ 1 es-US model significantly improved the utility in many countries, and launched in Colombia, Ecuador, Guatemala, Mexico, and Venezuela. For the smaller population in Spain, the DP guarantee of es-ES LM is improved from ε=5.37 to ε=3.42 by only replacing DP-FTRL with MF-DP-FTRL without increasing the number of devices participating every round. More technical details are disclosed in the colab for privacy accounting. \n\n\n\n\n\n\n\nDP guarantees for Gboard NWP LMs (the purple bar represents the first es-ES launch of ε=8.9; cyan bars represent privacy improvements for models trained with MF-DP-FTRL; tiers are from “How to DP-fy ML“ guide; en-US* and es-ES* are additionally trained with SecAgg).\n\nDiscussion and next steps\nand a large number of devices' contributions are aggregated. Privacy-utility-computation trade-offs can be improved by using public data, the new MF-DP-FTRL algorithm, and tightening accounting. With these techniques, a strong DP guarantee of ε ≤ 1 is possible but still challenging. Active research on empirical privacy auditing [1, 2] suggests that DP models are potentially more private than the worst-case DP guarantees imply. While we keep pushing the frontier of algorithms, which dimension of privacy-utility-computation should be prioritized?\n\n\nWe are actively working on all privacy aspects of ML, including extending DP-FTRL to distributed DP and improving auditability and verifiability. Trusted Execution Environment opens the opportunity for substantially increasing the model size with verifiable privacy. The recent breakthrough in large LMs (LLMs) motivates us to rethink the usage of public information in private training and more future interactions between LLMs, on-device LMs, and Gboard production.  \n\n\n\n\n\n    \nAcknowledgments\nThe authors would like to thank Peter Kairouz, Brendan McMahan, and Daniel Ramage for their early feedback on the blog post itself, Shaofeng Li and Tom Small for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The collaborators below directly contribute to the presented results:\n\n\nResearch and algorithm development: Galen Andrew, Stanislav Chiknavaryan, Christopher A. Choquette-Choo, Arun Ganesh, Peter Kairouz, Ryan McKenna, H. Brendan McMahan, Jesse Rosenstock, Timon Van Overveldt, Keith Rush, Shuang Song, Thomas Steinke, Abhradeep Guha Thakurta, Om Thakkar, and Yuanbo Zhang.\n\n\nInfrastructure, production and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis, Yun Wang, Shanshan Wu, Yu Xiao, and Shumin Zhai.",
      "summary": "Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google Language models (LMs) trained to predict the next word given inp...",
      "summaryJa": "Posted by zheng xu, 研究 scientist, and yanxiang zhang, software engineer, google language models (lms) trained to predict the next word given inp...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 95
    },
    {
      "title": "Gemma: Introducing new state-of-the-art open models",
      "titleJa": "Gemma: introducing new state-of-the-art open models",
      "link": "https://deepmind.google/discover/blog/gemma-introducing-new-state-of-the-art-open-models/",
      "pubDate": "Wed, 21 Feb 2024 13:06:00 +0000",
      "content": "Gemma is built for responsible AI development from the same research and technology used to create Gemini models.",
      "summary": "Gemma is built for responsible AI development from the same research and technology used to create Gemini models.",
      "summaryJa": "Gemma is built for responsible ai development from the same 研究 and technology used to create gemini models.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Our next-generation model: Gemini 1.5",
      "titleJa": "Our next-generation モデル: gemini 1.5",
      "link": "https://deepmind.google/discover/blog/our-next-generation-model-gemini-15/",
      "pubDate": "Thu, 15 Feb 2024 15:00:00 +0000",
      "content": "The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.",
      "summary": "The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.",
      "summaryJa": "The モデル delivers dramatically enhanced performance, with a ブレークスルー in long-context understanding across modalities.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 60
    },
    {
      "title": "Learning the importance of training data under concept drift",
      "titleJa": "Learning the importance of 学習 data under concept drift",
      "link": "http://blog.research.google/2024/02/learning-importance-of-training-data.html",
      "pubDate": "2024-02-14T18:32:00.000Z",
      "content": "Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research\n\n\n\n\nThe constantly changing nature of the world around us poses a significant challenge for the development of AI models. Often, models are trained on longitudinal data with the hope that the training data used will accurately represent inputs the model may receive in the future. More generally, the default assumption that all training data are equally relevant often breaks in practice. For example, the figure below shows images from the CLEAR nonstationary learning benchmark, and it illustrates how visual features of objects evolve significantly over a 10 year span (a phenomenon we refer to as slow concept drift), posing a challenge for object categorization models. \n\n\n\n\n\n\n\n\nSample images from the CLEAR benchmark. (Adapted from Lin et al.)\n\nonline and continual learning, repeatedly update a model with small amounts of recent data in order to keep it current. This implicitly prioritizes recent data, as the learnings from past data are gradually erased by subsequent updates. However in the real world, different kinds of information lose relevance at different rates, so there are two key issues: 1) By design they focus exclusively on the most recent data and lose any signal from older data that is erased. 2) Contributions from data instances decay uniformly over time irrespective of the contents of the data.\n\n\nIn our recent work, “Instance-Conditional Timescales of Decay for Non-Stationary Learning”, we propose to assign each instance an importance score during training in order to maximize model performance on future data. To accomplish this, we employ an auxiliary model that produces these scores using the training instance as well as its age. This model is jointly learned with the primary model. We address both the above challenges and achieve significant gains over other robust learning methods on a range of benchmark datasets for nonstationary learning. For instance, on a recent large-scale benchmark for nonstationary learning (~39M photos over a 10 year period), we show up to 15% relative accuracy gains through learned reweighting of training data.\n\n\n\n\n\n    \nThe challenge of concept drift for supervised learning\nrecent photo categorization task, comprising roughly 39M photographs sourced from social media websites over a 10 year period. We compared offline training, which iterated over all the training data multiple times in random order, and continual training, which iterated multiple times over each month of data in sequential (temporal) order. We measured model accuracy both during the training period and during a subsequent period where both models were frozen, i.e., not updated further on new data (shown below). At the end of the training period (left panel, x-axis = 0), both approaches have seen the same amount of data, but show a large performance gap. This is due to catastrophic forgetting, a problem in continual learning where a model’s knowledge of data from early on in the training sequence is diminished in an uncontrolled manner. On the other hand, forgetting has its advantages — over the test period (shown on the right), the continual trained model degrades much less rapidly than the offline model because it is less dependent on older data. The decay of both models’ accuracy in the test period is confirmation that the data is indeed evolving over time, and both models become increasingly less relevant.\n\n\n\n\n\n\n\nComparing offline and continually trained models on the photo classification task.\n\nTime-sensitive reweighting of training data\nM, given some training data collected over time. We propose to also train a helper model that assigns a weight to each point based on its contents and age. This weight scales the contribution from that data point in the training objective for M. The objective of the weights is to improve the performance of M on future data. \n\n\nIn our work, we describe how the helper model can be meta-learned, i.e., learned alongside M in a manner that helps the learning of the model M itself. A key design choice of the helper model is that we separated out instance- and age-related contributions in a factored manner. Specifically, we set the weight by combining contributions from multiple different fixed timescales of decay, and learn an approximate “assignment” of a given instance to its most suited timescales. We find in our experiments that this form of the helper model outperforms many other alternatives we considered, ranging from unconstrained joint functions to a single timescale of decay (exponential or linear), due to its combination of simplicity and expressivity. Full details may be found in the paper. \n\n\n\n\n\n    \nInstance weight scoring\nCLEAR object recognition challenge; older-looking objects are correspondingly down-weighted. On closer examination (bottom figure below, gradient-based feature importance assessment), we see that the helper model focuses on the primary object within the image, as opposed to, e.g., background features that may spuriously be correlated with instance age.\n\n\n\n\n\n\n\nSample images from the CLEAR benchmark (camera & computer categories) assigned the highest and lowest weights respectively by our helper model.\n\n\n\nFeature importance analysis of our helper model on sample images from the CLEAR benchmark.\n\nResults\nGains on large-scale data \nphoto categorization task (PCAT) on the YFCC100M dataset discussed earlier, using the first five years of data for training and the next five years as test data. Our method (shown in red below) improves substantially over the no-reweighting baseline (black) as well as many other robust learning techniques. Interestingly, our method deliberately trades off accuracy on the distant past (training data unlikely to reoccur in the future) in exchange for marked improvements in the test period. Also, as desired, our method degrades less than other baselines in the test period. \n\n\n\n\n\n\n\nComparison of our method and relevant baselines on the PCAT dataset.\n\nBroad applicability\n1, 2, 3, 4 for details) that spans data sources and modalities (photos, satellite images, social media text, medical records, sensor readings, tabular data) and sizes (ranging from 10k to 39M instances). We report significant gains in the test period when compared to the nearest published benchmark method for each dataset (shown below). Note that the previous best-known method may be different for each dataset. These results showcase the broad applicability of our approach.\n\n\n\n\n\n\n\n\nPerformance gain of our method on a variety of tasks studying natural concept drift. Our reported gains are over the previous best-known method for each dataset.\n\nExtensions to continual learning\nwithin the context of each bucket of data being used to sequentially update the model. This proposal still retains some limitations of continual learning, e.g., model updates are performed only on most-recent data, and all optimization decisions (including our reweighting) are only made over that data. Nevertheless, our approach consistently beats regular continual learning as well as a wide range of other continual learning algorithms on the photo categorization benchmark (see below). Since our approach is complementary to the ideas in many baselines compared here, we anticipate even larger gains when combined with them.\n\n\n\n\n\n\n\n\nResults of our method adapted to continual learning, compared to the latest baselines.\n\n \n\n    \n  \nConclusion\nAcknowledgements\nWe thank Mike Mozer for many interesting discussions in the early phase of this work, as well as very helpful advice and feedback during its development.",
      "summary": "Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research The constantly changing nature of the world a...",
      "summaryJa": "Posted by nishant jain, pre-doctoral researcher, and pradeep shenoy, 研究 scientist, google 研究 the constantly changing nature of the world a...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "DP-Auditorium: A flexible library for auditing differential privacy",
      "titleJa": "Dp-auditorium: a flexible library for auditing differential プライバシー",
      "link": "http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html",
      "pubDate": "2024-02-13T22:11:00.000Z",
      "content": "Posted by Mónica Ribero Díaz, Research Scientist, Google Research\n\n\n\n\n\nDifferential privacy (DP) is a property of randomized mechanisms that limit the influence of any individual user’s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies across industries and government applications (e.g., the US census) without compromising individual user identities.  As its adoption increases, it’s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, researchers compared six sparse vector technique (SVT) variations and found that only two of the six actually met the asserted privacy guarantee. Even when mathematical proofs are correct, the code implementing the mechanism is vulnerable to human error.\n\n\n\nHowever, practical and efficient DP auditing is challenging primarily due to the inherent randomness of the mechanisms and the probabilistic nature of the tested guarantees. In addition, a range of guarantee types exist, (e.g., pure DP, approximate DP, Rényi DP, and concentrated DP), and this diversity contributes to the complexity of formulating the auditing problem. Further, debugging mathematical proofs and code bases is an intractable task given the volume of proposed mechanisms. While ad hoc testing techniques exist under specific assumptions of mechanisms, few efforts have been made to develop an extensible tool for testing DP mechanisms. \n\n\n\nTo that end, in “DP-Auditorium: A Large Scale Library for Auditing Differential Privacy”, we introduce an open source library for auditing DP guarantees with only black-box access to a mechanism (i.e., without any knowledge of the mechanism’s internal properties). DP-Auditorium is implemented in Python and provides a flexible interface that allows contributions to continuously improve its testing capabilities. We also introduce new testing algorithms that perform divergence optimization over function spaces for Rényi DP, pure DP, and approximate DP. We demonstrate that DP-Auditorium can efficiently identify DP guarantee violations, and suggest which tests are most suitable for detecting particular bugs under various privacy guarantees.\n\n\n\n\n    \nDP guarantees\nM (D)) that satisfies a mathematical property ensuring the privacy of user data. A DP guarantee is thus tightly related to properties between pairs of probability distributions. A mechanism is differentially private if the probability distributions determined by M on dataset D and a neighboring dataset D’, which differ by only one record, are indistinguishable under a given divergence metric. \n\n\n\nFor example, the classical approximate DP definition states that a mechanism is approximately DP with parameters (ε, δ) if the hockey-stick divergence of order eε, between M(D) and M(D’), is at most δ. Pure DP is a special instance of approximate DP where δ = 0. Finally, a mechanism is considered Rényi DP with parameters (𝛼, ε) if the Rényi divergence of order 𝛼, is at most ε (where ε is a small positive value). In these three definitions, ε is not interchangeable but intuitively conveys the same concept; larger values of ε imply larger divergences between the two distributions or less privacy, since the two distributions are easier to distinguish.  \n\n\n\n    \nDP-Auditorium\ngradient descent mechanism variants. \n\n\n\nProperty testers determine if evidence exists to reject the hypothesis that a given divergence between two probability distributions, P and Q, is bounded by a prespecified budget determined by the DP guarantee being tested. They compute a lower bound from samples from P and Q, rejecting the property if the lower bound value exceeds the expected divergence. No guarantees are provided if the result is indeed bounded. To test for a range of privacy guarantees, DP-Auditorium introduces three novel testers: (1) HockeyStickPropertyTester, (2) RényiPropertyTester, and (3) MMDPropertyTester. Unlike other approaches, these testers don’t depend on explicit histogram approximations of the tested distributions. They rely on variational representations of the hockey-stick divergence, Rényi divergence, and maximum mean discrepancy (MMD) that enable the estimation of divergences through optimization over function spaces. As a baseline, we implement HistogramPropertyTester, a commonly used approximate DP tester. While our three testers follow a similar approach, for brevity, we focus on the HockeyStickPropertyTester in this post.\n\n\n\nGiven two neighboring datasets, D and D’, the HockeyStickPropertyTester finds a lower bound,^δ  for the hockey-stick divergence between M(D) and M(D’) that holds with high probability. Hockey-stick divergence enforces that the two distributions M(D) and M(D’) are close under an approximate DP guarantee. Therefore, if a privacy guarantee claims that the hockey-stick divergence is at most δ, and^δ  > δ, then with high probability the divergence is higher than what was promised on D and D’ and the mechanism cannot satisfy the given approximate DP guarantee. The lower bound^δ  is computed as an empirical and tractable counterpart of a variational formulation of the hockey-stick divergence (see the paper for more details). The accuracy of^δ  increases with the number of samples drawn from the mechanism, but decreases as the variational formulation is simplified. We balance these factors in order to ensure that^δ  is both accurate and easy to compute. \n\n\n\nDataset finders use black-box optimization to find datasets D and D’ that maximize^δ, a lower bound on the divergence value δ. Note that black-box optimization techniques are specifically designed for settings where deriving gradients for an objective function may be impractical or even impossible. These optimization techniques oscillate between exploration and exploitation phases to estimate the shape of the objective function and predict areas where the objective can have optimal values. In contrast, a full exploration algorithm, such as the grid search method, searches over the full space of neighboring datasets D and D’. DP-Auditorium implements different dataset finders through the open sourced black-box optimization library Vizier. \n\n\nRunning existing components on a new mechanism only requires defining the mechanism as a Python function that takes an array of data D and a desired number of samples n to be output by the mechanism computed on D. In addition, we provide flexible wrappers for testers and dataset finders that allow practitioners to implement their own testing and dataset search algorithms.\n\n\n\n    \nKey results\nε, and report the number of times each tester identifies privacy bugs. While no tester consistently outperforms the others, we identify bugs that would be missed by previous techniques (HistogramPropertyTester). Note that the HistogramPropertyTester is not applicable to SVT mechanisms. \n\n\n\n\nNumber of times each property tester finds the privacy violation for the tested non-private mechanisms. NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the Laplace and Gaussian mechanisms for computing the mean.\n\nDP gradient descent algorithm (DP-GD) in TensorFlow that computes gradients of the loss function on private data. To preserve privacy, DP-GD employs a clipping mechanism to bound the l2-norm of the gradients by a value G, followed by the addition of Gaussian noise. This implementation incorrectly assumes that the noise added has a scale of G, while in reality, the scale is sG, where s is a positive scalar. This discrepancy leads to an approximate DP guarantee that holds only for values of s greater than or equal to 1.\n\n\n\nWe evaluate the effectiveness of property testers in detecting this bug and show that HockeyStickPropertyTester and RényiPropertyTester exhibit superior performance in identifying privacy violations, outperforming MMDPropertyTester and HistogramPropertyTester. Notably, these testers detect the bug even for values of s as high as 0.6. It is worth highlighting that s = 0.5 corresponds to a common error in literature that involves missing a factor of two when accounting for the privacy budget ε. DP-Auditorium successfully captures this bug as shown below. For more details see section 5.6 here.\n\n\n\n\nEstimated divergences and test thresholds for different values of s when testing DP-GD with the HistogramPropertyTester (left) and the HockeyStickPropertyTester (right).\n\n\n\nEstimated divergences and test thresholds for different values of s when testing DP-GD with the RényiPropertyTester (left) and the MMDPropertyTester (right)\n\npaper.\n\n\n\n\n    \nConclusion\nopen sourcing DP-Auditorium, we aim to establish a standard for end-to-end testing of new differentially private algorithms.\n\n\n\n    \nAcknowledgements\nThe work described here was done jointly with Andrés Muñoz Medina, William Kong and Umar Syed. We thank Chris Dibak and Vadym Doroshenko for helpful engineering support and interface suggestions for our library.",
      "summary": "Posted by Mónica Ribero Díaz, Research Scientist, Google Research Differential privacy (DP) is a property of randomized mechanisms that limit the infl...",
      "summaryJa": "Posted by mónica ribero díaz, 研究 scientist, google 研究 differential プライバシー (dp) is a property of randomized mechanisms that limit the infl...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "The next chapter of our Gemini era",
      "titleJa": "The next chapter of our gemini era",
      "link": "https://deepmind.google/discover/blog/google-gemini-update-sundar-pichai-2024/",
      "pubDate": "Thu, 08 Feb 2024 13:00:00 +0000",
      "content": "We're bringing Gemini to more Google products",
      "summary": "We're bringing Gemini to more Google products",
      "summaryJa": "We're bringing gemini to more google products",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Graph neural networks in TensorFlow",
      "titleJa": "Graph neural networks in tensorflow",
      "link": "http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html",
      "pubDate": "2024-02-06T19:17:00.000Z",
      "content": "Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML\n\n\n\n\nObjects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks. Discrete mathematics and computer science have a long history of formalizing such networks as graphs, consisting of nodes connected by edges in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. \n\n\n\n\nGraph neural networks, or GNNs for short, have emerged as a powerful technique to leverage both the graph’s connectivity (as in the older algorithms DeepWalk and Node2Vec) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What’s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph's discrete, relational information in a continuous way so that it can be included naturally in another deep learning system.\n\n\nWe are excited to announce the release of TensorFlow GNN 1.0 (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN's heterogeneous focus makes it natural to represent them.\n\n\n  Inside TensorFlow, such graphs are represented by objects of type tfgnn.GraphTensor. This is a composite tensor type (a collection of tensors in one Python class) accepted as a first-class citizen in tf.data.Dataset, tf.function, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level Keras API, or directly using the tfgnn.GraphTensor primitive.\n\n\n\n\n\n    \nGNNs: Making predictions for an object in context\n\n\nPictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.\n\nthis one), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by Apache Beam for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for in-memory and beam-based sampling, respectively.\n\n\nOn those same sampled subgraphs, the GNN’s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node's neighborhood. One classical approach is message-passing neural networks. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After n rounds, the hidden state of the root node reflects the aggregate information from all nodes within n edges (pictured below for n = 2). The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges\n\n\n\n\n\n\n\nPictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.\n\nloss (to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. \n\n\nBeyond supervised training (i.e., minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (i.e., without labels). This lets us compute a continuous representation (or embedding) of the discrete graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs.\n\n\n\n\n\n\n    \nBuilding GNN architectures\n\n\n\nGraphNets. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN user guide and model collection.\n\n\n\n\n\n\n    \nTraining orchestration\nTF-GNN Runner also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this:\n\n\n\n\n\ntfgnn.GraphTensor padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks:\n\n\n\n\n\nintegrated gradients for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. \n\n\n\n\n\n\n    \nConclusion\nColab demo with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our user guides and Colabs, or take a look at our paper.\n\n\n\n\n\n\n    \nAcknowledgements\nThe TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind: Alvaro Sanchez-Gonzalez and Lisa Wang.",
      "summary": "Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML Objects and their relationships are ubiqui...",
      "summaryJa": "Posted by dustin zelle, software engineer, google 研究, and arno eigenwillig, software engineer, coreml objects and their relationships are ubiqui...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 65
    },
    {
      "title": "A decoder-only foundation model for time-series forecasting",
      "titleJa": "A decoder-only foundation モデル for time-series forecasting",
      "link": "http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html",
      "pubDate": "2024-02-02T19:07:00.000Z",
      "content": "Posted by Rajat Sen and Yichen Zhou, Google Research\n\n\n\n\n\nTime-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models performed well in the M5 competition).\n\n\n\nAt the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as translation, retrieval-augmented generation, and code completion. These models are trained on massive amounts of textual data derived from a variety of sources like common crawl and open-source code that allows them to identify patterns in languages. This makes them very powerful zero-shot tools; for instance, when paired with retrieval, they can answer questions about and summarize current events.\n\n\n\nDespite DL-based forecasters largely outperforming traditional methods and progress being made in reducing training and inference costs, they face challenges: most DL architectures require long and involved training and validation cycles before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like retail demand planning.\n\n\n\nTo that end, in “A decoder-only foundation model for time-series forecasting”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in Google Cloud Vertex AI.\n\n\n\n    \nA decoder-only foundation model for time-series forecasting\ndecoder-only fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal transformer layers that produce an output corresponding to each input token (it cannot attend to future tokens). Finally, the output corresponding to the i-th token summarizes all the information from previous tokens and predicts the (i+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”.\n\n\n\nA foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining dataset. Similar to LLMs, we use stacked transformer layers (self-attention and feedforward layers) as the main building blocks for the TimesFM model. In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent long-horizon forecasting work. The task then is to forecast the (i+1)-th patch of time-points given the i-th output at the end of the stacked transformer layers. \n\n\n\nHowever, there are several key differences from language models. Firstly, we need a multilayer perceptron block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with positional encodings (PE). For that, we use a residual block similar to our prior work in long-horizon forecasting. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, i.e., the output patch length can be larger than the input patch length.  \n\n\n\nConsider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting\n\n\n\n\n\nTimesFM architecture.\n\nPretraining data\nSynthetic data helps with the basics. Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting.\n\n  \n    Real-world data adds real-world flavor. We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are Google Trends and Wikipedia Pageviews, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training.\n\nZero-shot evaluation results\nARIMA, ETS and can match or outperform powerful DL models like DeepAR, PatchTST that have been explicitly trained on the target time-series.\n\n\n\nWe used the Monash Forecasting Archive to evaluate TimesFM’s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the mean absolute error (MAE) appropriately scaled so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to GPT-3.5 for forecasting using a specific prompting technique proposed by llmtime(ZS). We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller.\n\n\n\n\n\nScaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.\n\nPatchTST (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on ETT datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the llmtime paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. \n\n\n\n\n\n\n\n\nLast window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.\n\nConclusion\nAcknowledgements\nThis work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.",
      "summary": "Posted by Rajat Sen and Yichen Zhou, Google Research Time-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing,...",
      "summaryJa": "Posted by rajat sen and yichen zhou, google 研究 time-series forecasting is ubiquitous in various domains, such as retail, finance, manufacturing,...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 95
    },
    {
      "title": "Intervening on early readouts for mitigating spurious features and simplicity bias",
      "titleJa": "Intervening on early readouts for mitigating spurious features and simplicity バイアス",
      "link": "http://blog.research.google/2024/02/intervening-on-early-readouts-for.html",
      "pubDate": "2024-02-02T17:49:00.000Z",
      "content": "Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research\n\n\n\n\nMachine learning models in the real world are often trained on limited data that may contain unintended statistical biases. For example, in the CELEBA celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as medical diagnosis. \n\n\n\n\n\nSurprisingly, recent work has also discovered an inherent tendency of deep networks to amplify such statistical biases, through the so-called simplicity bias of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. \n\n\nWith the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying early readouts and feature forgetting. First, in “Using Early Readouts to Mediate Featural Bias in Distillation”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in model distillation, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “Overcoming Simplicity Bias in Deep Networks using a Feature Sieve”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model’s ability to generalize to unseen domains compared to previous approaches. Our AI Principles and our Responsible AI practices guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases.\n\n\n\n\n\n\n\nAnimation comparing hypothetical responses from two models trained with and without the feature sieve.\n\nEarly readouts for debiasing distillation\nearly readouts and their application in debiased distillation, i.e., making sure that the student model inherits the teacher model’s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the cross-entropy loss between student outputs and the ground-truth labels) and teacher matching (minimizing the KL divergence loss between student and teacher outputs for any given input). \n\n\nSuppose one trains a linear decoder, i.e., a small auxiliary neural network named as Aux, on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model’s dependence on potentially spurious features.\n\n\n\n\n\n\n\n\nIllustrating the usage of early readouts (i.e., output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.\n\nWaterbirds, CelebA, CivilComments, MNLI). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color. Thus, a measure of model performance is its worst group accuracy, i.e., the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our paper.\n\n\n\n\n\n\n\n\nComparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.\n\nOvercoming simplicity bias with a feature sieve\nfeature learning and generalization. The workflow alternates between identifying problematic features and erasing identified features from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned.  \n\n\n\n\n\n\n\nTraining workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).\n\nIdentifying simple features:  We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. \n\n\nApplying the feature sieve: We aim to erase the identified features in the early layers of the neural network with the use of a novel forgetting loss, Lf , which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged.\n\n\n\nWe can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via hyperparameter search to maximize validation accuracy, a  standard measure of generalization. Since we include “no-forgetting” (i.e., the baseline model) in the search space, we expect to find settings that are at least as good as the baseline.\n\n\nBelow we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (BAR) and animal categorization (NICO). Feature importance was estimated using post-hoc gradient-based importance scoring (GRAD-CAM), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. \n\n\n\n\n\n\n\nFeature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.\n\nBAR, CelebA Hair, NICO and ImagenetA, by margins up to 11% (see figure below). More details are available in our paper.\n\n\n\n\n\n\n\nOur feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.\n\nConclusion\nAcknowledgements \nThe work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at IIT Bombay. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.",
      "summary": "Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research Machine learning models in the real world a...",
      "summaryJa": "Posted by rishabh tiwari, pre-doctoral researcher, and pradeep shenoy, 研究 scientist, google 研究 機械学習 models in the real world a...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "MobileDiffusion: Rapid text-to-image generation on-device",
      "titleJa": "Mobilediffusion: rapid text-to-image generation on-device",
      "link": "http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html",
      "pubDate": "2024-01-31T21:59:00.000Z",
      "content": "Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML\n\n\n\n\nText-to-image diffusion models have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., Stable Diffusion, DALL·E, and Imagen). While recent advancements in inference solutions on Android via MediaPipe and iOS via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.\n \n\nTo that end, in “MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt DiffusionGAN to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.\n\n\n\n\n  \n\n    \n          \n  \n  \n  \n  \n  \n  \n  \n\n\nRapid text-to-image generation on-device.\n\nBackground\niterative denoising to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.\n\n\nThe optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., DPM) or distillation techniques (e.g., progressive distillation, consistency distillation), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like DiffusionGAN and Adversarial Diffusion Distillation, even reduce to a single necessary step. \n\n\nHowever, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., SnapFusion). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.\n\n\n\n\n\n\n    \nMobileDiffusion\nUNet architecture. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.\n\n\nThe design of MobileDiffusion follows that of latent diffusion models. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use CLIP-ViT/L14, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. \n\n\n\n\n\n    \nDiffusion UNet\nUViT architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. \n\n\n\n\n\n\n\n\nOur UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.\n\nResNet blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is separable convolution. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.\n\n\nIn the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of FLOPs (floating-point operations) and number of parameters. \n\n\n\n\n\n\n\nComparison of some diffusion UNets.\n\nImage decoder\nvariational autoencoder (VAE) to encode an RGB image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our paper.\n\n\n\n\n\n\n\nVAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).\n\n\n   Decoder\n   \n     #Params (M)  \n   \n     PSNR↑  \n   \n     SSIM↑  \n   \n     LPIPS↓  \n   \n  \nSD\n   \n   49.5\n   \n   26.7\n   \n   0.76\n   \n   0.037\n   \n  \nOurs\n   \n   39.3\n   \n   30.0\n   \n   0.83\n   \n   0.032\n   \n  \nOurs-Lite    \n   \n   9.8\n   \n   30.2\n   \n   0.84\n   \n   0.032\n   \n  \n\n\n\n\nQuality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).\n\nOne-step sampling\nDiffusionGAN hybrid to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., StyleGAN-T, GigaGAN) confront similar complexities, resulting in highly intricate and expensive training.\n\n\nTo overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.\n\n\nThe figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  \n\n\n\n\n\n\nIllustration of DiffusionGAN fine-tuning.\n\nResults\n\n\nImages generated by our MobileDiffusion\n\n\n\nLatency measurements (s) on mobile devices.\n\nConclusion\nresponsible AI practices.\n\n\n\n\n\n    \nAcknowledgments\nWe like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.",
      "summary": "Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML Text-to-image diffusion models have shown excep...",
      "summaryJa": "Posted by yang zhao, senior software engineer, and tingbo hou, senior staff software engineer, core ml text-to-image diffusion models have shown excep...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 50
    },
    {
      "title": "Mixed-input matrix multiplication performance optimizations",
      "titleJa": "Mixed-input matrix multiplication performance optimizations",
      "link": "http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html",
      "pubDate": "2024-01-26T19:56:00.000Z",
      "content": "Posted by Manish Gupta, Staff Software Engineer, Google Research\n\n\n\n\nAI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs).  LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver tens of exaflops of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively.\n\n\n\nThe bulk of an LLM’s memory and compute are consumed by weights in matrix multiplication operations. Using narrower data types reduces memory consumption. For example, storing weights in the 8-bit integer (i.e., U8 or S8) data type reduces the memory footprint by 4× relative to single-precision (F32) and 2× relative to half-precision (F16) or bfloat16 (BF16). Furthermore, previous work has shown that LLM models running matrix multiplications with weights in S8 and input in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as weight-only quantization and requires efficient implementation of matrix multiplication with mixed-inputs, e.g., half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations.\n\n\nTo that end, in this blog we focus on mapping mixed-input matrix multiplication onto the NVIDIA Ampere architecture. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source NVIDIA/CUTLASS repository. \n\n\n\n\nMemory footprint for an 175B parameter LLM model with various data types formats.\n\nThe matrix-multiply-accumulate operation\nGoogle’s TPU and NVIDIA’s GPU multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the matrix-multiply-accumulate (mma) operation. For the rest of the blog the reference to mma is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the mma operation are fixed in hardware. This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. \n\n\n\nThe Tensor Core mma operation is defined by specifying two input matrices (e.g., A & B, shown below) to produce a result matrix, C. The mma operation natively supports mixed-precision. Mixed-precision Tensor Cores allow mixing input (A and B) data type with the result (C) data type. In contrast, mixed-input matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software.\n\n\n\n\nTensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.\n\nChallenges of mixed-input matrix multiplication\nhierarchy of memory, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core mma operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a warp. The supported data type and layout within a warp are fixed for an mma operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. \n\n\n\n    \nData type conversion \nmma operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the mixed-input matrix multiplication to hardware-supported mixed-precision Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve  performance.\n\n\n\n\n    \nLayout conformance \nmma operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix B of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called layout conformance and needs to be achieved in the software. \n\n\nThe figure below shows an mma operation consuming matrix A and matrix B from registers to produce matrix C in registers, distributed across one warp. The thread T0 is highlighted and zoomed in to show the weight matrix B goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core operation.\n\n\n\n\nThe mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100.)\n\nSoftware strategies addressing challenges\n\n\nNumericArrayConvertor from 4xU8 to 2x(2xF16) in 32-bit registers.\n\nNarrower bitwidth shared memory loads: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in two 32-bit registers, with each register containing 2xF16 values (shown above for the matrix B’s thread T0). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth.\n\n\nPre-processing in global memory: An alternative strategy involves rearranging the data within the global memory (one level above the shared memory in memory hierarchy), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. NVIDIA/FasterTransformer adopts this method to effectively address layout conformance challenges.\n\n\n\n\n\n    \nOptimized software strategies\nFastNumericArrayConvertor and FragmentShuffler, respectively. \n\n\nFastNumericArrayConvertor operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. \n\n\nThe conversion sequence for U8-to-F16 is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. FastNumericArrayConvertor uses the permute byte to rearrange bytes of 4xU8 into two registers. Additionally, FastNumericArrayConvertor does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in two 32-bit registers containing  2x(2xF16) values. The FastNumericArrayConvertor for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above.\n\n\n\n\nFastNumericArrayConvertor utilizes permute bytes and packed arithmetic, reducing the number of instructions in the data type conversion.\n\nFragmentShuffler handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations. \n\n\nNVIDIA Ampere architecture provides a load matrix instruction (ldmatrix). The ldmatrix is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the shape and layout that mma matrix A and B consume. The use of ldmatrix reduces the number of load instructions and increases the memory bandwidth utilization. Since the ldmatrix instruction moves U8 data to registers, the layout after the load conforms with U8*U8 mma operation, and not with F16*F16 mma operation. We implemented FragmentShuffler to rearrange the data within registers using shuffle (shfl.sync) operations to achieve the layout conformance. \n\n\nFastNumericArrayConvertor covering data type conversion from U8-to-F16, S8-to-F16, U8-to-BF16, and S8-to-BF16.\n\n\n\n\n\n    \nPerformance results\nour method (shown below in blue and red; varying the data types of matrix A and B) and two mixed-precision data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in FLOPS (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. \n\n\n\n\nMixed-input matrix multiplication  performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape m=3456, n=4096, k=2048.\n\nAcknowledgements\nWe would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.",
      "summary": "Posted by Manish Gupta, Staff Software Engineer, Google Research AI-driven technologies are weaving themselves into the fabric of our daily routines, ...",
      "summaryJa": "Posted by manish gupta, staff software engineer, google 研究 ai-driven technologies are weaving themselves into the fabric of our daily routines, ...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 65
    },
    {
      "title": "Exphormer: Scaling transformers for graph-structured data",
      "titleJa": "Exphormer: scaling transformers for graph-structured data",
      "link": "http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html",
      "pubDate": "2024-01-23T22:27:00.000Z",
      "content": "Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google\n\n\n\n\nGraphs, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. \n\n\n\n\nA common approach to learning on graphs are graph neural networks (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a message-passing framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors.\n\n\nRecently, graph transformer models have emerged as a popular alternative to message-passing GNNs. These models build on the success of Transformer architectures in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see the first open problem here).\n\n\nA natural remedy is to use a sparse interaction graph with fewer edges. Many sparse and efficient transformers have been proposed to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner.\n\n\nIn “Exphormer: Sparse Transformers for Graphs”, presented at ICML 2023, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from spectral graph theory, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on GitHub.\n\n \n\nExpander graphs\nexpander graphs, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, i.e., a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes.\n\n\nA common class of expander graphs are d-regular expanders, in which there are d edges from every node (i.e., every node has degree d). The quality of an expander graph is measured by its spectral gap, an algebraic property of its adjacency matrix (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as Ramanujan graphs — they achieve a gap of d - 2*√(d-1), which is essentially the best possible among d-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of d. We use a randomized expander construction of Friedman, which produces near-Ramanujan graphs.\n\n\n\n\nExpander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.\n\nExphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse d-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that d is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.\n \n\nExphormer: Constructing a sparse interaction graph\nEdges from the input graph (local attention)\n\n\nEdges from a constant-degree expander graph (expander attention)\n\n\nEdges from every node to a small set of virtual nodes (global attention)\n\n\n\n\n\nExphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.\n\n1, 2].\n\n\n    \nRelation to sparse Transformers for sequences\nBigBird, which  builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an Erdős-Rényi random graph model for the remaining components.\n\n\nWindow attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs.\n\n\nThe Erdős-Rényi graph on n nodes, G(n, p), which connects every pair of nodes independently with probability p, also functions as an expander graph for suitably high p. However, a superlinear number of edges (Ω(n log n)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a linear number of edges.\n\n \n\nExperimental results\nGraphGPS framework [3], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters.\n\n\nFurthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the Coauthor dataset, and even beyond to larger graphs such as the well-known ogbn-arxiv dataset, a citation network, which consists of 170K nodes and 1.1 million edges.\n\n\n\n\nResults comparing Exphormer to standard GraphGPS on the five Long Range Graph Benchmark datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper’s publication.\n\n\n\nResults comparing Exphormer to standard GraphGPS on the five Long Range Graph Benchmark datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.\n\n-->\n\n\n   Model \n   \n    PascalVOC-SP \n\n      F1 score ↑ \n   \n    COCO-SP \n\n      F1 score ↑ \n   \n    Peptides-Func \n\n      AP ↑ \n   \n    Peptides-Struct \n\n      MAE ↓ \n   \n    PCQM-Contact\n\n      MRR ↑\n   \n  \n\n\n\n \n  \nStandard GraphGPS \n   \n    0.375 ± 0.011 \n   \n    0.341 ± 0.004 \n   \n     0.654 ± 0.004  \n   \n    0.250 ± 0.001 \n   \n    0.334 ± 0.001\n   \n  \nExphormer (ours) \n   \n    0.398 ± 0.004 \n   \n    0.346 ± 0.001 \n   \n    0.653 ± 0.004 \n   \n    0.248 ± 0.001 \n   \n    0.364 ± 0.002\n   \n  \n \n\n--> \n\n\n\nFinally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies. The Long Range Graph Benchmark is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions. Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication).\n\n \n\nConclusion\nvideo from ICML 2023.\n\n \n\nAcknowledgements\nWe thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.",
      "summary": "Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google Graphs, in which objects and their...",
      "summaryJa": "Posted by ameya velingker, 研究 scientist, google 研究, and balaji venkatachalam, software engineer, google graphs, in which objects and their...",
      "source": "Google AI Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Shaping the future of advanced robotics",
      "titleJa": "Shaping the future of advanced ロボティクス",
      "link": "https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/",
      "pubDate": "Thu, 04 Jan 2024 11:39:00 +0000",
      "content": "Introducing AutoRT, SARA-RT, and RT-Trajectory",
      "summary": "Introducing AutoRT, SARA-RT, and RT-Trajectory",
      "summaryJa": "Introducing autort, sara-rt, and rt-trajectory",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "2023: A Year of Groundbreaking Advances in AI and Computing",
      "titleJa": "2023: a year of groundbreaking advances in ai and computing",
      "link": "https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/",
      "pubDate": "Fri, 22 Dec 2023 13:30:00 +0000",
      "content": "This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications.",
      "summary": "This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications.",
      "summaryJa": "This has been a year of incredible progress in the field of 人工知能 (ai) 研究 and its practical applications.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    },
    {
      "title": "FunSearch: Making new discoveries in mathematical sciences using Large Language Models",
      "titleJa": "Funsearch: making new discoveries in mathematical sciences using large language models",
      "link": "https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
      "pubDate": "Thu, 14 Dec 2023 16:00:00 +0000",
      "content": "In a paper published in Nature, we introduce FunSearch, a method for searching for “functions” written in computer code, and find new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated “evaluator”, which guards against hallucinations and incorrect ideas.",
      "summary": "In a paper published in Nature, we introduce FunSearch, a method for searching for “functions” written in computer code, and find new solutions in mat...",
      "summaryJa": "In a 論文 published in nature, we introduce funsearch, a method for searching for “functions” written in computer code, and find new solutions in mat...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Google DeepMind at NeurIPS 2023",
      "titleJa": "Google deepmind at neurips 2023",
      "link": "https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/",
      "pubDate": "Fri, 08 Dec 2023 15:01:00 +0000",
      "content": "The Neural Information Processing Systems (NeurIPS) is the largest artificial intelligence (AI) conference in the world. NeurIPS 2023 will be taking place December 10-16 in New Orleans, USA.Teams from across Google DeepMind are presenting more than 150 papers at the main conference and workshops.",
      "summary": "The Neural Information Processing Systems (NeurIPS) is the largest artificial intelligence (AI) conference in the world.",
      "summaryJa": "The neural information processing systems (neurips) is the largest 人工知能 (ai) conference in the world.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Introducing Gemini: our largest and most capable AI model",
      "titleJa": "Introducing gemini: our largest and most capable ai モデル",
      "link": "https://deepmind.google/discover/blog/introducing-gemini-our-largest-and-most-capable-ai-model/",
      "pubDate": "Wed, 06 Dec 2023 15:13:00 +0000",
      "content": "Making AI more helpful for everyone",
      "summary": "Making AI more helpful for everyone",
      "summaryJa": "Making ai more helpful for everyone",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Scaling up learning across many different robot types",
      "titleJa": "Scaling up learning across many different robot types",
      "link": "https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/",
      "pubDate": "Tue, 03 Oct 2023 15:00:00 +0000",
      "content": "Robots are great specialists, but poor generalists. Typically, you have to train a model for each task, robot, and environment. Changing a single variable often requires starting from scratch. But what if we could combine the knowledge across robotics and create a way to train a general-purpose robot?",
      "summary": "Robots are great specialists, but poor generalists.",
      "summaryJa": "Robots are great specialists, but poor generalists.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "RT-2: New model translates vision and language into action",
      "titleJa": "Rt-2: new モデル translates vision and language into action",
      "link": "https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/",
      "pubDate": "Fri, 28 Jul 2023 00:00:00 +0000",
      "content": "Robotic Transformer 2 (RT-2) is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control.",
      "summary": "Robotic Transformer 2 (RT-2) is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge ...",
      "summaryJa": "Robotic トランスフォーマー 2 (rt-2) is a novel vision-language-action (vla) モデル that learns from both web and ロボティクス data, and translates this knowledge ...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Google DeepMind’s latest research at ICML 2023",
      "titleJa": "Google deepmind’s latest 研究 at icml 2023",
      "link": "https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/",
      "pubDate": "Thu, 20 Jul 2023 00:00:00 +0000",
      "content": "Exploring AI safety, adaptability, and efficiency for the real world",
      "summary": "Exploring AI safety, adaptability, and efficiency for the real world",
      "summaryJa": "Exploring ai safety, adaptability, and efficiency for the real world",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Developing reliable AI tools for healthcare",
      "titleJa": "Developing reliable ai tools for ヘルスケア",
      "link": "https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/",
      "pubDate": "Mon, 17 Jul 2023 00:00:00 +0000",
      "content": "We’ve published our joint paper with Google Research in Nature Medicine, which proposes CoDoC (Complementarity-driven Deferral-to-Clinical Workflow), an AI system that learns when to rely on predictive AI tools or defer to a clinician for the most accurate interpretation of medical images.",
      "summary": "We’ve published our joint paper with Google Research in Nature Medicine, which proposes CoDoC (Complementarity-driven Deferral-to-Clinical Workflow), ...",
      "summaryJa": "We’ve published our joint 論文 with google 研究 in nature medicine, which proposes codoc (complementarity-driven deferral-to-clinical workflow), ...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "RoboCat: A self-improving robotic agent",
      "titleJa": "Robocat: a self-improving robotic agent",
      "link": "https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/",
      "pubDate": "Tue, 20 Jun 2023 00:00:00 +0000",
      "content": "Robots are quickly becoming part of our everyday lives, but they’re often only programmed to perform specific tasks well. While harnessing recent advances in AI could lead to robots that could help in many more ways, progress in building general-purpose robots is slower in part because of the time needed to collect real-world training data. Our latest paper introduces a self-improving AI agent for robotics, RoboCat, that learns to perform a variety of tasks across different arms, and then self-generates new training data to improve its technique.",
      "summary": "Robots are quickly becoming part of our everyday lives, but they’re often only programmed to perform specific tasks well.",
      "summaryJa": "Robots are quickly becoming part of our everyday lives, but they’re often only programmed to perform specific tasks well.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "Google Cloud: Driving digital transformation",
      "titleJa": "Google クラウド: driving digital transformation",
      "link": "https://deepmind.google/discover/blog/google-cloud-driving-digital-transformation/",
      "pubDate": "Wed, 14 Jun 2023 14:51:00 +0000",
      "content": "Google Cloud empowers organizations to digitally transform themselves into smarter businesses. It offers cloud computing, data analytics, and the latest artificial intelligence (AI) and machine learning tools.",
      "summary": "Google Cloud empowers organizations to digitally transform themselves into smarter businesses.",
      "summaryJa": "Google クラウド empowers organizations to digitally transform themselves into smarter businesses.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "DeepMind’s latest research at ICLR 2023",
      "titleJa": "Deepmind’s latest 研究 at iclr 2023",
      "link": "https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/",
      "pubDate": "Thu, 27 Apr 2023 00:00:00 +0000",
      "content": "Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda. This will be the first major artificial intelligence (AI) conference to be hosted in Africa and the first in-person event since the start of the pandemic. Researchers from around the world will gather to share their cutting-edge work in deep learning spanning the fields of AI, statistics and data science, and applications including machine vision, gaming and robotics. We’re proud to support the conference as a Diamond sponsor and DEI champion.",
      "summary": "Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda.",
      "summaryJa": "Next week marks the start of the 11th international conference on learning representations (iclr), taking place 1-5 may in kigali, rwanda.",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 45
    },
    {
      "title": "Announcing Google DeepMind",
      "titleJa": "Announcing google deepmind",
      "link": "https://deepmind.google/discover/blog/announcing-google-deepmind/",
      "pubDate": "Thu, 20 Apr 2023 00:00:00 +0000",
      "content": "DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.",
      "summary": "DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenge...",
      "summaryJa": "Deepmind and the brain team from google 研究 will join forces to accelerate progress towards a world in which ai helps solve the biggest challenge...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 35
    },
    {
      "title": "DeepMind’s latest research at NeurIPS 2022",
      "titleJa": "Deepmind’s latest 研究 at neurips 2022",
      "link": "https://deepmind.google/discover/blog/deepminds-latest-research-at-neurips-2022/",
      "pubDate": "Fri, 25 Nov 2022 00:00:00 +0000",
      "content": "NeurIPS is the world’s largest conference in artificial intelligence (AI) and machine learning (ML), and we’re proud to support the event as Diamond sponsors, helping foster the exchange of research advances in the AI and ML community. Teams from across DeepMind are presenting 47 papers, including 35 external collaborations in virtual panels and poster sessions.",
      "summary": "NeurIPS is the world’s largest conference in artificial intelligence (AI) and machine learning (ML), and we’re proud to support the event as Diamond s...",
      "summaryJa": "Neurips is the world’s largest conference in 人工知能 (ai) and 機械学習 (ml), and we’re proud to support the event as diamond s...",
      "source": "DeepMind Blog",
      "category": "research",
      "importance": 30
    }
  ]
}